{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPAz8S9QmOV9"
      },
      "source": [
        "# *** DOWNLOADING & MERGING DATASETS + APPLYING CLAHE & GITHUB SEGMENTATION *** (no need to upload dataset, it is in drive + you can run all this section at once)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyVCTeFWmR5S",
        "outputId": "d47bebe7-3679-43ed-d821-e3011020d05d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 2) Clone the repository and cd into it\n",
        "!git clone https://github.com/YBIGTA/pytorch-hair-segmentation.git\n",
        "import os\n",
        "os.chdir('/content/pytorch-hair-segmentation')\n",
        "\n",
        "# 3) Install necessary dependencies (PyTorch, OpenCV, NumPy)\n",
        "!pip install --quiet torch torchvision opencv-contrib-python numpy\n",
        "\n",
        "# 4) Download & prepare the Figaro-1k dataset (no args → creates ./data/Figaro1k)\n",
        "!chmod +x data/figaro.sh\n",
        "!bash data/figaro.sh\n",
        "\n",
        "# 5) Copy Figaro-1k into /content for faster I/O\n",
        "!cp -r data/Figaro1k /content/Figaro1k\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC9L2r4xSo7m"
      },
      "source": [
        "Figaro1k segmentation training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsXXmiwds3sh",
        "outputId": "aa48d07d-52e5-4a9c-d2a6-1c85a4d41009"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Single Colab Cell: End‐to‐End Hair Segmentation with DeepLabV3+\n",
        "# (Handling “Training” & “Testing” Subfolders with PBM Masks)\n",
        "#\n",
        "# This pipeline will:\n",
        "#  1. Clone the YBIGTA hair‐segmentation repo.\n",
        "#  2. Use its data/figaro.sh script to download Figaro‐1k into the repo’s data folder,\n",
        "#     which creates:\n",
        "#       • data/Figaro1k/Original/Training/FrameXXXXX-org.jpg\n",
        "#       • data/Figaro1k/GT/Training/    FrameXXXXX-gt.pbm\n",
        "#       (and similarly for “Testing” subfolders).\n",
        "#  3. Train a DeepLabV3+ (ResNet‐50 backbone) model on all images in Original/Training.\n",
        "#  4. Evaluate IoU/F1 on all images in Original/Testing.\n",
        "#  5. Run inference on your own “flat_images” folder in Colab’s local storage to produce overlays.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#  • Select GPU runtime: Runtime → Change runtime type → GPU.\n",
        "#  • Create a folder “/content/flat_images/” via the Colab UI and upload your own .jpg/.png files there.\n",
        "################################################################################\n",
        "\n",
        "# (1) Install required libraries: PyTorch, TorchVision, OpenCV, tqdm\n",
        "!pip install --quiet torch torchvision opencv-python tqdm\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "################################################################################\n",
        "# (2) Clone the YBIGTA repo & download Figaro‐1k locally\n",
        "################################################################################\n",
        "\n",
        "if not os.path.isdir('/content/pytorch-hair-segmentation'):\n",
        "    !git clone https://github.com/YBIGTA/pytorch-hair-segmentation.git /content/pytorch-hair-segmentation\n",
        "\n",
        "os.chdir('/content/pytorch-hair-segmentation')\n",
        "\n",
        "# Run the provided script (no arguments) to download into data/Figaro1k\n",
        "# !chmod +x data/figaro.sh\n",
        "# !bash data/figaro.sh\n",
        "\n",
        "# Confirm directory structure:\n",
        "dataset_root = '/content/pytorch-hair-segmentation/data/Figaro1k'\n",
        "assert os.path.isdir(os.path.join(dataset_root, 'Original', 'Training')),  \"Original/Training not found\"\n",
        "assert os.path.isdir(os.path.join(dataset_root, 'Original', 'Testing')),  \"Original/Testing not found\"\n",
        "assert os.path.isdir(os.path.join(dataset_root, 'GT', 'Training')),        \"GT/Training not found\"\n",
        "assert os.path.isdir(os.path.join(dataset_root, 'GT', 'Testing')),         \"GT/Testing not found\"\n",
        "\n",
        "################################################################################\n",
        "# (3) Define a PyTorch Dataset class that matches “*‐org.jpg” to “*‐gt.pbm”\n",
        "################################################################################\n",
        "\n",
        "class FigaroHairDatasetNested(Dataset):\n",
        "    def __init__(self, root_dir, split=\"train\", img_size=256):\n",
        "        \"\"\"\n",
        "        root_dir: \"/content/pytorch-hair-segmentation/data/Figaro1k\"\n",
        "        split: \"train\"  → use all of Original/Training and corresponding GT/Training\n",
        "               \"test\"   → use all of Original/Testing  and corresponding GT/Testing\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.img_size = img_size\n",
        "\n",
        "        if split == \"train\":\n",
        "            img_dir  = os.path.join(root_dir, \"Original\", \"Training\")\n",
        "            mask_dir = os.path.join(root_dir, \"GT\",       \"Training\")\n",
        "        elif split == \"test\":\n",
        "            img_dir  = os.path.join(root_dir, \"Original\", \"Testing\")\n",
        "            mask_dir = os.path.join(root_dir, \"GT\",       \"Testing\")\n",
        "        else:\n",
        "            raise ValueError(\"split must be 'train' or 'test'\")\n",
        "\n",
        "        # List all image files ending with \"-org.jpg\"\n",
        "        self.img_paths = sorted([\n",
        "            os.path.join(img_dir, fname)\n",
        "            for fname in os.listdir(img_dir)\n",
        "            if fname.lower().endswith(\"-org.jpg\")\n",
        "        ])\n",
        "        # For each image, compute corresponding mask by replacing \"-org.jpg\" → \"-gt.pbm\"\n",
        "        self.mask_paths = []\n",
        "        for img_path in self.img_paths:\n",
        "            base = os.path.basename(img_path)               # e.g. \"Frame00001-org.jpg\"\n",
        "            mask_name = base.replace(\"-org.jpg\", \"-gt.pbm\")  # \"Frame00001-gt.pbm\"\n",
        "            mask_path = os.path.join(mask_dir, mask_name)\n",
        "            if not os.path.isfile(mask_path):\n",
        "                raise FileNotFoundError(f\"Mask file not found: {mask_path}\")\n",
        "            self.mask_paths.append(mask_path)\n",
        "\n",
        "        # Transforms for images\n",
        "        self.img_transform = transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "        # Transforms for masks (PBM loaded as grayscale)\n",
        "        self.mask_transform = transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size), interpolation=Image.NEAREST),\n",
        "            transforms.ToTensor(),  # yields shape (1,img_size,img_size) with values [0,1]\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path  = self.img_paths[idx]\n",
        "        mask_path = self.mask_paths[idx]\n",
        "        # Load image\n",
        "        img  = Image.open(img_path).convert(\"RGB\")\n",
        "        # Load mask (PBM format is supported by PIL)\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "        # Apply transforms\n",
        "        x = self.img_transform(img)             # (3, img_size, img_size)\n",
        "        y = self.mask_transform(mask)           # (1, img_size, img_size)\n",
        "        # Binary threshold: hair=1, background=0\n",
        "        y = (y > 0.5).float()                   # (1, img_size, img_size)\n",
        "        y = y.squeeze(0).long()                 # (img_size, img_size), dtype long\n",
        "        return x, y\n",
        "\n",
        "################################################################################\n",
        "# (4) Instantiate DataLoaders for “train” and “test”\n",
        "################################################################################\n",
        "\n",
        "batch_size = 4\n",
        "img_size   = 256\n",
        "\n",
        "train_ds = FigaroHairDatasetNested(root_dir=dataset_root, split=\"train\", img_size=img_size)\n",
        "test_ds  = FigaroHairDatasetNested(root_dir=dataset_root, split=\"test\",  img_size=img_size)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "################################################################################\n",
        "# (5) Define the Model: DeepLabV3+ with a ResNet‐50 Backbone\n",
        "################################################################################\n",
        "\n",
        "model = deeplabv3_resnet50(pretrained_backbone=True, pretrained=False, num_classes=2)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "################################################################################\n",
        "# (6) Training Loop: Loss, Optimizer, and Saving Checkpoints Locally\n",
        "################################################################################\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()   # for 2‐class segmentation\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "num_epochs  = 10\n",
        "save_folder = \"/content/models\"\n",
        "os.makedirs(save_folder, exist_ok=True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for imgs, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
        "        imgs  = imgs.to(device)    # (B,3,H,W)\n",
        "        masks = masks.to(device)   # (B,H,W)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)[\"out\"]    # (B,2,H,W)\n",
        "        loss    = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_ds)\n",
        "    print(f\"[Epoch {epoch+1}] Train Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    # Save a checkpoint for this epoch\n",
        "    ckpt_path = os.path.join(save_folder, f\"deeplabv3_epoch_{epoch:02d}.pth\")\n",
        "    torch.save(model.state_dict(), ckpt_path)\n",
        "    print(f\"Saved checkpoint: {ckpt_path}\")\n",
        "\n",
        "# After training finishes, the final checkpoint is:\n",
        "final_ckpt = os.path.join(save_folder, f\"deeplabv3_epoch_{num_epochs-1:02d}.pth\")\n",
        "\n",
        "################################################################################\n",
        "# (7) Evaluate on “Testing” Folder Using the Final Checkpoint\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(final_ckpt))\n",
        "model.eval()\n",
        "\n",
        "test_iou_sum = 0.0\n",
        "test_f1_sum  = 0.0\n",
        "num_batches  = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, masks in tqdm(test_loader, desc=\"Testing\"):\n",
        "        imgs  = imgs.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        outputs = model(imgs)[\"out\"]       # (B,2,H,W)\n",
        "        preds   = torch.argmax(outputs, dim=1)  # (B,H,W)\n",
        "\n",
        "        preds_flat = preds.view(-1)\n",
        "        masks_flat = masks.view(-1)\n",
        "\n",
        "        inter = torch.sum((preds_flat == 1) & (masks_flat == 1)).item()\n",
        "        union = torch.sum((preds_flat == 1) | (masks_flat == 1)).item()\n",
        "        iou   = 1.0 if union == 0 else inter / union\n",
        "\n",
        "        TP = inter\n",
        "        FP = torch.sum((preds_flat == 1) & (masks_flat == 0)).item()\n",
        "        FN = torch.sum((preds_flat == 0) & (masks_flat == 1)).item()\n",
        "        f1 = 1.0 if (2 * TP + FP + FN) == 0 else 2 * TP / (2 * TP + FP + FN)\n",
        "\n",
        "        test_iou_sum += iou\n",
        "        test_f1_sum  += f1\n",
        "        num_batches  += 1\n",
        "\n",
        "avg_test_iou = test_iou_sum / num_batches\n",
        "avg_test_f1  = test_f1_sum / num_batches\n",
        "print(f\"Testing Results → IoU: {avg_test_iou:.4f},  F1: {avg_test_f1:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599,
          "referenced_widgets": [
            "e63c47c9cd264d548e85673e1df8ba17",
            "d3b7a274677a4abd9dff3bed5ddaf1da",
            "c0a43c2bfdb54ad6af2b14c719f334e5",
            "e83f621ee7eb4b2284be0b304fb6562e",
            "6fd8de0e3a794e6486818cbf20ad547e",
            "09e1d70b49ac46ea8cb503e05abb5848",
            "c59cf49a0fe84a1f98dc18dea43f857c",
            "b2f981efbe034fa09f3e7ba7815b305e",
            "2612f9c633ff4a54a3c0895b2750ca18",
            "af18df28813e4976aebb5cd98731de71",
            "3f497bf091e947f4888364cfff0beec4",
            "7399733e3453480194d9b193f1bf04ed",
            "c7c397483182460f8fc56fd18bd59cc6",
            "f5934541a91e402bb27abc76e332a5c3",
            "ca6a389c63324a2d956431cbcdccfff0",
            "7033e35b84764f14b4fecd164d45cdcf",
            "6e474c849dd6460b816d9c8b83f987a9",
            "46ce572993a84db59cb79678d767518f",
            "46baf0dac1ef4949ae430cb10746440d",
            "53e363d04b4a4b608d146e1255a2f91c"
          ]
        },
        "id": "Kc4XlxrThpXZ",
        "outputId": "ab3dd625-c115-4781-c102-762e0600b52b"
      },
      "outputs": [],
      "source": [
        "# ################################################################################\n",
        "# # (8) Upload DeepLabV3+ Hair Segmentation Model to Hugging Face Hub\n",
        "# ################################################################################\n",
        "\n",
        "# !pip install -q huggingface_hub\n",
        "\n",
        "# from huggingface_hub import login, create_repo, upload_folder\n",
        "# import json\n",
        "\n",
        "# # 🔐 Log in to Hugging Face (you'll paste your token once)\n",
        "# login()\n",
        "\n",
        "# # Define repo info\n",
        "# #hf_username = \"RyanThawkho\"  # ← CHANGE THIS\n",
        "# hf_username = \"alamb98\"  # ← CHANGE THIS\n",
        "# repo_name   = \"deeplabv3-hair-segmentation\"\n",
        "# repo_id     = f\"{hf_username}/{repo_name}\"\n",
        "\n",
        "# # Create model repo if it doesn't exist yet\n",
        "# create_repo(repo_id, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "# # Save final model weights (if not already done)\n",
        "# torch.save(model.state_dict(), \"deeplabv3_final.pth\")\n",
        "\n",
        "# # Save basic metadata\n",
        "# metadata = {\n",
        "#     \"model_name\": \"DeepLabV3+ (ResNet50 backbone)\",\n",
        "#     \"image_size\": [img_size, img_size],\n",
        "#     \"num_classes\": 2,\n",
        "#     \"labels\": [\"background\", \"hair\"],\n",
        "#     \"test_iou\": round(avg_test_iou, 4),\n",
        "#     \"test_f1\": round(avg_test_f1, 4)\n",
        "# }\n",
        "# with open(\"metadata.json\", \"w\") as f:\n",
        "#     json.dump(metadata, f)\n",
        "\n",
        "# # Upload to Hugging Face (only the files we want)\n",
        "# upload_folder(\n",
        "#     folder_path=\".\",\n",
        "#     repo_id=repo_id,\n",
        "#     repo_type=\"model\",\n",
        "#     allow_patterns=[\"deeplabv3_final.pth\", \"metadata.json\"]\n",
        "# )\n",
        "\n",
        "# print(f\"✅ Model uploaded to: https://huggingface.co/{repo_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR0StmgvIyII"
      },
      "source": [
        "download & combine our two datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDA4mZ8RHoeY",
        "outputId": "8809ae2f-f8ba-42d5-de77-8e8269d57abc"
      },
      "outputs": [],
      "source": [
        "# (0) Install necessary tools\n",
        "!pip install --quiet gdown\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "import gdown\n",
        "\n",
        "################################################################################\n",
        "# (1) GOOGLE DRIVE LINKS FOR THE TWO ZIP FILES\n",
        "#     Replace these with your actual shareable links.\n",
        "################################################################################\n",
        "\n",
        "LINK_ZIP1 = \"https://drive.google.com/file/d/1nP7u3PqnvdYEUnE9oxRo-SAvOO28w_Az/view?usp=drive_link\"\n",
        "LINK_ZIP2 = \"https://drive.google.com/file/d/1iRoI1JI7kVPP4dFbR1xiZM4lOpf9Osy5/view?usp=drive_link\"\n",
        "\n",
        "def download_from_drive(shareable_link: str, dest_path: str):\n",
        "    \"\"\"\n",
        "    Given a Google Drive shareable link, download the file to dest_path using gdown.\n",
        "    \"\"\"\n",
        "    file_id = shareable_link.split(\"/d/\")[1].split(\"/\")[0]\n",
        "    gdown_url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "    print(f\"Downloading {shareable_link} → {dest_path}\")\n",
        "    gdown.download(gdown_url, dest_path, quiet=False)\n",
        "\n",
        "# Paths where we'll save the two downloaded ZIPs\n",
        "ZIP1_PATH = \"/content/dataset1.zip\"\n",
        "ZIP2_PATH = \"/content/dataset2.zip\"\n",
        "\n",
        "download_from_drive(LINK_ZIP1, ZIP1_PATH)\n",
        "download_from_drive(LINK_ZIP2, ZIP2_PATH)\n",
        "\n",
        "################################################################################\n",
        "# (2) UNZIP EACH INTO ITS OWN FOLDER\n",
        "################################################################################\n",
        "\n",
        "UNZIP_DIR1 = \"/content/unzipped_dataset1\"\n",
        "UNZIP_DIR2 = \"/content/unzipped_dataset2\"\n",
        "\n",
        "def reset_dir(path: str):\n",
        "    if os.path.isdir(path):\n",
        "        shutil.rmtree(path)\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "reset_dir(UNZIP_DIR1)\n",
        "reset_dir(UNZIP_DIR2)\n",
        "\n",
        "print(f\"Unzipping {ZIP1_PATH} → {UNZIP_DIR1}\")\n",
        "with zipfile.ZipFile(ZIP1_PATH, \"r\") as zf1:\n",
        "    zf1.extractall(UNZIP_DIR1)\n",
        "\n",
        "print(f\"Unzipping {ZIP2_PATH} → {UNZIP_DIR2}\")\n",
        "with zipfile.ZipFile(ZIP2_PATH, \"r\") as zf2:\n",
        "    zf2.extractall(UNZIP_DIR2)\n",
        "\n",
        "################################################################################\n",
        "# (3) INSPECT and LOCATE the TRUE “BASE” FOLDER containing train/valid/test\n",
        "#\n",
        "# We write a helper that skips any “__MACOSX” and finds a subfolder that\n",
        "# contains “train” (i.e., has a “train” directory).\n",
        "################################################################################\n",
        "\n",
        "def find_split_base(unzip_dir: str) -> str:\n",
        "    \"\"\"\n",
        "    Traverse one level down under unzip_dir, ignoring folders named \"__MACOSX\".\n",
        "    If any folder at that level contains a 'train' subdirectory, return its path.\n",
        "    If unzip_dir itself contains 'train', return unzip_dir.\n",
        "    Otherwise, raise an error.\n",
        "    \"\"\"\n",
        "    # Case A: unzip_dir/train exists\n",
        "    if os.path.isdir(os.path.join(unzip_dir, \"train\")):\n",
        "        return unzip_dir\n",
        "\n",
        "    # Otherwise check children (skip \"__MACOSX\")\n",
        "    for child in sorted(os.listdir(unzip_dir)):\n",
        "        if child.startswith(\"__MACOSX\"):\n",
        "            continue\n",
        "        child_path = os.path.join(unzip_dir, child)\n",
        "        if os.path.isdir(child_path) and os.path.isdir(os.path.join(child_path, \"train\")):\n",
        "            return child_path\n",
        "\n",
        "    raise FileNotFoundError(\n",
        "        f\"Could not locate a 'train' folder under {unzip_dir} or its valid child.\"\n",
        "    )\n",
        "\n",
        "# Determine BASE1 and BASE2 automatically\n",
        "BASE1 = find_split_base(UNZIP_DIR1)\n",
        "BASE2 = find_split_base(UNZIP_DIR2)\n",
        "print(f\"Detected BASE1 = {BASE1}\")\n",
        "print(f\"Detected BASE2 = {BASE2}\")\n",
        "\n",
        "# (Optional) Show contents at that base to double-check\n",
        "print(\"\\nContents at BASE1 (first two levels):\")\n",
        "!find \"{BASE1}\" -maxdepth 2 | sed 's/^/  /'\n",
        "print(\"\\nContents at BASE2 (first two levels):\")\n",
        "!find \"{BASE2}\" -maxdepth 2 | sed 's/^/  /'\n",
        "\n",
        "################################################################################\n",
        "# (4) DEFINE COMBINED OUTPUT DIRECTORY\n",
        "################################################################################\n",
        "\n",
        "COMBINED_DIR = \"/content/norwood_dataset_combined\"\n",
        "reset_dir(COMBINED_DIR)\n",
        "\n",
        "################################################################################\n",
        "# (5) MERGE “train/Level X”, “valid/Level X”, “test/Level X” FROM BOTH BASES\n",
        "#\n",
        "# Copy every .jpg/.jpeg/.png under BASE1/<split>/<level>/ and BASE2/<split>/<level>/\n",
        "# into COMBINED_DIR/<split>/<level>/. If a filename collides, append \"_dup\".\n",
        "################################################################################\n",
        "\n",
        "def merge_split_levels(src_base: str, dst_base: str):\n",
        "    for split in [\"train\", \"valid\", \"test\"]:\n",
        "        for level in [f\"Level {i}\" for i in range(2, 8)]:\n",
        "            src_dir = os.path.join(src_base, split, level)\n",
        "            dst_dir = os.path.join(dst_base, split, level)\n",
        "\n",
        "            if not os.path.isdir(src_dir):\n",
        "                continue\n",
        "\n",
        "            os.makedirs(dst_dir, exist_ok=True)\n",
        "\n",
        "            for fname in os.listdir(src_dir):\n",
        "                lower = fname.lower()\n",
        "                if not (lower.endswith(\".jpg\") or lower.endswith(\".jpeg\") or lower.endswith(\".png\")):\n",
        "                    continue\n",
        "\n",
        "                src_path = os.path.join(src_dir, fname)\n",
        "                dst_path = os.path.join(dst_dir, fname)\n",
        "\n",
        "                # If a file of same name exists, append \"_dup\"\n",
        "                if os.path.exists(dst_path):\n",
        "                    name, ext = os.path.splitext(fname)\n",
        "                    dst_path = os.path.join(dst_dir, f\"{name}_dup{ext}\")\n",
        "\n",
        "                shutil.copyfile(src_path, dst_path)\n",
        "\n",
        "print(\"Merging unzipped_dataset1 → combined\")\n",
        "merge_split_levels(BASE1, COMBINED_DIR)\n",
        "\n",
        "print(\"Merging unzipped_dataset2 → combined\")\n",
        "merge_split_levels(BASE2, COMBINED_DIR)\n",
        "\n",
        "print(f\"\\n✅ Done! Combined dataset is now at: {COMBINED_DIR}\")\n",
        "for split in [\"train\", \"valid\", \"test\"]:\n",
        "    for level in [f\"Level {i}\" for i in range(2, 8)]:\n",
        "        combined_path = os.path.join(COMBINED_DIR, split, level)\n",
        "        if os.path.isdir(combined_path):\n",
        "            count = len(os.listdir(combined_path))\n",
        "            print(f\"  • {split}/{level}: {count} files\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_cB9hp3Z_AE"
      },
      "source": [
        "Segment our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "J-fk98NPjej1",
        "outputId": "0cacdaa1-61c2-4911-e5d2-ae66308f60b4"
      },
      "outputs": [],
      "source": [
        "# # ────────────────────────────────────────────────────────────────\n",
        "# # UPLOAD THE TRAINED MODEL CHECKPOINT TO HUGGING FACE\n",
        "# # ────────────────────────────────────────────────────────────────\n",
        "# !pip install --quiet huggingface_hub\n",
        "\n",
        "# from huggingface_hub import login, create_repo, upload_file\n",
        "\n",
        "# # 1. Log into Hugging Face\n",
        "# login(token=\"your_huggingface_token\")  # Replace with your HF token\n",
        "\n",
        "# # 2. Create or point to your repo\n",
        "# repo_id = \"your-username/deeplabv3-hair-segmentation\"  # Choose a name\n",
        "# create_repo(repo_id, private=True)  # Set to public=False if you want it private\n",
        "\n",
        "# # 3. Upload the checkpoint\n",
        "# upload_file(\n",
        "#     path_or_fileobj=\"/content/models/deeplabv3_epoch_09.pth\",  # Your model file\n",
        "#     path_in_repo=\"deeplabv3_epoch_09.pth\",                      # How it appears in repo\n",
        "#     repo_id=repo_id,\n",
        "#     repo_type=\"model\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAzyMhpwR-lV"
      },
      "source": [
        "## ***Segmentation with lines not fill***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rt5h5ZW4R9ne",
        "outputId": "be442a44-2c7e-4777-c913-cc85af1092f8"
      },
      "outputs": [],
      "source": [
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# COLAB‐CELL: Run Hair Segmentation on train/valid/test → Save Boundary‐Only Overlays\n",
        "#             under /content/segmented_boundary/<split>/Level X/…\n",
        "#\n",
        "# Instead of filling the entire hair region in solid red, this version draws only\n",
        "# the red outline (contour) of the detected hair mask on top of the original image.\n",
        "#\n",
        "# Assumes:\n",
        "#  • Your ZIP (already unzipped) created these folders:\n",
        "#      /content/norwood_dataset/hyehye2.v3i.folder/train/Level 2 … Level 7/\n",
        "#      /content/norwood_dataset/hyehye2.v3i.folder/valid/Level 2 … Level 7/\n",
        "#      /content/norwood_dataset/hyehye2.v3i.folder/test/Level 2 … Level 7/\n",
        "#  • You have a DeepLabV3+ checkpoint at /content/models/deeplabv3_epoch_09.pth\n",
        "#  • You want to save “hair‐region boundary in red” overlays for all images.\n",
        "#\n",
        "# After this cell finishes, browse in Colab’s left “Files” pane under\n",
        "# /content/segmented_boundary/ to see the saved boundary overlays.\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "!pip install --quiet torch torchvision opencv-python tqdm\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1) Load the pretrained DeepLabV3+ model checkpoint\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "CHECKPOINT_PATH = \"/content/models/deeplabv3_epoch_09.pth\"  # <— adjust if needed\n",
        "\n",
        "model = deeplabv3_resnet50(pretrained_backbone=True, pretrained=False, num_classes=2)\n",
        "model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=DEVICE))\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "# 2) Define the same image transforms used during segmentation inference\n",
        "IMG_SIZE = 256\n",
        "transform_img = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# 3) Point to your “ordinarily‐located” folders of raw images\n",
        "BASE = \"/content/norwood_dataset_combined\"\n",
        "SPLITS = [\"train\", \"valid\", \"test\"]\n",
        "LEVELS = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", … \"Level 7\"]\n",
        "\n",
        "# 4) Create /content/segmented_boundary/<split>/Level X/… if they don’t already exist\n",
        "SEGMENTED_BOUNDARY_ROOT = \"/content/segmented_boundary\"\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVELS:\n",
        "        os.makedirs(os.path.join(SEGMENTED_BOUNDARY_ROOT, split, lvl), exist_ok=True)\n",
        "\n",
        "# 5) Loop over each split (train/valid/test) & each Level X, run segmentation,\n",
        "#    find contours of hair mask, draw red contours on original, and save\n",
        "for split in SPLITS:\n",
        "    print(f\"\\n▶ Processing {split.upper()} set …\")\n",
        "    for lvl in LEVELS:\n",
        "        raw_dir = os.path.join(BASE, split, lvl)\n",
        "        save_dir = os.path.join(SEGMENTED_BOUNDARY_ROOT, split, lvl)\n",
        "\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Expected folder not found: {raw_dir}\")\n",
        "\n",
        "        print(f\"  • Level: {lvl}  →  {len(os.listdir(raw_dir))} images\")\n",
        "        for fname in tqdm(os.listdir(raw_dir), desc=f\"    {lvl}\", leave=False):\n",
        "            if not fname.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\")):\n",
        "                continue\n",
        "\n",
        "            # Load the raw image\n",
        "            img_path = os.path.join(raw_dir, fname)\n",
        "            orig = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "            # Resize to model’s expected size\n",
        "            resized = orig.resize((IMG_SIZE, IMG_SIZE))\n",
        "            x = transform_img(resized).unsqueeze(0).to(DEVICE)  # (1,3,256,256)\n",
        "\n",
        "            # Forward‐pass → get binary mask (0=background, 1=hair)\n",
        "            with torch.no_grad():\n",
        "                out = model(x)[\"out\"]                             # (1,2,256,256)\n",
        "                pred = torch.argmax(out, dim=1).squeeze(0).cpu().numpy()  # (256,256)\n",
        "\n",
        "            # Resize mask back to original image resolution\n",
        "            mask_resized = cv2.resize(pred.astype(np.uint8),\n",
        "                                      (orig.width, orig.height),\n",
        "                                      interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "            # Convert mask to 8‐bit single‐channel for contour detection\n",
        "            mask_uint8 = (mask_resized * 255).astype(np.uint8)\n",
        "\n",
        "            # 6) Find contours of the hair region\n",
        "            contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "            # 7) Draw red contours on top of the original\n",
        "            overlay = np.array(orig).astype(np.uint8).copy()\n",
        "            cv2.drawContours(overlay, contours, -1, color=(255, 0, 0), thickness=2)  # (H,W,3), BGR by OpenCV\n",
        "\n",
        "            # Note: cv2 draws in BGR by default; convert original to BGR before drawing:\n",
        "            # Actually, overlay is in RGB, so we need to convert the color tuple:\n",
        "            # But here, cv2.drawContours expects BGR if the image is a NumPy array in BGR.\n",
        "            # Since `overlay` is actually in RGB order, we can swap the color channels:\n",
        "            #   overlay[..., [2,1,0]] if needed.\n",
        "            # To keep it simple, let's convert overlay to BGR, draw, then convert back:\n",
        "\n",
        "            # Step a) Convert RGB→BGR\n",
        "            overlay_bgr = cv2.cvtColor(np.array(orig).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
        "            cv2.drawContours(overlay_bgr, contours, -1, color=(0, 0, 255), thickness=2)  # Red in BGR\n",
        "            # Step b) Convert back to RGB\n",
        "            overlay_rgb = cv2.cvtColor(overlay_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # 8) Save just the contour overlay (no blending)\n",
        "            base_name = os.path.splitext(fname)[0]\n",
        "            out_name = base_name + \"_boundary.png\"\n",
        "            out_path = os.path.join(save_dir, out_name)\n",
        "            Image.fromarray(overlay_rgb).save(out_path)\n",
        "\n",
        "    print(f\"✔ Finished boundary‐only segmentation for {split}.\\n\")\n",
        "\n",
        "print(\"\\n✅ All boundary‐only overlays are now saved under /content/segmented_boundary/\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_IYEEh0SKQB"
      },
      "source": [
        "Apply clahe on the segmented dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0vaK5jJ17-WY",
        "outputId": "4954291b-1b40-41e0-cd49-a16d1c3dff64"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# (A) Change these two paths to match your environment\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "INPUT_ROOT  = \"/content/segmented_boundary\"\n",
        "OUTPUT_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# (B) CLAHE setup\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# (C) Walk splits → levels → images, apply CLAHE, save to OUTPUT_ROOT\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "for split in [\"train\", \"valid\", \"test\"]:\n",
        "    split_in_dir  = os.path.join(INPUT_ROOT, split)\n",
        "    split_out_dir = os.path.join(OUTPUT_ROOT, split)\n",
        "\n",
        "    if not os.path.isdir(split_in_dir):\n",
        "        print(f\"[WARN] Skipping missing split folder: {split_in_dir}\")\n",
        "        continue\n",
        "\n",
        "    # Create the split folder in the output if it doesn't exist\n",
        "    os.makedirs(split_out_dir, exist_ok=True)\n",
        "\n",
        "    # Each “level” subfolder (Level 2, Level 3, … Level 7)\n",
        "    for level_folder in sorted(os.listdir(split_in_dir)):\n",
        "        level_in_path = os.path.join(split_in_dir, level_folder)\n",
        "        if not os.path.isdir(level_in_path):\n",
        "            continue  # skip any stray files\n",
        "\n",
        "        # Create the same level subfolder in output\n",
        "        level_out_path = os.path.join(split_out_dir, level_folder)\n",
        "        os.makedirs(level_out_path, exist_ok=True)\n",
        "\n",
        "        # Process each image in this level folder (allow .jpg, .jpeg, .png)\n",
        "        for img_name in sorted(os.listdir(level_in_path)):\n",
        "            lower = img_name.lower()\n",
        "            if not (lower.endswith(\".jpg\") or lower.endswith(\".jpeg\") or lower.endswith(\".png\")):\n",
        "                continue\n",
        "\n",
        "            input_path  = os.path.join(level_in_path, img_name)\n",
        "            output_name = os.path.splitext(img_name)[0] + \".png\"  # save as .png\n",
        "            output_path = os.path.join(level_out_path, output_name)\n",
        "\n",
        "            img = cv2.imread(input_path)\n",
        "            if img is None:\n",
        "                print(f\"  [SKIP] Could not read image: {input_path}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Convert BGR → LAB\n",
        "                lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
        "                l, a, b = cv2.split(lab)\n",
        "\n",
        "                # Apply CLAHE on the L channel\n",
        "                cl = clahe.apply(l)\n",
        "\n",
        "                # Merge the enhanced L channel back with A and B\n",
        "                merged = cv2.merge((cl, a, b))\n",
        "\n",
        "                # Convert LAB → BGR\n",
        "                enhanced_img = cv2.cvtColor(merged, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "                # Save as PNG (retaining the same basename)\n",
        "                cv2.imwrite(output_path, enhanced_img)\n",
        "                print(f\"  ✓ Saved CLAHE: {output_path}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  [ERROR] processing {input_path}: {e}\")\n",
        "\n",
        "print(\"\\n➡️  Done with CLAHE preprocessing.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF21mIEvSoKG"
      },
      "source": [
        "combine raw with clahe in one folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d48_Yry_vFv"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import shutil\n",
        "\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "# # (A) Adjust these four source paths to match your environment exactly\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# SEG_ORIG_ROOT   = \"/content/segmented_boundary\"\n",
        "# SEG_CLAHE_ROOT  = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# RAW_ORIG_ROOT   = \"/content/norwood_dataset/hyehye2.v3i.folder 2\"\n",
        "# RAW_CLAHE_ROOT  = \"/content/norwood_dataset/hyehye2.v3i.folder 2_clahe\"\n",
        "\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "# # (B) Define your two “combined” destination roots\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# COMBINED_SEG_ROOT = \"/content/combined_segmented_boundary\"\n",
        "# COMBINED_RAW_ROOT = \"/content/combined_norwood_raw\"\n",
        "\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "# # (C) List of splits & levels (we assume exactly these subfolders exist in each source)\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# SPLITS      = [\"train\", \"valid\", \"test\"]\n",
        "# LEVEL_NAMES = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "# # (D) Helper to “safe copy” an entire directory tree, appending a suffix to filenames\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# def copy_tree_with_suffix(src_root: str, dst_root: str, suffix: str):\n",
        "#     \"\"\"\n",
        "#     Walk through every file under `src_root/<split>/<level>/...` and copy it into\n",
        "#     `dst_root/<split>/<level>/...`, appending `suffix` before the file extension.\n",
        "\n",
        "#     Example:\n",
        "#       src_root = \"/content/segmented_boundary\"\n",
        "#       dst_root = \"/content/combined_segmented_boundary\"\n",
        "#       suffix   = \"_orig\"\n",
        "\n",
        "#       then \"/content/segmented_boundary/train/Level 2/img123.jpg\"\n",
        "#       → copies to \"/content/combined_segmented_boundary/train/Level 2/img123_orig.jpg\"\n",
        "#     \"\"\"\n",
        "#     for split in SPLITS:\n",
        "#         for lvl in LEVEL_NAMES:\n",
        "#             in_dir  = os.path.join(src_root, split, lvl)\n",
        "#             out_dir = os.path.join(dst_root, split, lvl)\n",
        "\n",
        "#             if not os.path.isdir(in_dir):\n",
        "#                 # If a split/level folder doesn’t exist, skip it\n",
        "#                 continue\n",
        "\n",
        "#             # Ensure the destination folder exists\n",
        "#             os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "#             for fname in os.listdir(in_dir):\n",
        "#                 # Only copy image files (assume .png or .jpg)\n",
        "#                 if not (fname.lower().endswith(\".jpg\") or fname.lower().endswith(\".jpeg\") or fname.lower().endswith(\".png\")):\n",
        "#                     continue\n",
        "\n",
        "#                 src_path = os.path.join(in_dir, fname)\n",
        "#                 name, ext = os.path.splitext(fname)\n",
        "#                 dst_fname = f\"{name}{suffix}{ext}\"\n",
        "#                 dst_path = os.path.join(out_dir, dst_fname)\n",
        "\n",
        "#                 try:\n",
        "#                     shutil.copyfile(src_path, dst_path)\n",
        "#                 except Exception as e:\n",
        "#                     print(f\"  [ERROR] copying {src_path} → {dst_path}: {e}\")\n",
        "\n",
        "\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "# # (E) 1) Copy all segmented (no‐CLAHE) into COMBINED_SEG_ROOT with suffix \"_orig\"\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# print(\"Copying segmented (no‐CLAHE) → combined (suffix=_orig)\")\n",
        "# copy_tree_with_suffix(SEG_ORIG_ROOT, COMBINED_SEG_ROOT, suffix=\"_orig\")\n",
        "\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "# # (F) 2) Copy all segmented (CLAHE) into COMBINED_SEG_ROOT with suffix \"_clahe\"\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# print(\"Copying segmented (CLAHE) → combined (suffix=_clahe)\")\n",
        "# copy_tree_with_suffix(SEG_CLAHE_ROOT, COMBINED_SEG_ROOT, suffix=\"_clahe\")\n",
        "\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "# # (G) 3) Copy all raw (no‐CLAHE) into COMBINED_RAW_ROOT with suffix \"_orig\"\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# print(\"Copying raw (no‐CLAHE) → combined (suffix=_orig)\")\n",
        "# copy_tree_with_suffix(RAW_ORIG_ROOT, COMBINED_RAW_ROOT, suffix=\"_orig\")\n",
        "\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "# # (H) 4) Copy all raw (CLAHE) into COMBINED_RAW_ROOT with suffix \"_clahe\"\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# print(\"Copying raw (CLAHE) → combined (suffix=_clahe)\")\n",
        "# copy_tree_with_suffix(RAW_CLAHE_ROOT, COMBINED_RAW_ROOT, suffix=\"_clahe\")\n",
        "\n",
        "# print(\"\\n✅ Done! Two combined folders created:\")\n",
        "# print(f\"  • Combined segmented images: {COMBINED_SEG_ROOT}\")\n",
        "# print(f\"  • Combined raw images:       {COMBINED_RAW_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rv43QQq6d5RU",
        "outputId": "4402d505-f99b-44c0-d2af-9b10a80694ca"
      },
      "outputs": [],
      "source": [
        "# prompt: write a code which counts that amount of images in each folder in /content/segmented_boundary_clahe\n",
        "\n",
        "import os\n",
        "\n",
        "target_directory = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "if not os.path.isdir(target_directory):\n",
        "  print(f\"Directory not found: {target_directory}\")\n",
        "else:\n",
        "  print(f\"Counting images in subfolders of: {target_directory}\")\n",
        "  for root, dirs, files in os.walk(target_directory):\n",
        "    image_count = 0\n",
        "    for name in files:\n",
        "      # Check for common image file extensions (case-insensitive)\n",
        "      if name.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
        "        image_count += 1\n",
        "    # Only print if the directory is not the root and contains images\n",
        "    if root != target_directory and image_count > 0:\n",
        "      print(f\"  {root}: {image_count} images\")\n",
        "    # If it's the root directory, print its image count if any\n",
        "    elif root == target_directory and image_count > 0:\n",
        "         print(f\"  {root}: {image_count} images (directly in root)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a-BwG5oaWao"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oWo1qno5lnO"
      },
      "source": [
        "# ***MODELS***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPjDFfIUczNH"
      },
      "source": [
        "Loop for ratio [IGNORE]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7MwdcSdc1eN"
      },
      "outputs": [],
      "source": [
        "# ################################################################################\n",
        "# # Colab‐Cell: Hyperparameter Search over expand_ratio (0%–60%) for EfficientNet-B0\n",
        "# # on “Cropped‐Interior” Norwood‐Stage Classification\n",
        "# #\n",
        "# # We loop expand_ratio from 0.00 to 0.60 in 0.01 increments. For each ratio:\n",
        "# #   1) Instantiate a dataset that crops original images to the hair‐region’s bounding\n",
        "# #      box expanded by expand_ratio * max(width, height).\n",
        "# #   2) Train EfficientNet-B0 (pretrained, final layer → 6 classes) for 10 epochs on train/valid.\n",
        "# #   3) Evaluate on the test split, compute macro F1.\n",
        "# #   4) Track the highest test‐set macro F1 and the corresponding expand_ratio.\n",
        "# #\n",
        "# # At the end, we print the best expand_ratio and F1, and save that model’s weights.\n",
        "# #\n",
        "# # BEFORE RUNNING:\n",
        "# #  • Ensure folders exist:\n",
        "# #      /content/segmented_boundary/<split>/Level X/*_boundary.png\n",
        "# #      /content/norwood_dataset/hyehye2.v3i.folder/<split>/Level X>/*.jpg\n",
        "# #  • Select GPU runtime: Runtime → Change runtime type → GPU.\n",
        "# ################################################################################\n",
        "\n",
        "# # (0) Install & import dependencies\n",
        "# !pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "# import os\n",
        "# import cv2\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torchvision import transforms\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from PIL import Image\n",
        "# import numpy as np\n",
        "# from tqdm import tqdm\n",
        "# from collections import Counter\n",
        "\n",
        "# # EfficientNet-B0\n",
        "# from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# # sklearn metrics\n",
        "# from sklearn.metrics import confusion_matrix, f1_score\n",
        "# import pandas as pd\n",
        "# import seaborn as sns\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# ################################################################################\n",
        "# # (1) Custom Dataset: Crop to an expanded hair‐region bounding box (variable margin)\n",
        "# ################################################################################\n",
        "\n",
        "# class NorwoodCroppedExpandedDataset(Dataset):\n",
        "#     def __init__(self,\n",
        "#                  raw_root: str,\n",
        "#                  boundary_root: str,\n",
        "#                  split: str,\n",
        "#                  level_names: list,\n",
        "#                  transform=None,\n",
        "#                  expand_ratio: float = 0.10):\n",
        "#         \"\"\"\n",
        "#         raw_root:      \"/content/norwood_dataset/hyehye2.v3i.folder\"\n",
        "#         boundary_root: \"/content/segmented_boundary\"\n",
        "#         split:         one of \"train\", \"valid\", \"test\"\n",
        "#         level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "#         transform:     torchvision transforms applied to each cropped image\n",
        "#         expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "#         \"\"\"\n",
        "#         self.samples = []  # (raw_img_path, boundary_img_path, label_index)\n",
        "#         self.transform = transform\n",
        "#         self.expand_ratio = expand_ratio\n",
        "\n",
        "#         for idx, lvl in enumerate(level_names):\n",
        "#             raw_dir = os.path.join(raw_root, split, lvl)\n",
        "#             bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "#             if not os.path.isdir(raw_dir):\n",
        "#                 raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "#             if not os.path.isdir(bnd_dir):\n",
        "#                 raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "#             for fname in os.listdir(bnd_dir):\n",
        "#                 if not fname.lower().endswith(\"_boundary.png\"):\n",
        "#                     continue\n",
        "#                 base = fname[:-len(\"_boundary.png\")]  # e.g. \"21-Front_jpg.rf...\"\n",
        "#                 raw_fname = base + \".jpg\"\n",
        "#                 raw_path = os.path.join(raw_dir, raw_fname)\n",
        "#                 bnd_path = os.path.join(bnd_dir, fname)\n",
        "#                 if not os.path.isfile(raw_path):\n",
        "#                     raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "#                 self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "#         if len(self.samples) == 0:\n",
        "#             raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.samples)\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "#         # Load raw and boundary as RGB NumPy arrays\n",
        "#         raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "#         bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "#         H, W, _ = raw_img.shape\n",
        "\n",
        "#         # Binary mask of red boundary via HSV threshold\n",
        "#         hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "#         lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "#         lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "#         mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "#         mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "#         red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "#         # Find external contours\n",
        "#         contours, _ = cv2.findContours(\n",
        "#             red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "#         )\n",
        "#         if len(contours) == 0:\n",
        "#             x, y, w, h = 0, 0, W, H\n",
        "#         else:\n",
        "#             all_pts = np.vstack(contours).squeeze()\n",
        "#             xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "#             x_min, x_max = xs.min(), xs.max()\n",
        "#             y_min, y_max = ys.min(), ys.max()\n",
        "#             x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "#         # Expand bounding box by expand_ratio * max(w, h)\n",
        "#         margin = int(self.expand_ratio * max(w, h))\n",
        "#         x1 = max(0, x - margin)\n",
        "#         y1 = max(0, y - margin)\n",
        "#         x2 = min(W, x + w + margin)\n",
        "#         y2 = min(H, y + h + margin)\n",
        "\n",
        "#         # Crop raw image\n",
        "#         cropped = raw_img[y1:y2, x1:x2]\n",
        "#         if cropped.size == 0:\n",
        "#             cropped = raw_img.copy()\n",
        "\n",
        "#         # Convert to PIL and apply transforms\n",
        "#         cropped_pil = Image.fromarray(cropped)\n",
        "#         if self.transform:\n",
        "#             cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "#         return cropped_pil, label\n",
        "\n",
        "# ################################################################################\n",
        "# # (2) Paths, Classes, and Transforms\n",
        "# ################################################################################\n",
        "\n",
        "# RAW_ROOT      = \"/content/norwood_dataset/hyehye2.v3i.folder\"\n",
        "# BOUNDARY_ROOT = \"/content/segmented_boundary\"\n",
        "# SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "# LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# # Verify folder structure once\n",
        "# for split in SPLITS:\n",
        "#     for lvl in LEVEL_NAMES:\n",
        "#         raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "#         bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "#         if not os.path.isdir(raw_dir):\n",
        "#             raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "#         if not os.path.isdir(bnd_dir):\n",
        "#             raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "# print(\"Verified raw + boundary folder structure.\")\n",
        "\n",
        "# # Transforms: resize to 224×224, augment on train\n",
        "# train_transform = transforms.Compose([\n",
        "#     transforms.Resize((224, 224)),\n",
        "#     transforms.RandomHorizontalFlip(p=0.5),\n",
        "#     transforms.RandomRotation(10),\n",
        "#     transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#                          std=[0.229, 0.224, 0.225]),\n",
        "# ])\n",
        "# eval_transform = transforms.Compose([\n",
        "#     transforms.Resize((224, 224)),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#                          std=[0.229, 0.224, 0.225]),\n",
        "# ])\n",
        "\n",
        "# ################################################################################\n",
        "# # (3) Hyperparameter Search Loop: Expand Ratio from 0.00 to 0.60 (step 0.01)\n",
        "# ################################################################################\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# best_f1    = 0.0\n",
        "# best_ratio = 0.0\n",
        "# best_model_path = \"/content/best_efficientnetb0_expand.pth\"\n",
        "\n",
        "# for ratio_percent in range(0, 101):  # 0 → 60 inclusive\n",
        "#     expand_ratio = ratio_percent / 100.0\n",
        "#     print(f\"\\n▶ Starting expand_ratio = {expand_ratio:.2f}\")\n",
        "\n",
        "#     # (3a) Create Datasets & DataLoaders for this expand_ratio\n",
        "#     train_dataset = NorwoodCroppedExpandedDataset(\n",
        "#         raw_root=RAW_ROOT,\n",
        "#         boundary_root=BOUNDARY_ROOT,\n",
        "#         split=\"train\",\n",
        "#         level_names=LEVEL_NAMES,\n",
        "#         transform=train_transform,\n",
        "#         expand_ratio=expand_ratio\n",
        "#     )\n",
        "#     valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "#         raw_root=RAW_ROOT,\n",
        "#         boundary_root=BOUNDARY_ROOT,\n",
        "#         split=\"valid\",\n",
        "#         level_names=LEVEL_NAMES,\n",
        "#         transform=eval_transform,\n",
        "#         expand_ratio=expand_ratio\n",
        "#     )\n",
        "#     test_dataset = NorwoodCroppedExpandedDataset(\n",
        "#         raw_root=RAW_ROOT,\n",
        "#         boundary_root=BOUNDARY_ROOT,\n",
        "#         split=\"test\",\n",
        "#         level_names=LEVEL_NAMES,\n",
        "#         transform=eval_transform,\n",
        "#         expand_ratio=expand_ratio\n",
        "#     )\n",
        "\n",
        "#     train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "#                               num_workers=2, pin_memory=True)\n",
        "#     valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "#                               num_workers=2, pin_memory=True)\n",
        "#     test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "#                               num_workers=2, pin_memory=True)\n",
        "\n",
        "#     # (3b) Build a fresh EfficientNet-B0 for each ratio\n",
        "#     model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "#     in_features = model._fc.in_features\n",
        "#     model._fc = nn.Linear(in_features, len(LEVEL_NAMES))  # 6 classes\n",
        "#     model = model.to(device)\n",
        "\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "#     optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "#     # (3c) Train for 10 epochs, track best valid accuracy\n",
        "#     best_valid_acc = 0.0\n",
        "#     checkpoint_path = f\"/content/temp_efficientnetb0_expand_{ratio_percent:02d}.pth\"\n",
        "\n",
        "#     for epoch in range(1, 11):  # 10 epochs\n",
        "#         # --- Training Phase ---\n",
        "#         model.train()\n",
        "#         running_loss = 0.0\n",
        "#         running_corrects = 0\n",
        "#         total_train = 0\n",
        "\n",
        "#         for inputs, labels in train_loader:\n",
        "#             inputs = inputs.to(device)\n",
        "#             labels = labels.to(device)\n",
        "\n",
        "#             optimizer.zero_grad()\n",
        "#             outputs = model(inputs)\n",
        "#             loss = criterion(outputs, labels)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             running_loss += loss.item() * inputs.size(0)\n",
        "#             _, preds = torch.max(outputs, 1)\n",
        "#             running_corrects += torch.sum(preds == labels).item()\n",
        "#             total_train += inputs.size(0)\n",
        "\n",
        "#         epoch_acc = running_corrects / total_train\n",
        "#         print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "#         # --- Validation Phase ---\n",
        "#         model.eval()\n",
        "#         val_corrects = 0\n",
        "#         total_valid = 0\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             for inputs, labels in valid_loader:\n",
        "#                 inputs = inputs.to(device)\n",
        "#                 labels = labels.to(device)\n",
        "\n",
        "#                 outputs = model(inputs)\n",
        "#                 _, preds = torch.max(outputs, 1)\n",
        "#                 val_corrects += torch.sum(preds == labels).item()\n",
        "#                 total_valid += inputs.size(0)\n",
        "\n",
        "#         val_acc = val_corrects / total_valid\n",
        "#         print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "#         # Save checkpoint if validation improved\n",
        "#         if val_acc > best_valid_acc:\n",
        "#             best_valid_acc = val_acc\n",
        "#             torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "#     # Load the best checkpoint for test evaluation\n",
        "#     model.load_state_dict(torch.load(checkpoint_path))\n",
        "#     model.eval()\n",
        "\n",
        "#     # (3d) Evaluate on test split → compute macro F1\n",
        "#     all_preds = []\n",
        "#     all_labels = []\n",
        "#     with torch.no_grad():\n",
        "#         for inputs, labels in test_loader:\n",
        "#             inputs = inputs.to(device)\n",
        "#             labels = labels.to(device)\n",
        "#             outputs = model(inputs)\n",
        "#             _, preds = torch.max(outputs, 1)\n",
        "#             all_preds.extend(preds.cpu().tolist())\n",
        "#             all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "#     f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "#     print(f\"▶ expand_ratio={expand_ratio:.2f} → Test F1 (macro): {f1_macro:.4f}\")\n",
        "\n",
        "#     # Track best F1 and save that model\n",
        "#     if f1_macro > best_f1:\n",
        "#         best_f1 = f1_macro\n",
        "#         best_ratio = expand_ratio\n",
        "#         torch.save(model.state_dict(), best_model_path)\n",
        "#         print(f\"  → New best F1: {best_f1:.4f} at expand_ratio={best_ratio:.2f}\")\n",
        "\n",
        "#     # Clean up before next iteration\n",
        "#     del model\n",
        "#     torch.cuda.empty_cache()\n",
        "\n",
        "# ################################################################################\n",
        "# # (4) Report Best Result\n",
        "# ################################################################################\n",
        "\n",
        "# print(f\"\\n✅ Best expand_ratio: {best_ratio:.2f}  with macro F1: {best_f1:.4f}\")\n",
        "# print(f\"Saved best model weights to: {best_model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HZwpgjndXDv"
      },
      "source": [
        "efficientNetBO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3jh0lSwdbO-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "\n",
        "# --- Dataset Paths ---\n",
        "base_dir = r\"C:\\Users\\Ryan\\Desktop\\hair_fall_images_clahe\"\n",
        "train_dir = os.path.join(base_dir, \"train\")\n",
        "valid_dir = os.path.join(base_dir, \"valid\")\n",
        "test_dir  = os.path.join(base_dir, \"test\")\n",
        "\n",
        "# --- Parameters ---\n",
        "img_size = (224, 224)\n",
        "batch_size = 32\n",
        "\n",
        "# --- Data Generators ---\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    zoom_range=0.2,\n",
        "    rotation_range=15,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "test_datagen  = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(train_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
        "valid_gen = valid_datagen.flow_from_directory(valid_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
        "test_gen  = test_datagen.flow_from_directory(test_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical', shuffle=False)\n",
        "\n",
        "# --- Load EfficientNetB0 ---\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# --- Add Custom Classification Layers ---\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "predictions = Dense(train_gen.num_classes, activation='softmax')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# UnFreeze last 20 layers\n",
        "for layer in base_model.layers[-20:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# --- Compile & Train ---\n",
        "model.compile(optimizer=Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# add more weight to level 6,7\n",
        "class_weights_dict = {\n",
        "    0: 1.0,\n",
        "    1: 1.2,\n",
        "    2: 1.0,\n",
        "    3: 1.4,\n",
        "    4: 1.8,  # Level 6\n",
        "    5: 2.0   # Level 7\n",
        "}\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    patience=8,  # stop if no val improvement after 8 epochs\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# reduce_lr = ReduceLROnPlateau(\n",
        "#     patience=3,  # reduce LR if no val improvement for 3 epochs\n",
        "#     factor=0.5,  # reduce LR by half\n",
        "#     verbose=1\n",
        "# )\n",
        "\n",
        "model.fit(\n",
        "    train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    epochs=30,\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=[early_stop]\n",
        ")\n",
        "\n",
        "# --- Evaluate on Test Set ---\n",
        "loss, acc = model.evaluate(test_gen)\n",
        "print(f\"\\nTest Loss: {loss:.4f}, Accuracy: {acc:.4f}\")\n",
        "\n",
        "# --- Prediction & Reporting ---\n",
        "y_pred = model.predict(test_gen)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = test_gen.classes\n",
        "class_labels = list(test_gen.class_indices.keys())\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred_classes, target_names=class_labels))\n",
        "\n",
        "f1_macro = f1_score(y_true, y_pred_classes, average='macro')\n",
        "f1_weighted = f1_score(y_true, y_pred_classes, average='weighted')\n",
        "print(f\"Macro F1-score: {f1_macro:.4f}\")\n",
        "print(f\"Weighted F1-score: {f1_weighted:.4f}\")\n",
        "\n",
        "# --- Confusion Matrix ---\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('EfficientNetB0 - Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5My0z4wprFg9"
      },
      "source": [
        "Effeicientnetb0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjMPFRjODOWC"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build EfficientNet-B0 classifier, move to GPU\n",
        "################################################################################\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load pretrained EfficientNet-B0, replace final layer\n",
        "model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "in_features = model._fc.in_features\n",
        "model._fc = nn.Linear(in_features, len(LEVEL_NAMES))  # 6 classes\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK2hdDzFpi-q"
      },
      "source": [
        "EFFECIENTNETB0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGl9Phkwhp6o"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build EfficientNet-B0 classifier, move to GPU\n",
        "################################################################################\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load pretrained EfficientNet-B0, replace final layer\n",
        "model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "in_features = model._fc.in_features\n",
        "model._fc = nn.Linear(in_features, len(LEVEL_NAMES))  # 6 classes\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQQJS2iPcaaj"
      },
      "source": [
        "Comparasion with other models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPYMGZ7nld6t"
      },
      "source": [
        "ResNet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "55b2e6f329dd4dd7bb33a6175dd0674e",
            "e579c80be5e74ff59197ac83de7bb8cb",
            "2e23acffcd00437db07e8641759c4949",
            "b1a8c2f4fb024c79abc4eba51f3d8d0e",
            "43fcb1b22c364c748b89891857011e29",
            "9e9dfafe028e4e33b623fccfb828f019",
            "33b976d13ecc4cf1999b4cc33b7114df",
            "38049d92939545b1916e72b26b63300d",
            "dec697e9c3a3493e9db275f06b2c6bb0",
            "3b25f03278f84042a8aed2320f1761a9",
            "048a31892d2340ff846fec152bc12692",
            "f4014b7f04944ab8a51a76b6dedbf5af",
            "f165849f936c437a8959af0066f9814c",
            "f322c0123ef54d588aff5dde49ebf8e3",
            "b2f1f595119d4fec940f7a7ce0e45328",
            "d4d841d9e50f4720ab3a17548d8295ee",
            "bc5c4f26fae443958de1229d2c100d8a",
            "39e4f3661a69460ab58315c42a027257",
            "d47cdf87bc6740fba2c1043503d847ff",
            "128ed33b329f4389935dbce22aa09d29",
            "030dc819b0af410b83aa907b7defcd02",
            "74625abb9239466c95559809a23350fe",
            "ce72272810f74e6fb7975efa8f99cb96",
            "c26884c312c54ee88b24970417d55bff",
            "491fc6f9dd9a4f23bdbabb3ca486fda9",
            "2c70402c6aa048d6832c538634a30bd8",
            "37da321c152c4e3b80b8951dbf5ae661",
            "e58aeb7d2e94401e8937332a8600da9f",
            "1816f61f91c84c4c94b5940b8dc01aad",
            "9f273521e5744713a358e2f61f6f2531",
            "690b5d40d42945ba88bead3144aa5e4d",
            "3e378bfd7b59415789cf29923cb69c74",
            "50e64c611d224f26ae2a58b829068b25",
            "d476a017467b4900aa222b8d28a7828d",
            "2a31a6a494854aa09484c456a3aea269"
          ]
        },
        "id": "-yuLhcA0cZ9r",
        "outputId": "bda6a97b-95ee-46a6-f292-f2f4740d931b"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import json\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "with open(\"/content/labels.json\", \"w\") as f:\n",
        "    json.dump(LEVEL_NAMES, f)\n",
        "\n",
        "deploy_cfg = {\n",
        "    \"image_size\": [224, 224],\n",
        "    \"normalize_mean\": [0.485, 0.456, 0.406],\n",
        "    \"normalize_std\": [0.229, 0.224, 0.225],\n",
        "    \"model_name\": MODEL_NAME\n",
        "}\n",
        "with open(\"/content/preprocess.json\", \"w\") as f:\n",
        "    json.dump(deploy_cfg, f)\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build your classifier & move to GPU\n",
        "################################################################################\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "MODEL_NAME = \"resnet18\"   # ← change this to mobilenet_v2, resnet18, vgg16, or custom_cnn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if MODEL_NAME == \"efficientnet-b0\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v2\":\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet18\":\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vgg16\":\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    in_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"custom_cnn\":\n",
        "    # Define your CustomCNN somewhere above:\n",
        "    # class CustomCNN(nn.Module):\n",
        "    #     def __init__(self, num_classes): …\n",
        "    #     def forward(self, x): …\n",
        "    model = CustomCNN(num_classes=len(LEVEL_NAMES))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown MODEL_NAME: {MODEL_NAME}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# (7) Upload best model weights + metadata to Hugging Face Hub\n",
        "################################################################################\n",
        "\n",
        "!pip install -q huggingface_hub\n",
        "\n",
        "from huggingface_hub import login, create_repo, upload_folder\n",
        "\n",
        "# 🔐 Log in with your Hugging Face token (only once per session)\n",
        "login()\n",
        "\n",
        "# Set your repo ID — change this to your actual username/repo name\n",
        "hf_username = \"alamb98\"  # ← change this\n",
        "model_name = MODEL_NAME  # or mobilenetv3..., effnetb0...\n",
        "repo_id = f\"{hf_username}/{model_name}\"\n",
        "\n",
        "# Create a new model repo or skip if it exists\n",
        "create_repo(repo_id, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "# Save model weights\n",
        "torch.save(model.state_dict(), f\"{model_name}.pth\")\n",
        "\n",
        "# Save metadata\n",
        "metadata = {\n",
        "    \"model_name\": model_name,\n",
        "    \"image_size\": [224, 224],\n",
        "    \"expand_ratio\": EXPAND_RATIO,\n",
        "    \"labels\": LEVEL_NAMES,\n",
        "    \"test_accuracy\": round(test_acc, 4),\n",
        "    \"macro_f1_score\": round(test_f1_macro, 4)\n",
        "}\n",
        "with open(\"metadata.json\", \"w\") as f:\n",
        "    json.dump(metadata, f)\n",
        "\n",
        "# Upload both files to Hugging Face\n",
        "upload_folder(folder_path=\".\", repo_id=repo_id, repo_type=\"model\", allow_patterns=[\n",
        "    f\"{model_name}.pth\",\n",
        "    \"metadata.json\"\n",
        "])\n",
        "\n",
        "print(f\"\\n✅ Model uploaded to: https://huggingface.co/{repo_id}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ca6uL9jSwsV"
      },
      "source": [
        "RUN IT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "KmJkrs_1peu-",
        "outputId": "f0cec7c0-693b-4d17-c5ed-d7bfdc0d456e"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "# 0. Install requirements\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch huggingface_hub\n",
        "\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "# 1. Imports\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "import os, json, cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "from huggingface_hub import login, create_repo, upload_folder\n",
        "\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "# 2. Setup\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "MODEL_NAME = \"resnet18\"\n",
        "EXPAND_RATIO = 0.01\n",
        "hf_username = \"alamb98\"  # <- change if needed\n",
        "\n",
        "RAW_ROOT = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "SPLITS = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES = [f\"Level {i}\" for i in range(2, 8)]\n",
        "\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "# 3. Dataset\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self, raw_root, boundary_root, split, level_names, transform=None, expand_ratio=0.01):\n",
        "        self.samples = []\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "                name, ext = os.path.splitext(fname)\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "                if os.path.isfile(raw_path):\n",
        "                    self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        mask1 = cv2.inRange(hsv, (0, 100, 100), (10, 255, 255))\n",
        "        mask2 = cv2.inRange(hsv, (160, 100, 100), (180, 255, 255))\n",
        "        red_mask = cv2.bitwise_or(mask1, mask2)\n",
        "        contours, _ = cv2.findContours(red_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            x, y, w, h = np.min(all_pts[:, 0]), np.min(all_pts[:, 1]), np.ptp(all_pts[:, 0]), np.ptp(all_pts[:, 1])\n",
        "\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "        cropped = raw_img[y1:y2, x1:x2] if raw_img[y1:y2, x1:x2].size else raw_img.copy()\n",
        "\n",
        "        img_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            img_pil = self.transform(img_pil)\n",
        "        return img_pil, label\n",
        "\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "# 4. Transforms & DataLoaders\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(0.1, 0.1, 0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_set = NorwoodCroppedExpandedDataset(RAW_ROOT, BOUNDARY_ROOT, \"train\", LEVEL_NAMES, train_transform, EXPAND_RATIO)\n",
        "valid_set = NorwoodCroppedExpandedDataset(RAW_ROOT, BOUNDARY_ROOT, \"valid\", LEVEL_NAMES, eval_transform, EXPAND_RATIO)\n",
        "test_set  = NorwoodCroppedExpandedDataset(RAW_ROOT, BOUNDARY_ROOT, \"test\",  LEVEL_NAMES, eval_transform, EXPAND_RATIO)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(valid_set, batch_size=32)\n",
        "test_loader  = DataLoader(test_set,  batch_size=32)\n",
        "\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "# 5. Model Training\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, len(LEVEL_NAMES))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "for epoch in range(10):  # keep small for demo\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, labels in tqdm(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "# 6. Evaluation on Test Set\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES))\n",
        "print(f\"✅ Test Accuracy: {test_acc:.4f}, Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "# 7. Upload to Hugging Face\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "from huggingface_hub import login, create_repo, upload_folder\n",
        "\n",
        "# Login to Hugging Face (only once per session)\n",
        "login()  # You’ll be asked to paste your HF token\n",
        "\n",
        "# Define repo ID\n",
        "repo_id = f\"{hf_username}/{MODEL_NAME}\"\n",
        "\n",
        "# Create repo if it doesn't exist\n",
        "create_repo(repo_id, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "# Save model weights\n",
        "torch.save(model.state_dict(), f\"{MODEL_NAME}.pth\")\n",
        "\n",
        "# Save metadata\n",
        "metadata = {\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"expand_ratio\": EXPAND_RATIO,\n",
        "    \"image_size\": [224, 224],\n",
        "    \"labels\": LEVEL_NAMES,\n",
        "    \"test_accuracy\": round(test_acc, 4),\n",
        "    \"macro_f1_score\": round(test_f1_macro, 4)\n",
        "}\n",
        "with open(\"metadata.json\", \"w\") as f:\n",
        "    json.dump(metadata, f)\n",
        "\n",
        "# Upload model and metadata\n",
        "upload_folder(\n",
        "    folder_path=\".\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\",\n",
        "    allow_patterns=[f\"{MODEL_NAME}.pth\", \"metadata.json\"]\n",
        ")\n",
        "\n",
        "print(f\"\\n✅ Model uploaded to: https://huggingface.co/{repo_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rYMDUBGlTXe"
      },
      "source": [
        "mobilenet_v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ndvzg-qYkz6M",
        "outputId": "6f1e98a9-61a4-40ef-8760-25e6e3b5ed8b"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import json\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "with open(\"/content/labels.json\", \"w\") as f:\n",
        "    json.dump(LEVEL_NAMES, f)\n",
        "\n",
        "deploy_cfg = {\n",
        "    \"image_size\": [224, 224],\n",
        "    \"normalize_mean\": [0.485, 0.456, 0.406],\n",
        "    \"normalize_std\": [0.229, 0.224, 0.225],\n",
        "    \"model_name\": MODEL_NAME\n",
        "}\n",
        "with open(\"/content/preprocess.json\", \"w\") as f:\n",
        "    json.dump(deploy_cfg, f)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build your classifier & move to GPU\n",
        "################################################################################\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "MODEL_NAME = \"mobilenet_v2\"   # ← change this to mobilenet_v2, resnet18, vgg16, or custom_cnn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if MODEL_NAME == \"efficientnet-b0\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v2\":\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet18\":\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vgg16\":\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    in_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"custom_cnn\":\n",
        "    # Define your CustomCNN somewhere above:\n",
        "    # class CustomCNN(nn.Module):\n",
        "    #     def __init__(self, num_classes): …\n",
        "    #     def forward(self, x): …\n",
        "    model = CustomCNN(num_classes=len(LEVEL_NAMES))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown MODEL_NAME: {MODEL_NAME}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "## best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_mobilenet_v2_expand001.pth\"\n",
        "\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5vddSu1lO-C"
      },
      "source": [
        "vgg16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouCXSHrVk93T"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build your classifier & move to GPU\n",
        "################################################################################\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "MODEL_NAME = \"vgg16\"   # ← change this to mobilenet_v2, resnet18, vgg16, or custom_cnn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if MODEL_NAME == \"efficientnet-b0\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v2\":\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet18\":\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vgg16\":\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    in_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"custom_cnn\":\n",
        "    # Define your CustomCNN somewhere above:\n",
        "    # class CustomCNN(nn.Module):\n",
        "    #     def __init__(self, num_classes): …\n",
        "    #     def forward(self, x): …\n",
        "    model = CustomCNN(num_classes=len(LEVEL_NAMES))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown MODEL_NAME: {MODEL_NAME}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmDPYr8DnW2K"
      },
      "source": [
        "effecientnet b0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2wfMIWGOnIGK",
        "outputId": "981ee91f-bc8c-44f7-b637-54050cce812f"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build your classifier & move to GPU\n",
        "################################################################################\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "# You can now set MODEL_NAME to one of:\n",
        "# \"efficientnet-b0\", \"efficientnet-b2\",\n",
        "# \"mobilenet_v2\", \"mobilenet_v3_large\",\n",
        "# \"resnet18\", \"resnet34\", \"resnet50\",\n",
        "# \"resnext50_32x4d\",\n",
        "# \"vgg16\",\n",
        "# \"densenet121\",\n",
        "# \"shufflenet_v2_x1_0\",\n",
        "# \"convnext_tiny\",\n",
        "# \"vit_b_16\",\n",
        "# \"swin_tiny_patch4_window7_224\",\n",
        "# or \"custom_cnn\"\n",
        "\n",
        "MODEL_NAME = \"efficientnet-b0\"   # ← change as desired\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if MODEL_NAME == \"efficientnet-b0\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"efficientnet-b2\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v2\":\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v3_large\":\n",
        "    model = models.mobilenet_v3_large(pretrained=True)\n",
        "    in_features = model.classifier[3].in_features\n",
        "    model.classifier[3] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet18\":\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet34\":\n",
        "    model = models.resnet34(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet50\":\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnext50_32x4d\":\n",
        "    model = models.resnext50_32x4d(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vgg16\":\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    in_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"densenet121\":\n",
        "    model = models.densenet121(pretrained=True)\n",
        "    in_features = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"shufflenet_v2_x1_0\":\n",
        "    model = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"convnext_tiny\":\n",
        "    model = models.convnext_tiny(pretrained=True)\n",
        "    in_features = model.classifier[2].in_features\n",
        "    model.classifier[2] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vit_b_16\":\n",
        "    model = models.vit_b_16(pretrained=True)\n",
        "    in_features = model.heads.head.in_features\n",
        "    model.heads.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"swin_tiny_patch4_window7_224\":\n",
        "    model = models.swin_t(pretrained=True)\n",
        "    in_features = model.head.in_features\n",
        "    model.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"custom_cnn\":\n",
        "    # Make sure you have defined CustomCNN somewhere above:\n",
        "    # class CustomCNN(nn.Module):\n",
        "    #     def __init__(self, num_classes): …\n",
        "    #     def forward(self, x): …\n",
        "    model = CustomCNN(num_classes=len(LEVEL_NAMES))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown MODEL_NAME: {MODEL_NAME}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197,
          "referenced_widgets": [
            "4e73577f142b4abba9f5f242ffa04d99",
            "ab9df7184f9d4ef0891d6a1aeefd534f",
            "50ce5572c8e24a25b5be8ca4bcf8c3ba",
            "2382c53eee6241ddb6039a8dc8b1fe4c",
            "c730a45d27704e00a2a87f7ea30323c4",
            "5d99d9b066ff4cdeb007d98b70368c0b",
            "c56c50d4b97346a4b17a7786f01a59db",
            "053f7812e3a14f1eae6762629835f624",
            "3dcfb9d923e6461b82719dbfdcd48747",
            "669fb11cb724446990b9394a28b6deb2",
            "028ac75023f24cc88993dc7b1177c08f",
            "441c0431704f41f8bf89e092180a69bf",
            "f47ac73baea642549dbafee5f4212607",
            "3049829651814c45951bb0cb889cc280",
            "25cfa84cc88f4400982255dc21059f94",
            "9bb1db85d930439789026854be8bf656",
            "dd55b87d468442c890a3942ed914c5fa",
            "f33b6fec8c444c38b51d0be21162140c",
            "9fb1d080fc6f4210b0dd6b56a799a839",
            "6d1cceebff77421da9171af587e5bb92",
            "255ffb9efcc042b99a2197864f3b4a7e",
            "39d2a5f78e9e4eebbdc69b9c1a313f93",
            "036eebc17d3042eab30113c74ba97e81",
            "0172a41630864ffcb0e9252eb2d6248f",
            "eba0fa1932d549beb037df7e10444cb4",
            "85427a41d3ed46a7860cde4c7fc75abe",
            "4b3f5b0576ee4beba97baf04ab4f9ef1",
            "1f3ba421d43a4a9ea67d8e77455a13b4",
            "db40513ae30f4966961fb109aa064c0d",
            "fd23641e80f046108169bbf66dfa86cf",
            "42e1eaee788e49e78ad6cb4636422443",
            "701a6c57af6d40598e022be19c49a038",
            "e7f8ab59c56e497791b7ec0b0339b4bc",
            "919c718171594ef9b471595764e24f05",
            "32ac2c5f1e294ba3b3aba773ac4ed87c"
          ]
        },
        "id": "Z_HDMxAuZ5r-",
        "outputId": "b2076b0e-6525-44cd-ea70-ca6b732668f4"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "# Upload trained EfficientNet Norwood classifier to Hugging Face\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "!pip install --quiet huggingface_hub\n",
        "\n",
        "from huggingface_hub import login, create_repo, upload_folder\n",
        "import json\n",
        "\n",
        "# (A) Login with your Hugging Face token (only once per session)\n",
        "login()  # Paste your token when prompted\n",
        "\n",
        "# (B) Set repo name (change if needed)\n",
        "hf_username = \"alamb98\"  # 🔁 Your HF username\n",
        "repo_id = f\"{hf_username}/{MODEL_NAME}_norwood_classifier\"\n",
        "\n",
        "# (C) Create or reuse existing repo\n",
        "create_repo(repo_id, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "# (D) Save model checkpoint & metadata\n",
        "model_path = f\"{MODEL_NAME}_best.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "metadata = {\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"expand_ratio\": EXPAND_RATIO,\n",
        "    \"num_classes\": len(LEVEL_NAMES),\n",
        "    \"class_names\": LEVEL_NAMES,\n",
        "    \"test_accuracy\": round(float(test_acc), 4),\n",
        "    \"macro_f1_score\": round(float(test_f1_macro), 4),\n",
        "    \"architecture\": \"PyTorch\",\n",
        "    \"framework\": \"torchvision.models\",\n",
        "    \"description\": (\n",
        "        \"EfficientNet-based Norwood scale classifier trained on cropped male scalp \"\n",
        "        \"images from multiple angles (front, back, left, right, top-down). \"\n",
        "        \"Crops are obtained using segmentation boundaries and expanded by \"\n",
        "        f\"{EXPAND_RATIO*100:.1f}%.\"\n",
        "    )\n",
        "}\n",
        "with open(\"metadata.json\", \"w\") as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "# (E) Upload both model and metadata.json\n",
        "upload_folder(\n",
        "    folder_path=\".\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\",\n",
        "    allow_patterns=[model_path, \"metadata.json\"]\n",
        ")\n",
        "\n",
        "print(f\"\\n✅ Model & metadata uploaded: https://huggingface.co/{repo_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axdl53uxn4eT"
      },
      "source": [
        "mobilenet_v3_large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IgR1Zg5hnxuO",
        "outputId": "9eaf623a-e55f-4e78-e6fe-8abafb5c3823"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build your classifier & move to GPU\n",
        "################################################################################\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "# You can now set MODEL_NAME to one of:\n",
        "# \"efficientnet-b0\", \"efficientnet-b2\",\n",
        "# \"mobilenet_v2\", \"mobilenet_v3_large\",\n",
        "# \"resnet18\", \"resnet34\", \"resnet50\",\n",
        "# \"resnext50_32x4d\",\n",
        "# \"vgg16\",\n",
        "# \"densenet121\",\n",
        "# \"shufflenet_v2_x1_0\",\n",
        "# \"convnext_tiny\",\n",
        "# \"vit_b_16\",\n",
        "# \"swin_tiny_patch4_window7_224\",\n",
        "# or \"custom_cnn\"\n",
        "\n",
        "MODEL_NAME = \"mobilenet_v3_large\"   # ← change as desired\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if MODEL_NAME == \"efficientnet-b0\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"efficientnet-b2\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v2\":\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v3_large\":\n",
        "    model = models.mobilenet_v3_large(pretrained=True)\n",
        "    in_features = model.classifier[3].in_features\n",
        "    model.classifier[3] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet18\":\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet34\":\n",
        "    model = models.resnet34(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet50\":\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnext50_32x4d\":\n",
        "    model = models.resnext50_32x4d(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vgg16\":\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    in_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"densenet121\":\n",
        "    model = models.densenet121(pretrained=True)\n",
        "    in_features = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"shufflenet_v2_x1_0\":\n",
        "    model = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"convnext_tiny\":\n",
        "    model = models.convnext_tiny(pretrained=True)\n",
        "    in_features = model.classifier[2].in_features\n",
        "    model.classifier[2] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vit_b_16\":\n",
        "    model = models.vit_b_16(pretrained=True)\n",
        "    in_features = model.heads.head.in_features\n",
        "    model.heads.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"swin_tiny_patch4_window7_224\":\n",
        "    model = models.swin_t(pretrained=True)\n",
        "    in_features = model.head.in_features\n",
        "    model.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"custom_cnn\":\n",
        "    # Make sure you have defined CustomCNN somewhere above:\n",
        "    # class CustomCNN(nn.Module):\n",
        "    #     def __init__(self, num_classes): …\n",
        "    #     def forward(self, x): …\n",
        "    model = CustomCNN(num_classes=len(LEVEL_NAMES))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown MODEL_NAME: {MODEL_NAME}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169,
          "referenced_widgets": [
            "034ad1f086334200bc793ccd27796796",
            "9bcd5353852f43a4a70299227d42eedf",
            "c2d09d44b3b44ed6ad39f748ff7d94c5",
            "f575e212d1a44d15ab77ec08d45ad4bd",
            "cb2f9c815bfc474a8504477f12dc1d9b",
            "3e373320964a4d1da85380fdb28a019c",
            "0086f32407ae4af48e79aff5e5db3804",
            "bd07bae1f1664153b6cda5929fc36c9b",
            "2d7d896f82604054b0037d4bb502f691",
            "17c7c858039245a4ba1f3c2f866b48be",
            "f6933959d0544a83ae35994cf299a169",
            "8ad7dbeeeb8943f59c012ff5e46f784b",
            "dfb21724818b46a0826c1ea3f6a9217a",
            "ec58321961e34a0a8aa121b520135aef",
            "4bf4c53dba72468ba9cbca573c30f6ba",
            "e65e29ba548940a6a8d2b8cddae5acaa",
            "1f0fafd3fa9b4cea84823716835c679e",
            "2c89be3bc8214895bb54c300c2e91d2b",
            "65f5ec376a164b4c982c3879801bcf88",
            "6c6f73850cca44fb81b14ef0f220ddf9",
            "cb871555a9694bda925a8c34714c9545",
            "2c5d24e4b9ab47078ae7a0d7ef55ab5b",
            "50e998199cc244d593e097d5389d8d05",
            "2a64148a14404abc973b21287c28815a",
            "26be810c2f8b47b39263c862b44d58f4",
            "cf6f78d6e58e48059e75e2df19399c1a",
            "c912ad05683349dc84d2c7d457ae967b",
            "c19998596f5045c7b893d3f749d6883d",
            "0de8c3f756f041eb9ec016083d8addfe",
            "c99b3f0c4fe74268912b7ae250db62cd",
            "4aa69667a56945d99a0d63daa9ace205",
            "52fc0e54eabd4d5e9438ea1af691e502",
            "38bd19899cbc4c408112a9d1e2dde545",
            "c2b6df7428714a4d9c4ddc2d1a4061ef",
            "16bfe4754e82469b9e0ec1b6d7cf490d"
          ]
        },
        "id": "RKE7EORnhXAC",
        "outputId": "a64b402d-2cf3-4fb2-bf92-fe8c7023cabf"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "# Upload trained model checkpoint and metadata to Hugging Face\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "!pip install --quiet huggingface_hub\n",
        "\n",
        "from huggingface_hub import login, create_repo, upload_folder\n",
        "import json\n",
        "\n",
        "# (A) Login with your token (only once per session)\n",
        "login()  # Paste your Hugging Face token when prompted\n",
        "\n",
        "# (B) Set repo name (adjust if needed)\n",
        "hf_username = \"alamb98\"  # 🔁 Your HF username\n",
        "repo_id = f\"{hf_username}/{MODEL_NAME}_cropped_clahe_norwood_classifier\"\n",
        "\n",
        "# (C) Create or use existing repo\n",
        "create_repo(repo_id, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "# (D) Save model checkpoint and metadata\n",
        "model_path = f\"{MODEL_NAME}_cropped_clahe_best.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "metadata = {\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"expand_ratio\": float(EXPAND_RATIO),\n",
        "    \"num_classes\": len(LEVEL_NAMES),\n",
        "    \"class_names\": LEVEL_NAMES,\n",
        "    \"test_accuracy\": round(float(test_acc), 4),\n",
        "    \"macro_f1_score\": round(float(test_f1_macro), 4),\n",
        "    \"architecture\": MODEL_NAME,\n",
        "    \"framework\": \"PyTorch\",\n",
        "    \"trained_on\": \"cropped scalp images from combined orig + CLAHE raw & segmented datasets\",\n",
        "    \"description\": (\n",
        "        f\"{MODEL_NAME} classifier for Norwood stages (2–7) using cropped hair \"\n",
        "        \"region based on segmented boundaries and raw images.\"\n",
        "    )\n",
        "}\n",
        "with open(\"metadata.json\", \"w\") as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "# (E) Upload both model and metadata.json\n",
        "upload_folder(\n",
        "    folder_path=\".\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\",\n",
        "    allow_patterns=[model_path, \"metadata.json\"]\n",
        ")\n",
        "\n",
        "print(f\"\\n✅ Uploaded to: https://huggingface.co/{repo_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0chw-Uh0n-29"
      },
      "source": [
        "resnet34"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtTnQQXhn6Io"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build your classifier & move to GPU\n",
        "################################################################################\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "# You can now set MODEL_NAME to one of:\n",
        "# \"efficientnet-b0\", \"efficientnet-b2\",\n",
        "# \"mobilenet_v2\", \"mobilenet_v3_large\",\n",
        "# \"resnet18\", \"resnet34\", \"resnet50\",\n",
        "# \"resnext50_32x4d\",\n",
        "# \"vgg16\",\n",
        "# \"densenet121\",\n",
        "# \"shufflenet_v2_x1_0\",\n",
        "# \"convnext_tiny\",\n",
        "# \"vit_b_16\",\n",
        "# \"swin_tiny_patch4_window7_224\",\n",
        "# or \"custom_cnn\"\n",
        "\n",
        "MODEL_NAME = \"resnet34\"   # ← change as desired\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if MODEL_NAME == \"efficientnet-b0\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"efficientnet-b2\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v2\":\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v3_large\":\n",
        "    model = models.mobilenet_v3_large(pretrained=True)\n",
        "    in_features = model.classifier[3].in_features\n",
        "    model.classifier[3] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet18\":\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet34\":\n",
        "    model = models.resnet34(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet50\":\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnext50_32x4d\":\n",
        "    model = models.resnext50_32x4d(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vgg16\":\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    in_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"densenet121\":\n",
        "    model = models.densenet121(pretrained=True)\n",
        "    in_features = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"shufflenet_v2_x1_0\":\n",
        "    model = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"convnext_tiny\":\n",
        "    model = models.convnext_tiny(pretrained=True)\n",
        "    in_features = model.classifier[2].in_features\n",
        "    model.classifier[2] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vit_b_16\":\n",
        "    model = models.vit_b_16(pretrained=True)\n",
        "    in_features = model.heads.head.in_features\n",
        "    model.heads.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"swin_tiny_patch4_window7_224\":\n",
        "    model = models.swin_t(pretrained=True)\n",
        "    in_features = model.head.in_features\n",
        "    model.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"custom_cnn\":\n",
        "    # Make sure you have defined CustomCNN somewhere above:\n",
        "    # class CustomCNN(nn.Module):\n",
        "    #     def __init__(self, num_classes): …\n",
        "    #     def forward(self, x): …\n",
        "    model = CustomCNN(num_classes=len(LEVEL_NAMES))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown MODEL_NAME: {MODEL_NAME}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GwQfeRioJkx"
      },
      "source": [
        "resnet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kH7V_b_CoFyt"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build your classifier & move to GPU\n",
        "################################################################################\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "# You can now set MODEL_NAME to one of:\n",
        "# \"efficientnet-b0\", \"efficientnet-b2\",\n",
        "# \"mobilenet_v2\", \"mobilenet_v3_large\",\n",
        "# \"resnet18\", \"resnet34\", \"resnet50\",\n",
        "# \"resnext50_32x4d\",\n",
        "# \"vgg16\",\n",
        "# \"densenet121\",\n",
        "# \"shufflenet_v2_x1_0\",\n",
        "# \"convnext_tiny\",\n",
        "# \"vit_b_16\",\n",
        "# \"swin_tiny_patch4_window7_224\",\n",
        "# or \"custom_cnn\"\n",
        "\n",
        "MODEL_NAME = \"resnet50\"   # ← change as desired\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if MODEL_NAME == \"efficientnet-b0\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"efficientnet-b2\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v2\":\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v3_large\":\n",
        "    model = models.mobilenet_v3_large(pretrained=True)\n",
        "    in_features = model.classifier[3].in_features\n",
        "    model.classifier[3] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet18\":\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet34\":\n",
        "    model = models.resnet34(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet50\":\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnext50_32x4d\":\n",
        "    model = models.resnext50_32x4d(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vgg16\":\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    in_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"densenet121\":\n",
        "    model = models.densenet121(pretrained=True)\n",
        "    in_features = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"shufflenet_v2_x1_0\":\n",
        "    model = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"convnext_tiny\":\n",
        "    model = models.convnext_tiny(pretrained=True)\n",
        "    in_features = model.classifier[2].in_features\n",
        "    model.classifier[2] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vit_b_16\":\n",
        "    model = models.vit_b_16(pretrained=True)\n",
        "    in_features = model.heads.head.in_features\n",
        "    model.heads.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"swin_tiny_patch4_window7_224\":\n",
        "    model = models.swin_t(pretrained=True)\n",
        "    in_features = model.head.in_features\n",
        "    model.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"custom_cnn\":\n",
        "    # Make sure you have defined CustomCNN somewhere above:\n",
        "    # class CustomCNN(nn.Module):\n",
        "    #     def __init__(self, num_classes): …\n",
        "    #     def forward(self, x): …\n",
        "    model = CustomCNN(num_classes=len(LEVEL_NAMES))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown MODEL_NAME: {MODEL_NAME}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vNnZtoQoRS1"
      },
      "source": [
        "resnext50_32x4d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPKjGVd6oNO_"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build your classifier & move to GPU\n",
        "################################################################################\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "# You can now set MODEL_NAME to one of:\n",
        "# \"efficientnet-b0\", \"efficientnet-b2\",\n",
        "# \"mobilenet_v2\", \"mobilenet_v3_large\",\n",
        "# \"resnet18\", \"resnet34\", \"resnet50\",\n",
        "# \"resnext50_32x4d\",\n",
        "# \"vgg16\",\n",
        "# \"densenet121\",\n",
        "# \"shufflenet_v2_x1_0\",\n",
        "# \"convnext_tiny\",\n",
        "# \"vit_b_16\",\n",
        "# \"swin_tiny_patch4_window7_224\",\n",
        "# or \"custom_cnn\"\n",
        "\n",
        "MODEL_NAME = \"resnext50_32x4d\"   # ← change as desired\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if MODEL_NAME == \"efficientnet-b0\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"efficientnet-b2\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v2\":\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v3_large\":\n",
        "    model = models.mobilenet_v3_large(pretrained=True)\n",
        "    in_features = model.classifier[3].in_features\n",
        "    model.classifier[3] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet18\":\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet34\":\n",
        "    model = models.resnet34(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet50\":\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnext50_32x4d\":\n",
        "    model = models.resnext50_32x4d(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vgg16\":\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    in_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"densenet121\":\n",
        "    model = models.densenet121(pretrained=True)\n",
        "    in_features = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"shufflenet_v2_x1_0\":\n",
        "    model = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"convnext_tiny\":\n",
        "    model = models.convnext_tiny(pretrained=True)\n",
        "    in_features = model.classifier[2].in_features\n",
        "    model.classifier[2] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vit_b_16\":\n",
        "    model = models.vit_b_16(pretrained=True)\n",
        "    in_features = model.heads.head.in_features\n",
        "    model.heads.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"swin_tiny_patch4_window7_224\":\n",
        "    model = models.swin_t(pretrained=True)\n",
        "    in_features = model.head.in_features\n",
        "    model.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"custom_cnn\":\n",
        "    # Make sure you have defined CustomCNN somewhere above:\n",
        "    # class CustomCNN(nn.Module):\n",
        "    #     def __init__(self, num_classes): …\n",
        "    #     def forward(self, x): …\n",
        "    model = CustomCNN(num_classes=len(LEVEL_NAMES))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown MODEL_NAME: {MODEL_NAME}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guxKl3YJocFY"
      },
      "source": [
        "densenet121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mjQG5Reoodjq",
        "outputId": "d9217ab0-5928-466b-d9f1-47fa41455754"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build your classifier & move to GPU\n",
        "################################################################################\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "# You can now set MODEL_NAME to one of:\n",
        "# \"efficientnet-b0\", \"efficientnet-b2\",\n",
        "# \"mobilenet_v2\", \"mobilenet_v3_large\",\n",
        "# \"resnet18\", \"resnet34\", \"resnet50\",\n",
        "# \"resnext50_32x4d\",\n",
        "# \"vgg16\",\n",
        "# \"densenet121\",\n",
        "# \"shufflenet_v2_x1_0\",\n",
        "# \"convnext_tiny\",\n",
        "# \"vit_b_16\",\n",
        "# \"swin_tiny_patch4_window7_224\",\n",
        "# or \"custom_cnn\"\n",
        "\n",
        "MODEL_NAME = \"densenet121\"   # ← change as desired\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if MODEL_NAME == \"efficientnet-b0\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"efficientnet-b2\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v2\":\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v3_large\":\n",
        "    model = models.mobilenet_v3_large(pretrained=True)\n",
        "    in_features = model.classifier[3].in_features\n",
        "    model.classifier[3] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet18\":\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet34\":\n",
        "    model = models.resnet34(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet50\":\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnext50_32x4d\":\n",
        "    model = models.resnext50_32x4d(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vgg16\":\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    in_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"densenet121\":\n",
        "    model = models.densenet121(pretrained=True)\n",
        "    in_features = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"shufflenet_v2_x1_0\":\n",
        "    model = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"convnext_tiny\":\n",
        "    model = models.convnext_tiny(pretrained=True)\n",
        "    in_features = model.classifier[2].in_features\n",
        "    model.classifier[2] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vit_b_16\":\n",
        "    model = models.vit_b_16(pretrained=True)\n",
        "    in_features = model.heads.head.in_features\n",
        "    model.heads.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"swin_tiny_patch4_window7_224\":\n",
        "    model = models.swin_t(pretrained=True)\n",
        "    in_features = model.head.in_features\n",
        "    model.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"custom_cnn\":\n",
        "    # Make sure you have defined CustomCNN somewhere above:\n",
        "    # class CustomCNN(nn.Module):\n",
        "    #     def __init__(self, num_classes): …\n",
        "    #     def forward(self, x): …\n",
        "    model = CustomCNN(num_classes=len(LEVEL_NAMES))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown MODEL_NAME: {MODEL_NAME}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "47186eb3f9e64734965b1503daf3bac8",
            "3d798016f5c8491080174e5454336b39",
            "6485321ab55843bd9c70a2c40e3f8c52",
            "bceecfb9889d49bb89f94e5bc83cb6bf",
            "78f8168bad5d4265bd75c19e77da4255",
            "f44ebda2e48846f0908b4a78110e8c66",
            "09be5211d09c46f3aa9bfbb339d6abca",
            "4fe6dfddc15642298345eebd476bcadd",
            "d44f4f2c73ea45fcae527fb2ecd9c348",
            "91c555e8b3c34b209786dde10052fc97",
            "5cf81a9e7f204dbe98c6033a12c055a5",
            "7516284f683940219b6ac2f7448dff7b",
            "3cfeda64c5864633b1431c8a69ec7576",
            "cdb2832cbd104911861f7c704445ab43",
            "ec9260de7187424d987343f6ff2a291c",
            "b70b2f9f77bb4273b5378692559df3cc",
            "d9af9466b5974cd8a6a3ab86ce16e343",
            "e8f34cb2e4194abbb9d44c2c2d29c703",
            "f4c3d54403584c3ba4b697e2e3b10cf6",
            "26260a46c4584c90af7ea5857f55300d",
            "1cdf43a09a68434da1605911122f5891",
            "089d7b0841b540cbbbc5b47d0896de64",
            "d73c0a76ccd448349983edf62d524976",
            "6d662ffc0b894a8297db3b936c996e35",
            "fda3f086741b4b01bdd6626085f69108",
            "5e44168e7be04c07a619815aaab92bb2",
            "03f8c0f1232741a794e8ac3e63edfbe5",
            "666a4e1924e7480298e86549f56655b7",
            "6c83884f93b04a44a92c5892030adcef",
            "9c3fbd69e3d04e138468e7e72ee0d727",
            "248057b78ea544079a83614f931658ad",
            "9b78abe5fca547ee8773c8bbb31086fe",
            "a76d69b64dba4538936a01b90148c375",
            "da096eb330e14e7c9ada93773a83176e",
            "f37b481087554f44a6552c07a2eb4d74",
            "0168d60fb4b440ec8e58cdd56d79f62d",
            "71b170336d6643f2ac876240a45ddc1c",
            "62be97d1eb414b66b68a908d01d141cb",
            "491fb8c59a9b44019c7bea0563a27de4",
            "b6857e635f27449f9baedaa94885b1e7",
            "3569099dbdef45e48f58eca091d6d9a4",
            "e21393a709f947889c3bdc44630bc452",
            "b79b6f20e49547569dc46827ae8b521f",
            "207dbc983ee544498c30175a1d2c9f4c",
            "091c73a8430e486691552b0a0673420a",
            "55299c772c594ac894616a47917a6b6f",
            "85a01d3131f64653a6817af9b8c1e028",
            "7c3be50ee85244bb87425662432891c2",
            "382e39221d4d448d8aade1ba9fa90d62",
            "53944f045a644539935b14ebffec014b",
            "91e25d63304d46958ec3b95d211946a7",
            "300f7ef89bc640368879db7aaf65f99b",
            "d23ae5987d0f46f0918647f5f5016e20"
          ]
        },
        "id": "iNXS23vRVA_T",
        "outputId": "dcab3a42-d8dc-4b4c-cea1-f4828ce45090"
      },
      "outputs": [],
      "source": [
        "# # ───────────────────────────────────────────────────────────────────────────────\n",
        "# # 7. Upload to Hugging Face\n",
        "# # ───────────────────────────────────────────────────────────────────────────────\n",
        "# # Make sure you've run the evaluation block so test_acc and test_f1_macro exist.\n",
        "\n",
        "# import json  # not imported earlier\n",
        "# from huggingface_hub import login, create_repo, upload_folder\n",
        "\n",
        "# hf_username = \"alamb98\"  # <-- CHANGE THIS\n",
        "\n",
        "# # Login to Hugging Face (only once per session). Will prompt for token in Colab.\n",
        "# login()\n",
        "\n",
        "# # Define repo ID as \"<username>/<MODEL_NAME>\"\n",
        "# repo_id = f\"{hf_username}/{MODEL_NAME}\"\n",
        "\n",
        "# # Create repo if it doesn't exist\n",
        "# create_repo(repo_id, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "# # Save model weights (state_dict only)\n",
        "# torch.save(model.state_dict(), f\"{MODEL_NAME}.pth\")\n",
        "\n",
        "# # Save metadata\n",
        "# metadata = {\n",
        "#     \"model_name\": MODEL_NAME,\n",
        "#     \"expand_ratio\": EXPAND_RATIO,\n",
        "#     \"image_size\": [224, 224],\n",
        "#     \"labels\": LEVEL_NAMES,\n",
        "#     \"test_accuracy\": round(float(test_acc), 4),\n",
        "#     \"macro_f1_score\": round(float(test_f1_macro), 4)\n",
        "# }\n",
        "# with open(\"metadata.json\", \"w\") as f:\n",
        "#     json.dump(metadata, f, indent=2)\n",
        "\n",
        "# # Upload model and metadata files\n",
        "# upload_folder(\n",
        "#     folder_path=\".\",\n",
        "#     repo_id=repo_id,\n",
        "#     repo_type=\"model\",\n",
        "#     allow_patterns=[f\"{MODEL_NAME}.pth\", \"metadata.json\"]\n",
        "# )\n",
        "\n",
        "# print(f\"\\n✅ Model uploaded to: https://huggingface.co/{repo_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsjWKJJnol38"
      },
      "source": [
        "shufflenet_v2_x1_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H39cTcVromtV"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build your classifier & move to GPU\n",
        "################################################################################\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "# You can now set MODEL_NAME to one of:\n",
        "# \"efficientnet-b0\", \"efficientnet-b2\",\n",
        "# \"mobilenet_v2\", \"mobilenet_v3_large\",\n",
        "# \"resnet18\", \"resnet34\", \"resnet50\",\n",
        "# \"resnext50_32x4d\",\n",
        "# \"vgg16\",\n",
        "# \"densenet121\",\n",
        "# \"shufflenet_v2_x1_0\",\n",
        "# \"convnext_tiny\",\n",
        "# \"vit_b_16\",\n",
        "# \"swin_tiny_patch4_window7_224\",\n",
        "# or \"custom_cnn\"\n",
        "\n",
        "MODEL_NAME = \"shufflenet_v2_x1_0\"   # ← change as desired\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if MODEL_NAME == \"efficientnet-b0\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"efficientnet-b2\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v2\":\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v3_large\":\n",
        "    model = models.mobilenet_v3_large(pretrained=True)\n",
        "    in_features = model.classifier[3].in_features\n",
        "    model.classifier[3] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet18\":\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet34\":\n",
        "    model = models.resnet34(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet50\":\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnext50_32x4d\":\n",
        "    model = models.resnext50_32x4d(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vgg16\":\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    in_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"densenet121\":\n",
        "    model = models.densenet121(pretrained=True)\n",
        "    in_features = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"shufflenet_v2_x1_0\":\n",
        "    model = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"convnext_tiny\":\n",
        "    model = models.convnext_tiny(pretrained=True)\n",
        "    in_features = model.classifier[2].in_features\n",
        "    model.classifier[2] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vit_b_16\":\n",
        "    model = models.vit_b_16(pretrained=True)\n",
        "    in_features = model.heads.head.in_features\n",
        "    model.heads.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"swin_tiny_patch4_window7_224\":\n",
        "    model = models.swin_t(pretrained=True)\n",
        "    in_features = model.head.in_features\n",
        "    model.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"custom_cnn\":\n",
        "    # Make sure you have defined CustomCNN somewhere above:\n",
        "    # class CustomCNN(nn.Module):\n",
        "    #     def __init__(self, num_classes): …\n",
        "    #     def forward(self, x): …\n",
        "    model = CustomCNN(num_classes=len(LEVEL_NAMES))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown MODEL_NAME: {MODEL_NAME}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7J4v3WMo872"
      },
      "source": [
        "convnext_tiny"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0R7rlX8ho9rY",
        "outputId": "22bdf76c-883c-4af8-9c1f-330d2e5a5c58"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build your classifier & move to GPU\n",
        "################################################################################\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "# You can now set MODEL_NAME to one of:\n",
        "# \"efficientnet-b0\", \"efficientnet-b2\",\n",
        "# \"mobilenet_v2\", \"mobilenet_v3_large\",\n",
        "# \"resnet18\", \"resnet34\", \"resnet50\",\n",
        "# \"resnext50_32x4d\",\n",
        "# \"vgg16\",\n",
        "# \"densenet121\",\n",
        "# \"shufflenet_v2_x1_0\",\n",
        "# \"convnext_tiny\",\n",
        "# \"vit_b_16\",\n",
        "# \"swin_tiny_patch4_window7_224\",\n",
        "# or \"custom_cnn\"\n",
        "\n",
        "MODEL_NAME = \"convnext_tiny\"   # ← change as desired\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if MODEL_NAME == \"efficientnet-b0\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"efficientnet-b2\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v2\":\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v3_large\":\n",
        "    model = models.mobilenet_v3_large(pretrained=True)\n",
        "    in_features = model.classifier[3].in_features\n",
        "    model.classifier[3] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet18\":\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet34\":\n",
        "    model = models.resnet34(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet50\":\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnext50_32x4d\":\n",
        "    model = models.resnext50_32x4d(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vgg16\":\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    in_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"densenet121\":\n",
        "    model = models.densenet121(pretrained=True)\n",
        "    in_features = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"shufflenet_v2_x1_0\":\n",
        "    model = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"convnext_tiny\":\n",
        "    model = models.convnext_tiny(pretrained=True)\n",
        "    in_features = model.classifier[2].in_features\n",
        "    model.classifier[2] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vit_b_16\":\n",
        "    model = models.vit_b_16(pretrained=True)\n",
        "    in_features = model.heads.head.in_features\n",
        "    model.heads.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"swin_tiny_patch4_window7_224\":\n",
        "    model = models.swin_t(pretrained=True)\n",
        "    in_features = model.head.in_features\n",
        "    model.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"custom_cnn\":\n",
        "    # Make sure you have defined CustomCNN somewhere above:\n",
        "    # class CustomCNN(nn.Module):\n",
        "    #     def __init__(self, num_classes): …\n",
        "    #     def forward(self, x): …\n",
        "    model = CustomCNN(num_classes=len(LEVEL_NAMES))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown MODEL_NAME: {MODEL_NAME}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n",
        "\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "# Upload trained model checkpoint and metadata to Hugging Face\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "!pip install --quiet huggingface_hub\n",
        "\n",
        "from huggingface_hub import login, create_repo, upload_folder\n",
        "import json\n",
        "import torch\n",
        "\n",
        "# (A) Login with your token (only once per session)\n",
        "login()  # Paste your Hugging Face token when prompted\n",
        "\n",
        "# (B) Set repo name (adjust if needed)\n",
        "hf_username = \"alamb98\"  # 🔁 Your HF username\n",
        "repo_id = f\"{hf_username}/{MODEL_NAME}_clahe_norwood_classifier\"\n",
        "\n",
        "# (C) Create or use existing repo\n",
        "create_repo(repo_id, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "# (D) Save model checkpoint and metadata\n",
        "model_path = f\"{MODEL_NAME}_clahe_best.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "metadata = {\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"expand_ratio\": EXPAND_RATIO,\n",
        "    \"num_classes\": len(LEVEL_NAMES),\n",
        "    \"class_names\": LEVEL_NAMES,\n",
        "    \"test_accuracy\": round(float(test_acc), 4),\n",
        "    \"macro_f1_score\": round(float(test_f1_macro), 4),\n",
        "    \"architecture\": \"torchvision.models\",\n",
        "    \"framework\": \"PyTorch\",\n",
        "    \"trained_on\": \"combined raw + CLAHE + segmented scalp images\",\n",
        "    \"description\": \"ConvNeXt-Tiny classifier for male pattern baldness (Norwood stages 2–7)\"\n",
        "}\n",
        "with open(\"metadata.json\", \"w\") as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "# (E) Upload both model and metadata.json\n",
        "upload_folder(\n",
        "    folder_path=\".\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\",\n",
        "    allow_patterns=[model_path, \"metadata.json\"]\n",
        ")\n",
        "\n",
        "print(f\"\\n✅ Uploaded to: https://huggingface.co/{repo_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169,
          "referenced_widgets": [
            "29a9040263584f5fb32c3a41e61d2df8",
            "ac0d6439339e45ed9a904c4a996a0092",
            "002f10dd430748748e26d1ac4d2e0dc5",
            "b11418d389f14f61835ca88981efe14e",
            "41648911f2354641bd50058ea6293a01",
            "d4a19ec480d640da85e373529d9dbfce",
            "dcda11f9da084871a50a60ff96c390a8",
            "1d8637001d234cf3ac601f6c1222f479",
            "a63a93a2d1bc4253a5766c56a54c9c27",
            "c01841a3885543e199a96f72d2d9b3a3",
            "fcb6f5628847461b8fe20e3b8715b830",
            "269c55c3ce224bceb370ebfa1ddc4b10",
            "5a61a1bad9fb4bb2804b6d4615c8fcc5",
            "5b596c1de94243e498c5a92125afd9ff",
            "35acc52314504223b7e352b9e7da2bf9",
            "d59d32b6f6d04ecc9332073f376dada3",
            "3106f90866484f9bb11b102c920815b8",
            "ef9109f280d044f09a98bf6cc3d99413",
            "2bed2aff430b4afbaeb0644ee37cbfe2",
            "3add284622fa4af4bb4f34becf96a579",
            "2b01f693bc0643979791c115a3c23ee8",
            "2719564fdb754508aa2c89e0b4551543",
            "2d7440d3548843b88ca4045760af103a",
            "7f4e3d51b7df40e898a81eddef78ac67",
            "8fc0bb64d856494782f64779250a89af",
            "bbb3e6d4c06d4e96b6c854937e679791",
            "bcc8ac5e0d774af2afe06ce7b61c5b9a",
            "bbabf8d873374ee48fb8af159c95885b",
            "aab8f90650fe4b7dab655a7bfcc0f604",
            "ddf570c99bdd4e34a8694003e888246f",
            "99c0d062ad3e4dca902dbbad07566614",
            "01ee4c7e72de4974b01f1147840b1826",
            "b9974637402149d1879b39da44de3b58",
            "f533200ed0734d47b41778921b5750ba",
            "e8a742327d88496db8b7da507b911249"
          ]
        },
        "id": "_bqZeeWeLUM2",
        "outputId": "abacd709-a686-4e56-dfbc-87fd013c6dcd"
      },
      "outputs": [],
      "source": [
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "# Upload trained model checkpoint and metadata to Hugging Face\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "!pip install --quiet huggingface_hub\n",
        "\n",
        "from huggingface_hub import login, create_repo, upload_folder\n",
        "import json\n",
        "import torch\n",
        "\n",
        "# (A) Login with your token (only once per session)\n",
        "login()  # Paste your Hugging Face token when prompted\n",
        "\n",
        "# (B) Set repo name (adjust if needed)\n",
        "hf_username = \"alamb98\"  # 🔁 Your HF username\n",
        "repo_id = f\"{hf_username}/{MODEL_NAME}_clahe_norwood_classifier\"\n",
        "\n",
        "# (C) Create or use existing repo\n",
        "create_repo(repo_id, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "# (D) Save model checkpoint and metadata\n",
        "model_path = f\"{MODEL_NAME}_clahe_best.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "metadata = {\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"expand_ratio\": EXPAND_RATIO,\n",
        "    \"num_classes\": len(LEVEL_NAMES),\n",
        "    \"class_names\": LEVEL_NAMES,\n",
        "    \"test_accuracy\": round(float(test_acc), 4),\n",
        "    \"macro_f1_score\": round(float(test_f1_macro), 4),\n",
        "    \"architecture\": \"torchvision.models\",\n",
        "    \"framework\": \"PyTorch\",\n",
        "    \"trained_on\": \"combined raw + CLAHE + segmented scalp images\",\n",
        "    \"description\": \"ConvNeXt-Tiny classifier for male pattern baldness (Norwood stages 2–7)\"\n",
        "}\n",
        "with open(\"metadata.json\", \"w\") as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "# (E) Upload both model and metadata.json\n",
        "upload_folder(\n",
        "    folder_path=\".\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\",\n",
        "    allow_patterns=[model_path, \"metadata.json\"]\n",
        ")\n",
        "\n",
        "print(f\"\\n✅ Uploaded to: https://huggingface.co/{repo_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlpfIBRepD22"
      },
      "source": [
        "vit_b_16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tv08XV-9pEqx"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build your classifier & move to GPU\n",
        "################################################################################\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "# You can now set MODEL_NAME to one of:\n",
        "# \"efficientnet-b0\", \"efficientnet-b2\",\n",
        "# \"mobilenet_v2\", \"mobilenet_v3_large\",\n",
        "# \"resnet18\", \"resnet34\", \"resnet50\",\n",
        "# \"resnext50_32x4d\",\n",
        "# \"vgg16\",\n",
        "# \"densenet121\",\n",
        "# \"shufflenet_v2_x1_0\",\n",
        "# \"convnext_tiny\",\n",
        "# \"vit_b_16\",\n",
        "# \"swin_tiny_patch4_window7_224\",\n",
        "# or \"custom_cnn\"\n",
        "\n",
        "MODEL_NAME = \"vit_b_16\"   # ← change as desired\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if MODEL_NAME == \"efficientnet-b0\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"efficientnet-b2\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v2\":\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v3_large\":\n",
        "    model = models.mobilenet_v3_large(pretrained=True)\n",
        "    in_features = model.classifier[3].in_features\n",
        "    model.classifier[3] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet18\":\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet34\":\n",
        "    model = models.resnet34(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet50\":\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnext50_32x4d\":\n",
        "    model = models.resnext50_32x4d(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vgg16\":\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    in_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"densenet121\":\n",
        "    model = models.densenet121(pretrained=True)\n",
        "    in_features = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"shufflenet_v2_x1_0\":\n",
        "    model = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"convnext_tiny\":\n",
        "    model = models.convnext_tiny(pretrained=True)\n",
        "    in_features = model.classifier[2].in_features\n",
        "    model.classifier[2] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vit_b_16\":\n",
        "    model = models.vit_b_16(pretrained=True)\n",
        "    in_features = model.heads.head.in_features\n",
        "    model.heads.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"swin_tiny_patch4_window7_224\":\n",
        "    model = models.swin_t(pretrained=True)\n",
        "    in_features = model.head.in_features\n",
        "    model.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"custom_cnn\":\n",
        "    # Make sure you have defined CustomCNN somewhere above:\n",
        "    # class CustomCNN(nn.Module):\n",
        "    #     def __init__(self, num_classes): …\n",
        "    #     def forward(self, x): …\n",
        "    model = CustomCNN(num_classes=len(LEVEL_NAMES))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown MODEL_NAME: {MODEL_NAME}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\"x\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQTRUiF1pL0N"
      },
      "source": [
        "swin_tiny_patch4_window7_224"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rwa_9cGGpMri"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build your classifier & move to GPU\n",
        "################################################################################\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "# You can now set MODEL_NAME to one of:\n",
        "# \"efficientnet-b0\", \"efficientnet-b2\",\n",
        "# \"mobilenet_v2\", \"mobilenet_v3_large\",\n",
        "# \"resnet18\", \"resnet34\", \"resnet50\",\n",
        "# \"resnext50_32x4d\",\n",
        "# \"vgg16\",\n",
        "# \"densenet121\",\n",
        "# \"shufflenet_v2_x1_0\",\n",
        "# \"convnext_tiny\",\n",
        "# \"vit_b_16\",\n",
        "# \"swin_tiny_patch4_window7_224\",\n",
        "# or \"custom_cnn\"\n",
        "\n",
        "MODEL_NAME = \"swin_tiny_patch4_window7_224\"   # ← change as desired\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if MODEL_NAME == \"efficientnet-b0\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"efficientnet-b2\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v2\":\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v3_large\":\n",
        "    model = models.mobilenet_v3_large(pretrained=True)\n",
        "    in_features = model.classifier[3].in_features\n",
        "    model.classifier[3] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet18\":\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet34\":\n",
        "    model = models.resnet34(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet50\":\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnext50_32x4d\":\n",
        "    model = models.resnext50_32x4d(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vgg16\":\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    in_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"densenet121\":\n",
        "    model = models.densenet121(pretrained=True)\n",
        "    in_features = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"shufflenet_v2_x1_0\":\n",
        "    model = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"convnext_tiny\":\n",
        "    model = models.convnext_tiny(pretrained=True)\n",
        "    in_features = model.classifier[2].in_features\n",
        "    model.classifier[2] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vit_b_16\":\n",
        "    model = models.vit_b_16(pretrained=True)\n",
        "    in_features = model.heads.head.in_features\n",
        "    model.heads.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"swin_tiny_patch4_window7_224\":\n",
        "    model = models.swin_t(pretrained=True)\n",
        "    in_features = model.head.in_features\n",
        "    model.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"custom_cnn\":\n",
        "    # Make sure you have defined CustomCNN somewhere above:\n",
        "    # class CustomCNN(nn.Module):\n",
        "    #     def __init__(self, num_classes): …\n",
        "    #     def forward(self, x): …\n",
        "    model = CustomCNN(num_classes=len(LEVEL_NAMES))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown MODEL_NAME: {MODEL_NAME}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrGBF9T9xTFV"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsJNWuKyxTly"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build your classifier & move to GPU\n",
        "################################################################################\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "# You can now set MODEL_NAME to one of:\n",
        "# \"efficientnet-b0\", \"efficientnet-b2\",\n",
        "# \"mobilenet_v2\", \"mobilenet_v3_large\",\n",
        "# \"resnet18\", \"resnet34\", \"resnet50\",\n",
        "# \"resnext50_32x4d\",\n",
        "# \"vgg16\",\n",
        "# \"densenet121\",\n",
        "# \"shufflenet_v2_x1_0\",\n",
        "# \"convnext_tiny\",\n",
        "# \"vit_b_16\",\n",
        "# \"swin_tiny_patch4_window7_224\",\n",
        "# or \"custom_cnn\"\n",
        "\n",
        "MODEL_NAME = \"efficientnet-b2\"   # ← change as desired\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if MODEL_NAME == \"efficientnet-b0\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"efficientnet-b2\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v2\":\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v3_large\":\n",
        "    model = models.mobilenet_v3_large(pretrained=True)\n",
        "    in_features = model.classifier[3].in_features\n",
        "    model.classifier[3] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet18\":\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet34\":\n",
        "    model = models.resnet34(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet50\":\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnext50_32x4d\":\n",
        "    model = models.resnext50_32x4d(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vgg16\":\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    in_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"densenet121\":\n",
        "    model = models.densenet121(pretrained=True)\n",
        "    in_features = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"shufflenet_v2_x1_0\":\n",
        "    model = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"convnext_tiny\":\n",
        "    model = models.convnext_tiny(pretrained=True)\n",
        "    in_features = model.classifier[2].in_features\n",
        "    model.classifier[2] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vit_b_16\":\n",
        "    model = models.vit_b_16(pretrained=True)\n",
        "    in_features = model.heads.head.in_features\n",
        "    model.heads.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"swin_tiny_patch4_window7_224\":\n",
        "    model = models.swin_t(pretrained=True)\n",
        "    in_features = model.head.in_features\n",
        "    model.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"custom_cnn\":\n",
        "    # Make sure you have defined CustomCNN somewhere above:\n",
        "    # class CustomCNN(nn.Module):\n",
        "    #     def __init__(self, num_classes): …\n",
        "    #     def forward(self, x): …\n",
        "    model = CustomCNN(num_classes=len(LEVEL_NAMES))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown MODEL_NAME: {MODEL_NAME}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20aRVKMlkodl"
      },
      "source": [
        "### GRAPHS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3l9H89Y9krpo"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Prepare your data\n",
        "data = {\n",
        "    \"model\": [\n",
        "        \"EfficientNet-B0\", \"ResNet18\", \"MobileNetV2\", \"VGG16\",\n",
        "        \"MobileNetV3-Large\", \"ResNet34\", \"ResNet50\", \"ResNeXt50_32x4d\",\n",
        "        \"DenseNet121\", \"ShuffleNetV2_x1_0\", \"ConvNeXt-Tiny\", \"ViT-B/16\", \"Swin-Tiny\"\n",
        "    ],\n",
        "    \"f1_score\": [\n",
        "        0.75, 0.76, 0.73, 0.70,\n",
        "        0.75, 0.71, 0.72, 0.74,\n",
        "        0.74, 0.68, 0.75, 0.70, 0.72\n",
        "    ],\n",
        "    \"test_accuracy\": [\n",
        "        0.86, 0.86, 0.82, 0.80,\n",
        "        0.85, 0.84, 0.81, 0.86,\n",
        "        0.84, 0.82, 0.85, 0.81, 0.86\n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 2) Sort by descending F1 so highest is on the left\n",
        "df_sorted = df.sort_values(by=\"f1_score\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "# 3) Create bar chart\n",
        "x = np.arange(len(df_sorted))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 7))\n",
        "bars_f1 = ax.bar(x - width/2, df_sorted[\"f1_score\"], width, label='Macro F1 Score')\n",
        "bars_acc = ax.bar(x + width/2, df_sorted[\"test_accuracy\"], width, label='Test Accuracy')\n",
        "\n",
        "# 4) Annotate each bar with its value\n",
        "for bar in bars_f1:\n",
        "    h = bar.get_height()\n",
        "    ax.annotate(f\"{h:.2f}\",\n",
        "                xy=(bar.get_x() + bar.get_width()/2, h),\n",
        "                xytext=(0, 3),\n",
        "                textcoords=\"offset points\",\n",
        "                ha='center', va='bottom')\n",
        "\n",
        "for bar in bars_acc:\n",
        "    h = bar.get_height()\n",
        "    ax.annotate(f\"{h:.2f}\",\n",
        "                xy=(bar.get_x() + bar.get_width()/2, h),\n",
        "                xytext=(0, 3),\n",
        "                textcoords=\"offset points\",\n",
        "                ha='center', va='bottom')\n",
        "\n",
        "# 5) Formatting\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(df_sorted[\"model\"], rotation=45, ha='right')\n",
        "ax.set_xlabel('Model')\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Macro F1 Score and Test Accuracy by Model (Sorted by the Macro F1 Scrore - Left to right)')\n",
        "\n",
        "# Place legend outside to the right\n",
        "ax.legend(loc='upper left', bbox_to_anchor=(1.02, 1), borderaxespad=0)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 0.85, 1])  # leave space on the right for legend\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCxnHZSu-9L_"
      },
      "source": [
        "# HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Zs-CTFclin4",
        "outputId": "d5231785-c396-4f7c-f491-e106e38df32f"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKRyeL3zg9KW"
      },
      "source": [
        "resnet18 input with huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be76c17d",
        "outputId": "6171ee3d-a94d-46b0-ee24-1303f0c2662b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted: Level 5\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet torch torchvision Pillow huggingface_hub\n",
        "\n",
        "import json, torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# --- download weights + metadata from your HF repo ---\n",
        "state_dict_path = hf_hub_download(\"alamb98/resnet18\", filename=\"resnet18.pth\")\n",
        "meta_path       = hf_hub_download(\"alamb98/resnet18\", filename=\"metadata.json\")\n",
        "\n",
        "with open(meta_path, \"r\") as f:\n",
        "    meta = json.load(f)\n",
        "LEVEL_NAMES = meta.get(\"labels\", [f\"Level {i}\" for i in range(2, 8)])\n",
        "\n",
        "# --- build model the same way you trained it ---\n",
        "model = models.resnet18(weights=None)           # no built-in weights\n",
        "in_features = model.fc.in_features\n",
        "model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "model.load_state_dict(torch.load(state_dict_path, map_location=\"cpu\"))\n",
        "model.eval()\n",
        "\n",
        "# --- preprocessing identical to your eval_transform ---\n",
        "tfm = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# LOCAL path → open directly\n",
        "image_path = \"/content/10-Front_jpg.rf.11094538dd1409e0db0d7d91baa661b4.jpg\"\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "x = tfm(image).unsqueeze(0)  # (1,3,224,224)\n",
        "with torch.no_grad():\n",
        "    logits = model(x)\n",
        "pred_idx = logits.argmax(-1).item()\n",
        "print(\"Predicted:\", LEVEL_NAMES[pred_idx])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8g6LNqS0-yL"
      },
      "source": [
        "mobilenet_v3_large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226,
          "referenced_widgets": [
            "428a076fbed5463587662d9dd470e23e",
            "e9ac64047ce44a59b57babcf95abe13f",
            "e60897b477e6460d96609136bafd445a",
            "672dadbc846c4f95b0aa4af5d9b529d4",
            "bf60893451f2487abd1eb6ec810b9a34",
            "68f69bca68e74fc1b6b4ca44522c1129",
            "f361ce53cfd84a859d1107212088bb12",
            "c8a4e87781a744578d129ab0be2f7cfa",
            "a0b9c40dcd254fff802708fa00dcd954",
            "65ab429d7a514d6fa10b4bddf05d3704",
            "3c00484debea455db1bcd01d917776ac",
            "b9392e24db88439f8b677263d2797e83",
            "734d5361b28449898e6df8904b6c18fe",
            "5e876cb107a5434fa71ed4bfacdba859",
            "fd6133b5ef9340358df785f35ff85dc3",
            "66e63e0cb17049e580037b2af3eb22ae",
            "5c35423555ae4209b5a803693d93e23f",
            "a8eb50a96c2c47f7a999637e5adb22e1",
            "73574a7cbd8a4b418099318935f6340f",
            "1b26deed0ee442b4bdd770df2b531965",
            "c93250d3ce29458ba706e777ecdd641d",
            "b1f897fc3b554d35a5335211cf9a4038"
          ]
        },
        "id": "wiOkjuRV0_LM",
        "outputId": "e7f396af-7f98-401c-d46e-c4db142c16f9"
      },
      "outputs": [],
      "source": [
        "#!pip install --quiet torch torchvision Pillow huggingface_hub\n",
        "\n",
        "import json, torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# --- repo + filenames (adjust if your HF names differ) ---\n",
        "REPO         = \"alamb98/mobilenet_v3_large_cropped_clahe_norwood_classifier\"\n",
        "WEIGHTS_FILE = \"mobilenet_v3_large_cropped_clahe_best.pth\"   # <- exact filename in your HF repo\n",
        "META_FILE    = \"metadata.json\"            # <- contains labels/img_size/mean/std/label_base (if present)\n",
        "\n",
        "# --- download weights + metadata from your HF repo ---\n",
        "state_dict_path = hf_hub_download(REPO, filename=WEIGHTS_FILE)\n",
        "meta_path       = hf_hub_download(REPO, filename=META_FILE)\n",
        "\n",
        "with open(meta_path, \"r\") as f:\n",
        "    meta = json.load(f)\n",
        "\n",
        "LEVEL_NAMES = meta.get(\"labels\", [f\"Level {i}\" for i in range(2, 8)])\n",
        "IMG_SIZE    = int(meta.get(\"img_size\", 224))\n",
        "MEAN        = meta.get(\"mean\", [0.485, 0.456, 0.406])\n",
        "STD         = meta.get(\"std\",  [0.229, 0.224, 0.225])\n",
        "LABEL_BASE  = int(meta.get(\"label_base\", 0))  # 0 => logits 0..6 so +1; 1 => already 1..7\n",
        "\n",
        "# --- build model the same way you trained it ---\n",
        "model = models.mobilenet_v3_large(weights=None)   # no built-in pretrained weights\n",
        "in_features = model.classifier[3].in_features\n",
        "model.classifier[3] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "model.load_state_dict(torch.load(state_dict_path, map_location=\"cpu\"), strict=True)\n",
        "model.eval()\n",
        "\n",
        "# --- preprocessing identical to your eval transform ---\n",
        "tfm = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=MEAN, std=STD),\n",
        "])\n",
        "\n",
        "# --- predict helper (takes a PIL RGB image) ---\n",
        "def predict_mobilenet_v3(pil_rgb_image):\n",
        "    x = tfm(pil_rgb_image).unsqueeze(0)  # (1,3,H,W)\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)\n",
        "        pred_idx = int(logits.argmax(-1).item())\n",
        "        level = pred_idx + 1 if LABEL_BASE == 0 else pred_idx\n",
        "    # Return both numeric level and friendly label if available\n",
        "    label_str = LEVEL_NAMES[level - 1] if 1 <= level <= len(LEVEL_NAMES) else f\"Level {level}\"\n",
        "    return level, label_str\n",
        "\n",
        "# --- EXAMPLE: local path → open and run ---\n",
        "image_path = \"/content/11-Front_jpg.rf.6c9a7e546c587b57b46feae8c6f3276f.jpg\"   # <-- change this\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "level, label_str = predict_mobilenet_v3(image)\n",
        "print(f\"Predicted: {label_str} (level={level})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW6Mx5kA1XvL"
      },
      "source": [
        "effecientnetb0 with huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXyUAA4R1aYc",
        "outputId": "e3802a16-7752-4346-a036-2b81b5ede0fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted: Level 6 (level=5)\n"
          ]
        }
      ],
      "source": [
        "#!pip install --quiet torch torchvision Pillow huggingface_hub efficientnet_pytorch\n",
        "\n",
        "import json, torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from huggingface_hub import hf_hub_download\n",
        "from efficientnet_pytorch import EfficientNet  # pip install efficientnet_pytorch\n",
        "\n",
        "# --- repo + filenames (adjust if your HF names differ) ---\n",
        "REPO         = \"alamb98/efficientnet-b0_norwood_classifier\"\n",
        "WEIGHTS_FILE = \"efficientnet-b0_best.pth\"   # <- exact filename in your HF repo\n",
        "META_FILE    = \"metadata.json\"         # <- contains labels/img_size/mean/std/label_base (if present)\n",
        "\n",
        "# --- download weights + metadata ---\n",
        "state_dict_path = hf_hub_download(REPO, filename=WEIGHTS_FILE)\n",
        "meta_path       = hf_hub_download(REPO, filename=META_FILE)\n",
        "\n",
        "with open(meta_path, \"r\") as f:\n",
        "    meta = json.load(f)\n",
        "\n",
        "LEVEL_NAMES = meta.get(\"labels\", [f\"Level {i}\" for i in range(2, 8)])  # default: Level 2..7\n",
        "IMG_SIZE    = int(meta.get(\"img_size\", 224))\n",
        "MEAN        = meta.get(\"mean\", [0.485, 0.456, 0.406])\n",
        "STD         = meta.get(\"std\",  [0.229, 0.224, 0.225])\n",
        "LABEL_BASE  = int(meta.get(\"label_base\", 0))  # 0 => logits 0..6 so +1; 1 => already 1..7\n",
        "\n",
        "# --- build model exactly like training (no built-in pretrained weights) ---\n",
        "model = EfficientNet.from_name(\"efficientnet-b0\")  # no pretrained\n",
        "in_features = model._fc.in_features\n",
        "model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "# load trained weights (handles common wrapper keys/prefixes)\n",
        "state = torch.load(state_dict_path, map_location=\"cpu\")\n",
        "if isinstance(state, dict) and \"state_dict\" in state and isinstance(state[\"state_dict\"], dict):\n",
        "    state = state[\"state_dict\"]\n",
        "fixed = {}\n",
        "if isinstance(state, dict):\n",
        "    for k, v in state.items():\n",
        "        if isinstance(k, str) and k.startswith(\"module.\"):\n",
        "            k = k[len(\"module.\"):]\n",
        "        if isinstance(k, str) and k.startswith(\"model.\"):\n",
        "            k = k[len(\"model.\"):]\n",
        "        fixed[k] = v\n",
        "else:\n",
        "    fixed = state\n",
        "model.load_state_dict(fixed, strict=True)\n",
        "model.eval()\n",
        "\n",
        "# --- preprocessing identical to your eval transform (strict resize, no center-crop) ---\n",
        "tfm = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=MEAN, std=STD),\n",
        "])\n",
        "\n",
        "# --- predict helper (takes a PIL RGB image) ---\n",
        "def predict_efficientnet_b0(pil_rgb_image):\n",
        "    x = tfm(pil_rgb_image).unsqueeze(0)  # (1,3,H,W)\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)\n",
        "        pred_idx = int(logits.argmax(-1).item())\n",
        "        level = pred_idx + 1 if LABEL_BASE == 0 else pred_idx\n",
        "    label_str = LEVEL_NAMES[level - 1] if 1 <= level <= len(LEVEL_NAMES) else f\"Level {level}\"\n",
        "    return level, label_str\n",
        "\n",
        "# --- EXAMPLE: local path → open and run ---\n",
        "image_path = \"/content/10-Front_jpg.rf.11094538dd1409e0db0d7d91baa661b4.jpg\"   # <-- change this\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "level, label_str = predict_efficientnet_b0(image)\n",
        "print(f\"Predicted: {label_str} (level={level})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbflaG_-1hpT"
      },
      "source": [
        "convnext_tiny with hugginface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iy_pKWE01iI9",
        "outputId": "27022ea3-9444-474d-8bbc-6cafaaf2a828"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted: Level 6 (level=5)\n"
          ]
        }
      ],
      "source": [
        "#!pip install --quiet torch torchvision Pillow huggingface_hub\n",
        "\n",
        "import json, torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from huggingface_hub import hf_hub_download\n",
        "from PIL import Image\n",
        "\n",
        "# --- repo + filenames (adjust if your HF names differ) ---\n",
        "REPO         = \"alamb98/convnext_tiny_clahe_norwood_classifier\"\n",
        "WEIGHTS_FILE = \"convnext_tiny_clahe_best.pth\"   # exact filename in your repo\n",
        "META_FILE    = \"metadata.json\"                  # contains labels/img_size/mean/std/label_base\n",
        "\n",
        "# --- download weights + metadata ---\n",
        "state_dict_path = hf_hub_download(REPO, filename=WEIGHTS_FILE)\n",
        "meta_path       = hf_hub_download(REPO, filename=META_FILE)\n",
        "\n",
        "with open(meta_path, \"r\") as f:\n",
        "    meta = json.load(f)\n",
        "\n",
        "# Allow either \"labels\" or \"class_names\"\n",
        "LEVEL_NAMES = meta.get(\"labels\") or meta.get(\"class_names\") or [f\"Level {i}\" for i in range(2, 8)]\n",
        "IMG_SIZE    = int(meta.get(\"img_size\", 224))\n",
        "MEAN        = meta.get(\"mean\", [0.485, 0.456, 0.406])\n",
        "STD         = meta.get(\"std\",  [0.229, 0.224, 0.225])\n",
        "LABEL_BASE  = int(meta.get(\"label_base\", 0))  # 0 => logits 0..6 → add +1; 1 => already 1..7\n",
        "\n",
        "# --- build architecture exactly like training (no built-in weights) ---\n",
        "model = models.convnext_tiny(weights=None)\n",
        "in_features = model.classifier[2].in_features\n",
        "model.classifier[2] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "# load trained weights (unwrap common wrappers and strip prefixes)\n",
        "state = torch.load(state_dict_path, map_location=\"cpu\")\n",
        "if isinstance(state, dict) and \"state_dict\" in state and isinstance(state[\"state_dict\"], dict):\n",
        "    state = state[\"state_dict\"]\n",
        "fixed = {}\n",
        "if isinstance(state, dict):\n",
        "    for k, v in state.items():\n",
        "        if isinstance(k, str) and k.startswith(\"module.\"):\n",
        "            k = k[len(\"module.\"):]\n",
        "        if isinstance(k, str) and k.startswith(\"model.\"):\n",
        "            k = k[len(\"model.\"):]\n",
        "        fixed[k] = v\n",
        "else:\n",
        "    fixed = state\n",
        "model.load_state_dict(fixed, strict=True)\n",
        "model.eval()\n",
        "\n",
        "# --- preprocessing identical to your eval transform (strict resize, no center-crop) ---\n",
        "tfm = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=MEAN, std=STD),\n",
        "])\n",
        "\n",
        "# --- predict helper (takes a PIL RGB image) ---\n",
        "def predict_convnext_tiny(pil_rgb_image: Image.Image):\n",
        "    \"\"\"\n",
        "    Returns (level_int, label_str).\n",
        "    - If LABEL_BASE == 0: logits are 0..6 → add +1 to map to 1..7\n",
        "    - If LABEL_BASE == 1: logits already represent 1..7\n",
        "    \"\"\"\n",
        "    x = tfm(pil_rgb_image).unsqueeze(0)  # (1,3,H,W)\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)\n",
        "        pred_idx = int(logits.argmax(-1).item())\n",
        "        level = pred_idx + 1 if LABEL_BASE == 0 else pred_idx\n",
        "    # best-effort label string from metadata\n",
        "    if 1 <= level <= len(LEVEL_NAMES):\n",
        "        label_str = LEVEL_NAMES[level - 1]  # assumes labels are ordered Level 2..7\n",
        "    else:\n",
        "        label_str = f\"Level {level}\"\n",
        "    return level, label_str\n",
        "\n",
        "# --- EXAMPLE: local path → open and run ---\n",
        "if __name__ == \"__main__\":\n",
        "    image_path = \"/content/10-Front_jpg.rf.11094538dd1409e0db0d7d91baa661b4.jpg\"  # <-- change this\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    level, label_str = predict_convnext_tiny(img)\n",
        "    print(f\"Predicted: {label_str} (level={level})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCd8fYkzI6-8"
      },
      "source": [
        "XGBoost huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Afgl9riEI-Zz",
        "outputId": "f80e5708-5804-4054-f204-0105493f8da4"
      },
      "outputs": [],
      "source": [
        "# === Install deps (Colab-safe) ===\n",
        "!pip install -q pandas joblib huggingface_hub xgboost\n",
        "\n",
        "# === Imports ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# === Hugging Face repo + model artifact ===\n",
        "REPO_ID = \"alamb98/xgboost_hair_fall_classifier\"\n",
        "FILENAME = \"xgboost_hair_fall_classifier.joblib\"\n",
        "\n",
        "# === EXACT feature order expected by the trained model ===\n",
        "# (Taken from your mismatch error — do NOT change the order or names)\n",
        "FEATURE_COLUMNS = [\n",
        "    \"Do you stay up late at night?_Yes\",\n",
        "    \"Do you think that in your area water is a reason behind hair fall problems?_Yes\",\n",
        "    \"Is there anyone in your family having a hair fall problem or a baldness issue?_Yes\",\n",
        "    \"Do you use chemicals, hair gel, or color in your hair?_Yes\",\n",
        "    \"Do you have too much stress_Yes\",\n",
        "    \"Did you face any type of chronic illness in the past?_Yes\",\n",
        "    \"What is your age ?\"\n",
        "]\n",
        "\n",
        "def _to01(v):\n",
        "    \"\"\"Map various yes/no-like values to 1/0.\"\"\"\n",
        "    v = str(v).strip().lower()\n",
        "    return 1 if v in (\"yes\", \"y\", \"1\", \"true\", \"t\") else 0\n",
        "\n",
        "def form_to_features(form: dict) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Convert a simple dict form into the exact DataFrame shape/order your model expects.\n",
        "    form keys (your UI can name them anything):\n",
        "        - stay_up_late         -> Yes/No\n",
        "        - water_reason         -> Yes/No\n",
        "        - family_history       -> Yes/No\n",
        "        - use_chemicals        -> Yes/No\n",
        "        - stress               -> Yes/No\n",
        "        - chronic_illness      -> Yes/No\n",
        "        - age                  -> number\n",
        "    \"\"\"\n",
        "    row = {\n",
        "        \"Do you stay up late at night?_Yes\": _to01(form.get(\"stay_up_late\", \"No\")),\n",
        "        \"Do you think that in your area water is a reason behind hair fall problems?_Yes\": _to01(form.get(\"water_reason\", \"No\")),\n",
        "        \"Is there anyone in your family having a hair fall problem or a baldness issue?_Yes\": _to01(form.get(\"family_history\", \"No\")),\n",
        "        \"Do you use chemicals, hair gel, or color in your hair?_Yes\": _to01(form.get(\"use_chemicals\", \"No\")),\n",
        "        \"Do you have too much stress_Yes\": _to01(form.get(\"stress\", \"No\")),\n",
        "        \"Did you face any type of chronic illness in the past?_Yes\": _to01(form.get(\"chronic_illness\", \"No\")),\n",
        "        \"What is your age ?\": float(form.get(\"age\", 0)),\n",
        "    }\n",
        "    # Ensure column ORDER matches training exactly\n",
        "    return pd.DataFrame([row], columns=FEATURE_COLUMNS)\n",
        "\n",
        "# === Load the model from Hugging Face Hub ===\n",
        "model_path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\n",
        "xgb_model = joblib.load(model_path)\n",
        "\n",
        "def predict_from_form(form: dict):\n",
        "    \"\"\"\n",
        "    Returns dict with predicted class (0/1), label text, and probability for 'Yes'.\n",
        "    \"\"\"\n",
        "    X = form_to_features(form)\n",
        "    # XGBoost prefers float inputs\n",
        "    X = X.astype(float)\n",
        "\n",
        "    # Predict class and probability\n",
        "    y_pred = xgb_model.predict(X)[0]\n",
        "    # Some XGB versions use predict_proba; fall back if unavailable\n",
        "    if hasattr(xgb_model, \"predict_proba\"):\n",
        "        proba_yes = float(xgb_model.predict_proba(X)[0][1])\n",
        "    else:\n",
        "        # If model doesn't expose predict_proba, approximate via decision_function/sigmoid\n",
        "        # but typically XGBClassifier has predict_proba.\n",
        "        proba_yes = np.nan\n",
        "\n",
        "    label_map = {0: \"No hair fall\", 1: \"Yes hair fall\"}\n",
        "    return {\n",
        "        \"pred_class\": int(y_pred),\n",
        "        \"pred_label\": label_map.get(int(y_pred), str(y_pred)),\n",
        "        \"prob_yes\": proba_yes\n",
        "    }\n",
        "\n",
        "# === Example usage ===\n",
        "example_form = {\n",
        "    \"stay_up_late\": \"No\",\n",
        "    \"water_reason\": \"Yes\",\n",
        "    \"family_history\": \"Yes\",\n",
        "    \"use_chemicals\": \"No\",\n",
        "    \"stress\": \"Yes\",\n",
        "    \"chronic_illness\": \"No\",\n",
        "    \"age\": 27\n",
        "}\n",
        "\n",
        "result = predict_from_form(example_form)\n",
        "print(\"Prediction:\", result[\"pred_label\"])\n",
        "print(\"Probability (Yes):\", round(result[\"prob_yes\"], 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwFMkKLtFUIq"
      },
      "source": [
        "SEGMENTATION huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olwKD76nFWNN",
        "outputId": "aef050b8-7170-4ff6-c3a9-2d4a65fc216d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prob stats  min/max/mean: 3.3787836972720697e-09 0.9997944235801697 0.22628255188465118\n",
            "Saved: /content/mask.png and /content/boundary.png\n",
            "Unique mask values: [  0 255]\n",
            "Hair pixels ratio: 0.22526611328125\n"
          ]
        }
      ],
      "source": [
        "# !pip install --quiet torch torchvision opencv-python pillow huggingface_hub\n",
        "\n",
        "import torch, numpy as np, cv2\n",
        "from PIL import Image\n",
        "from huggingface_hub import hf_hub_download\n",
        "from torchvision import transforms\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50, deeplabv3_resnet101\n",
        "\n",
        "# ===== CONFIG =====\n",
        "REPO_ID   = \"alamb98/deeplabv3-hair-segmentation\"\n",
        "CKPT_NAME = \"deeplabv3_final.pth\"\n",
        "\n",
        "IMAGE_PATH   = \"/content/29-Front_jpg.rf.3048eecf9b365863f1c21f3562f4a778.jpg\"   # <-- change this\n",
        "OUTPUT_MASK  = \"/content/mask.png\"\n",
        "OUTPUT_BOUND = \"/content/boundary.png\"\n",
        "\n",
        "# Try your actual backbone; if you trained on resnet101, change here\n",
        "BACKBONE    = \"resnet50\"      # \"resnet50\" or \"resnet101\"\n",
        "NUM_CLASSES = 2               # background, hair\n",
        "HAIR_CLASS  = 1               # if nothing shows up, try 0\n",
        "IMG_SIZE    = 256             # try 520 if you trained/evaluated at 520\n",
        "\n",
        "MEAN = [0.485, 0.456, 0.406]\n",
        "STD  = [0.229, 0.224, 0.225]\n",
        "# ===================\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "ckpt_path = hf_hub_download(repo_id=REPO_ID, filename=CKPT_NAME)\n",
        "\n",
        "# Build model to match backbone\n",
        "if BACKBONE == \"resnet50\":\n",
        "    model = deeplabv3_resnet50(weights=None, num_classes=NUM_CLASSES, aux_loss=None)\n",
        "elif BACKBONE == \"resnet101\":\n",
        "    model = deeplabv3_resnet101(weights=None, num_classes=NUM_CLASSES, aux_loss=None)\n",
        "else:\n",
        "    raise ValueError(\"Unsupported BACKBONE.\")\n",
        "\n",
        "# Load weights and report mismatches\n",
        "state = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "if isinstance(state, dict) and \"state_dict\" in state and isinstance(state[\"state_dict\"], dict):\n",
        "    state = state[\"state_dict\"]\n",
        "fixed = {}\n",
        "if isinstance(state, dict):\n",
        "    for k, v in state.items():\n",
        "        if k.startswith(\"module.\"): k = k[7:]\n",
        "        if k.startswith(\"model.\"):  k = k[6:]\n",
        "        fixed[k] = v\n",
        "else:\n",
        "    fixed = state\n",
        "\n",
        "missing, unexpected = model.load_state_dict(fixed, strict=False)\n",
        "if missing or unexpected:\n",
        "    print(\"⚠️ load_state_dict warnings:\")\n",
        "    if missing:    print(\"  Missing keys:\", missing[:10], (\"...+more\" if len(missing)>10 else \"\"))\n",
        "    if unexpected: print(\"  Unexpected keys:\", unexpected[:10], (\"...+more\" if len(unexpected)>10 else \"\"))\n",
        "\n",
        "model.to(device).eval()\n",
        "\n",
        "# Preprocess\n",
        "tfm = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=MEAN, std=STD),\n",
        "])\n",
        "\n",
        "# Load image\n",
        "orig = Image.open(IMAGE_PATH).convert(\"RGB\")\n",
        "W, H = orig.size\n",
        "x = tfm(orig).unsqueeze(0).to(device)\n",
        "\n",
        "# Inference → logits → softmax probs\n",
        "with torch.no_grad():\n",
        "    out = model(x)\n",
        "    logits = out[\"out\"][0] if isinstance(out, dict) else out[0]        # (C,H,W)\n",
        "    probs  = torch.softmax(logits, dim=0)                              # (C,H,W)\n",
        "    hair_p = probs[HAIR_CLASS].cpu().numpy()                           # (H,W)\n",
        "\n",
        "# Resize prob map back to original\n",
        "hair_p = cv2.resize(hair_p, (W, H), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "# Diagnostics\n",
        "print(\"prob stats  min/max/mean:\", float(hair_p.min()), float(hair_p.max()), float(hair_p.mean()))\n",
        "\n",
        "# Convert to 8-bit and Otsu threshold\n",
        "hair_u8 = np.clip(hair_p * 255.0, 0, 255).astype(np.uint8)\n",
        "_, mask_u8 = cv2.threshold(hair_u8, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "# Small morph clean-up (optional; comment out if you don’t want it)\n",
        "kernel = np.ones((3,3), np.uint8)\n",
        "mask_u8 = cv2.morphologyEx(mask_u8, cv2.MORPH_OPEN, kernel, iterations=1)\n",
        "mask_u8 = cv2.morphologyEx(mask_u8, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
        "\n",
        "# Save mask for inspection\n",
        "cv2.imwrite(OUTPUT_MASK, mask_u8)\n",
        "\n",
        "# Contours on original (red border only)\n",
        "img_bgr = cv2.cvtColor(np.array(orig), cv2.COLOR_RGB2BGR)\n",
        "contours, _ = cv2.findContours(mask_u8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "cv2.drawContours(img_bgr, contours, -1, (0, 0, 255), 2)  # red in BGR\n",
        "overlay = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "Image.fromarray(overlay).save(OUTPUT_BOUND)\n",
        "\n",
        "print(\"Saved:\", OUTPUT_MASK, \"and\", OUTPUT_BOUND)\n",
        "print(\"Unique mask values:\", np.unique(mask_u8))\n",
        "print(\"Hair pixels ratio:\", float((mask_u8>0).mean()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JScHGZa9ZCqX"
      },
      "source": [
        "densenet121 with huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99,
          "referenced_widgets": [
            "75b2a0ab425d47f5a7f4c2ea630751dc",
            "769e165de76d4166a61bd99abd7f4fcb",
            "ceff33a00f4d42b8b2f27772279b8dd5",
            "a170e8431dc1413a83dda2469b0a52c7",
            "b0bf7b4ba0ad4ab185cfdc84e54c6a56",
            "4b729de8cc6445ee91c84d40b9da7b6f",
            "aeed17bbc56d405487e1ef0a70f46ac4",
            "1abdc373872049199c36d4476ab61978",
            "cc5d0c0931574396add894a480131399",
            "676d71ed16f743acba2d1549513736a4",
            "7e4d90c97dcc46a2908aee15c89bd8f7",
            "5213ea935dae40b4b6cf7524289f3ffc",
            "864781bf4b3c4feca7f7b8cc956ef087",
            "56394ee7894c42b8886fd67a2980cbbe",
            "8929649aa5a64fba8fafc85c7e4a44cb",
            "9c2c1f8792cc4dab81b2921ba548afda",
            "1848703da4b249ed82b848fcfebd1183",
            "078e8140f5f345f5beb6b7b8f68ee792",
            "7f0f48d3fa0d41219ef3e72861488697",
            "a46efd669e9d4eba990daa75ea35fa06",
            "7ae784365652478fa4a656da0f3fc83d",
            "7a6dd31436ab492e8a614433266bbfe4"
          ]
        },
        "id": "iVf0xZTCZAGE",
        "outputId": "8929a809-c3dd-4f1a-886f-9c9b04eab61e"
      },
      "outputs": [],
      "source": [
        "#!pip install --quiet torch torchvision Pillow huggingface_hub\n",
        "\n",
        "import json, torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from huggingface_hub import hf_hub_download\n",
        "from PIL import Image\n",
        "\n",
        "# --- repo + filenames (adjust if your HF names differ) ---\n",
        "REPO         = \"alamb98/densenet121\"\n",
        "WEIGHTS_FILE = \"densenet121.pth\"   # exact filename you uploaded\n",
        "META_FILE    = \"metadata.json\"     # contains labels, image_size, etc.\n",
        "\n",
        "# --- download weights + metadata ---\n",
        "state_dict_path = hf_hub_download(REPO, filename=WEIGHTS_FILE)\n",
        "meta_path       = hf_hub_download(REPO, filename=META_FILE)\n",
        "\n",
        "with open(meta_path, \"r\") as f:\n",
        "    meta = json.load(f)\n",
        "\n",
        "# Metadata defaults if keys are missing\n",
        "LEVEL_NAMES = meta.get(\"labels\") or meta.get(\"class_names\") or [f\"Level {i}\" for i in range(2, 8)]\n",
        "IMG_SIZE    = int(meta.get(\"img_size\", 224))\n",
        "MEAN        = meta.get(\"mean\", [0.485, 0.456, 0.406])\n",
        "STD         = meta.get(\"std\",  [0.229, 0.224, 0.225])\n",
        "LABEL_BASE  = int(meta.get(\"label_base\", 0))  # 0 => logits 0..6 → add +1; 1 => already 1..7\n",
        "\n",
        "# --- build architecture exactly like training (DenseNet121, no built-in weights) ---\n",
        "model = models.densenet121(weights=None)\n",
        "in_features = model.classifier.in_features\n",
        "model.classifier = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "# load trained weights (unwrap common wrappers and strip prefixes)\n",
        "state = torch.load(state_dict_path, map_location=\"cpu\")\n",
        "if isinstance(state, dict) and \"state_dict\" in state and isinstance(state[\"state_dict\"], dict):\n",
        "    state = state[\"state_dict\"]\n",
        "\n",
        "fixed = {}\n",
        "if isinstance(state, dict):\n",
        "    for k, v in state.items():\n",
        "        if isinstance(k, str) and k.startswith(\"module.\"):\n",
        "            k = k[len(\"module.\"):]\n",
        "        if isinstance(k, str) and k.startswith(\"model.\"):\n",
        "            k = k[len(\"model.\"):]\n",
        "        fixed[k] = v\n",
        "else:\n",
        "    fixed = state\n",
        "\n",
        "model.load_state_dict(fixed, strict=True)\n",
        "model.eval()\n",
        "\n",
        "# --- preprocessing identical to eval transform ---\n",
        "tfm = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=MEAN, std=STD),\n",
        "])\n",
        "\n",
        "# --- predict helper ---\n",
        "def predict_densenet121(pil_rgb_image: Image.Image):\n",
        "    \"\"\"\n",
        "    Returns (level_int, label_str).\n",
        "    - If LABEL_BASE == 0: logits are 0..6 → add +1 to map to 1..7\n",
        "    - If LABEL_BASE == 1: logits already represent 1..7\n",
        "    \"\"\"\n",
        "    x = tfm(pil_rgb_image).unsqueeze(0)  # (1,3,H,W)\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)\n",
        "        pred_idx = int(logits.argmax(-1).item())\n",
        "        level = pred_idx + 1 if LABEL_BASE == 0 else pred_idx\n",
        "    if 1 <= level <= len(LEVEL_NAMES):\n",
        "        label_str = LEVEL_NAMES[level - 1]\n",
        "    else:\n",
        "        label_str = f\"Level {level}\"\n",
        "    return level, label_str\n",
        "\n",
        "# --- EXAMPLE: run on a local image ---\n",
        "if __name__ == \"__main__\":\n",
        "    image_path = \"/content/unzipped_dataset1/hyehye2.v3i.folder/valid/Level 3/11-Top-Down_jpg.rf.503c9d816cbfdfc491388bda16933433.jpg\"  # <-- change to your image path\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    level, label_str = predict_densenet121(img)\n",
        "    print(f\"Predicted: {label_str} (level={level})\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "002f10dd430748748e26d1ac4d2e0dc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b11418d389f14f61835ca88981efe14e",
              "IPY_MODEL_41648911f2354641bd50058ea6293a01",
              "IPY_MODEL_d4a19ec480d640da85e373529d9dbfce"
            ],
            "layout": "IPY_MODEL_dcda11f9da084871a50a60ff96c390a8"
          }
        },
        "0086f32407ae4af48e79aff5e5db3804": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0168d60fb4b440ec8e58cdd56d79f62d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "0172a41630864ffcb0e9252eb2d6248f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01ee4c7e72de4974b01f1147840b1826": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "028ac75023f24cc88993dc7b1177c08f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "030dc819b0af410b83aa907b7defcd02": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "034ad1f086334200bc793ccd27796796": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_9bcd5353852f43a4a70299227d42eedf"
          }
        },
        "036eebc17d3042eab30113c74ba97e81": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03f8c0f1232741a794e8ac3e63edfbe5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "048a31892d2340ff846fec152bc12692": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "053f7812e3a14f1eae6762629835f624": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "078e8140f5f345f5beb6b7b8f68ee792": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "089d7b0841b540cbbbc5b47d0896de64": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "091c73a8430e486691552b0a0673420a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09be5211d09c46f3aa9bfbb339d6abca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "09e1d70b49ac46ea8cb503e05abb5848": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7033e35b84764f14b4fecd164d45cdcf",
            "placeholder": "​",
            "style": "IPY_MODEL_6e474c849dd6460b816d9c8b83f987a9",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "0de8c3f756f041eb9ec016083d8addfe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "128ed33b329f4389935dbce22aa09d29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16bfe4754e82469b9e0ec1b6d7cf490d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17c7c858039245a4ba1f3c2f866b48be": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "1816f61f91c84c4c94b5940b8dc01aad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1848703da4b249ed82b848fcfebd1183": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1abdc373872049199c36d4476ab61978": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b26deed0ee442b4bdd770df2b531965": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1cdf43a09a68434da1605911122f5891": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03f8c0f1232741a794e8ac3e63edfbe5",
            "placeholder": "​",
            "style": "IPY_MODEL_666a4e1924e7480298e86549f56655b7",
            "value": " 28.4MB / 28.4MB, 15.8MB/s  "
          }
        },
        "1d8637001d234cf3ac601f6c1222f479": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f0fafd3fa9b4cea84823716835c679e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50e998199cc244d593e097d5389d8d05",
            "placeholder": "​",
            "style": "IPY_MODEL_2a64148a14404abc973b21287c28815a",
            "value": " 17.1MB / 17.1MB, 2.94MB/s  "
          }
        },
        "1f3ba421d43a4a9ea67d8e77455a13b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_919c718171594ef9b471595764e24f05",
            "placeholder": "​",
            "style": "IPY_MODEL_32ac2c5f1e294ba3b3aba773ac4ed87c",
            "value": " 16.4MB / 16.4MB            "
          }
        },
        "207dbc983ee544498c30175a1d2c9f4c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2382c53eee6241ddb6039a8dc8b1fe4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_053f7812e3a14f1eae6762629835f624",
            "placeholder": "​",
            "style": "IPY_MODEL_3dcfb9d923e6461b82719dbfdcd48747",
            "value": "Processing Files (1 / 1)                : 100%"
          }
        },
        "248057b78ea544079a83614f931658ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0168d60fb4b440ec8e58cdd56d79f62d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71b170336d6643f2ac876240a45ddc1c",
            "value": 1
          }
        },
        "255ffb9efcc042b99a2197864f3b4a7e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "25cfa84cc88f4400982255dc21059f94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fb1d080fc6f4210b0dd6b56a799a839",
            "placeholder": "​",
            "style": "IPY_MODEL_6d1cceebff77421da9171af587e5bb92",
            "value": "New Data Upload                         : 100%"
          }
        },
        "2612f9c633ff4a54a3c0895b2750ca18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26260a46c4584c90af7ea5857f55300d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fda3f086741b4b01bdd6626085f69108",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5e44168e7be04c07a619815aaab92bb2",
            "value": 1
          }
        },
        "269c55c3ce224bceb370ebfa1ddc4b10": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26be810c2f8b47b39263c862b44d58f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf6f78d6e58e48059e75e2df19399c1a",
              "IPY_MODEL_c912ad05683349dc84d2c7d457ae967b",
              "IPY_MODEL_c19998596f5045c7b893d3f749d6883d"
            ],
            "layout": "IPY_MODEL_0de8c3f756f041eb9ec016083d8addfe"
          }
        },
        "2719564fdb754508aa2c89e0b4551543": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29a9040263584f5fb32c3a41e61d2df8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_ac0d6439339e45ed9a904c4a996a0092"
          }
        },
        "2a31a6a494854aa09484c456a3aea269": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a64148a14404abc973b21287c28815a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b01f693bc0643979791c115a3c23ee8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "2bed2aff430b4afbaeb0644ee37cbfe2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c5d24e4b9ab47078ae7a0d7ef55ab5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c70402c6aa048d6832c538634a30bd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f273521e5744713a358e2f61f6f2531",
            "placeholder": "​",
            "style": "IPY_MODEL_690b5d40d42945ba88bead3144aa5e4d",
            "value": "  ...orch-hair-segmentation/resnet18.pth: 100%"
          }
        },
        "2c89be3bc8214895bb54c300c2e91d2b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d7440d3548843b88ca4045760af103a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d7d896f82604054b0037d4bb502f691": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e23acffcd00437db07e8641759c4949": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b1a8c2f4fb024c79abc4eba51f3d8d0e",
              "IPY_MODEL_43fcb1b22c364c748b89891857011e29",
              "IPY_MODEL_9e9dfafe028e4e33b623fccfb828f019"
            ],
            "layout": "IPY_MODEL_33b976d13ecc4cf1999b4cc33b7114df"
          }
        },
        "300f7ef89bc640368879db7aaf65f99b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3049829651814c45951bb0cb889cc280": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_25cfa84cc88f4400982255dc21059f94",
              "IPY_MODEL_9bb1db85d930439789026854be8bf656",
              "IPY_MODEL_dd55b87d468442c890a3942ed914c5fa"
            ],
            "layout": "IPY_MODEL_f33b6fec8c444c38b51d0be21162140c"
          }
        },
        "3106f90866484f9bb11b102c920815b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d7440d3548843b88ca4045760af103a",
            "placeholder": "​",
            "style": "IPY_MODEL_7f4e3d51b7df40e898a81eddef78ac67",
            "value": "  111MB /  111MB, 11.1MB/s  "
          }
        },
        "32ac2c5f1e294ba3b3aba773ac4ed87c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33b976d13ecc4cf1999b4cc33b7114df": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3569099dbdef45e48f58eca091d6d9a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_091c73a8430e486691552b0a0673420a",
            "placeholder": "​",
            "style": "IPY_MODEL_55299c772c594ac894616a47917a6b6f",
            "value": "  ...h-hair-segmentation/densenet121.pth: 100%"
          }
        },
        "35acc52314504223b7e352b9e7da2bf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bed2aff430b4afbaeb0644ee37cbfe2",
            "placeholder": "​",
            "style": "IPY_MODEL_3add284622fa4af4bb4f34becf96a579",
            "value": "New Data Upload                         : 100%"
          }
        },
        "37da321c152c4e3b80b8951dbf5ae661": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e378bfd7b59415789cf29923cb69c74",
            "max": 44797308,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_50e64c611d224f26ae2a58b829068b25",
            "value": 44797308
          }
        },
        "38049d92939545b1916e72b26b63300d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "382e39221d4d448d8aade1ba9fa90d62": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38bd19899cbc4c408112a9d1e2dde545": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39d2a5f78e9e4eebbdc69b9c1a313f93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39e4f3661a69460ab58315c42a027257": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3add284622fa4af4bb4f34becf96a579": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b25f03278f84042a8aed2320f1761a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "3c00484debea455db1bcd01d917776ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3cfeda64c5864633b1431c8a69ec7576": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d798016f5c8491080174e5454336b39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fe6dfddc15642298345eebd476bcadd",
            "placeholder": "​",
            "style": "IPY_MODEL_d44f4f2c73ea45fcae527fb2ecd9c348",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "3dcfb9d923e6461b82719dbfdcd48747": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e373320964a4d1da85380fdb28a019c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ad7dbeeeb8943f59c012ff5e46f784b",
            "placeholder": "​",
            "style": "IPY_MODEL_dfb21724818b46a0826c1ea3f6a9217a",
            "value": " 17.1MB / 17.1MB, 2.94MB/s  "
          }
        },
        "3e378bfd7b59415789cf29923cb69c74": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f497bf091e947f4888364cfff0beec4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41648911f2354641bd50058ea6293a01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c01841a3885543e199a96f72d2d9b3a3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fcb6f5628847461b8fe20e3b8715b830",
            "value": 1
          }
        },
        "428a076fbed5463587662d9dd470e23e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9ac64047ce44a59b57babcf95abe13f",
              "IPY_MODEL_e60897b477e6460d96609136bafd445a",
              "IPY_MODEL_672dadbc846c4f95b0aa4af5d9b529d4"
            ],
            "layout": "IPY_MODEL_bf60893451f2487abd1eb6ec810b9a34"
          }
        },
        "42e1eaee788e49e78ad6cb4636422443": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43fcb1b22c364c748b89891857011e29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b25f03278f84042a8aed2320f1761a9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_048a31892d2340ff846fec152bc12692",
            "value": 1
          }
        },
        "441c0431704f41f8bf89e092180a69bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46baf0dac1ef4949ae430cb10746440d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46ce572993a84db59cb79678d767518f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46baf0dac1ef4949ae430cb10746440d",
            "placeholder": "​",
            "style": "IPY_MODEL_53e363d04b4a4b608d146e1255a2f91c",
            "value": "Connecting..."
          }
        },
        "47186eb3f9e64734965b1503daf3bac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_09be5211d09c46f3aa9bfbb339d6abca"
          }
        },
        "491fb8c59a9b44019c7bea0563a27de4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "491fc6f9dd9a4f23bdbabb3ca486fda9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c70402c6aa048d6832c538634a30bd8",
              "IPY_MODEL_37da321c152c4e3b80b8951dbf5ae661",
              "IPY_MODEL_e58aeb7d2e94401e8937332a8600da9f"
            ],
            "layout": "IPY_MODEL_1816f61f91c84c4c94b5940b8dc01aad"
          }
        },
        "4aa69667a56945d99a0d63daa9ace205": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b3f5b0576ee4beba97baf04ab4f9ef1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_701a6c57af6d40598e022be19c49a038",
            "max": 16357982,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e7f8ab59c56e497791b7ec0b0339b4bc",
            "value": 16357982
          }
        },
        "4b729de8cc6445ee91c84d40b9da7b6f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bf4c53dba72468ba9cbca573c30f6ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65f5ec376a164b4c982c3879801bcf88",
            "placeholder": "​",
            "style": "IPY_MODEL_6c6f73850cca44fb81b14ef0f220ddf9",
            "value": "New Data Upload                         : 100%"
          }
        },
        "4e73577f142b4abba9f5f242ffa04d99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_ab9df7184f9d4ef0891d6a1aeefd534f"
          }
        },
        "4fe6dfddc15642298345eebd476bcadd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50ce5572c8e24a25b5be8ca4bcf8c3ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2382c53eee6241ddb6039a8dc8b1fe4c",
              "IPY_MODEL_c730a45d27704e00a2a87f7ea30323c4",
              "IPY_MODEL_5d99d9b066ff4cdeb007d98b70368c0b"
            ],
            "layout": "IPY_MODEL_c56c50d4b97346a4b17a7786f01a59db"
          }
        },
        "50e64c611d224f26ae2a58b829068b25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50e998199cc244d593e097d5389d8d05": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5213ea935dae40b4b6cf7524289f3ffc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_864781bf4b3c4feca7f7b8cc956ef087",
              "IPY_MODEL_56394ee7894c42b8886fd67a2980cbbe",
              "IPY_MODEL_8929649aa5a64fba8fafc85c7e4a44cb"
            ],
            "layout": "IPY_MODEL_9c2c1f8792cc4dab81b2921ba548afda"
          }
        },
        "52fc0e54eabd4d5e9438ea1af691e502": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53944f045a644539935b14ebffec014b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53e363d04b4a4b608d146e1255a2f91c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55299c772c594ac894616a47917a6b6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55b2e6f329dd4dd7bb33a6175dd0674e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_e579c80be5e74ff59197ac83de7bb8cb"
          }
        },
        "56394ee7894c42b8886fd67a2980cbbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f0f48d3fa0d41219ef3e72861488697",
            "max": 259,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a46efd669e9d4eba990daa75ea35fa06",
            "value": 259
          }
        },
        "5a61a1bad9fb4bb2804b6d4615c8fcc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b596c1de94243e498c5a92125afd9ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_35acc52314504223b7e352b9e7da2bf9",
              "IPY_MODEL_d59d32b6f6d04ecc9332073f376dada3",
              "IPY_MODEL_3106f90866484f9bb11b102c920815b8"
            ],
            "layout": "IPY_MODEL_ef9109f280d044f09a98bf6cc3d99413"
          }
        },
        "5c35423555ae4209b5a803693d93e23f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cf81a9e7f204dbe98c6033a12c055a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d99d9b066ff4cdeb007d98b70368c0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_441c0431704f41f8bf89e092180a69bf",
            "placeholder": "​",
            "style": "IPY_MODEL_f47ac73baea642549dbafee5f4212607",
            "value": " 16.4MB / 16.4MB, 2.82MB/s  "
          }
        },
        "5e44168e7be04c07a619815aaab92bb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5e876cb107a5434fa71ed4bfacdba859": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73574a7cbd8a4b418099318935f6340f",
            "max": 561,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b26deed0ee442b4bdd770df2b531965",
            "value": 561
          }
        },
        "62be97d1eb414b66b68a908d01d141cb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6485321ab55843bd9c70a2c40e3f8c52": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_91c555e8b3c34b209786dde10052fc97",
            "placeholder": "​",
            "style": "IPY_MODEL_5cf81a9e7f204dbe98c6033a12c055a5",
            "value": ""
          }
        },
        "65ab429d7a514d6fa10b4bddf05d3704": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65f5ec376a164b4c982c3879801bcf88": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "666a4e1924e7480298e86549f56655b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "669fb11cb724446990b9394a28b6deb2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "66e63e0cb17049e580037b2af3eb22ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "672dadbc846c4f95b0aa4af5d9b529d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65ab429d7a514d6fa10b4bddf05d3704",
            "placeholder": "​",
            "style": "IPY_MODEL_3c00484debea455db1bcd01d917776ac",
            "value": " 17.1M/17.1M [00:01&lt;00:00, 14.2MB/s]"
          }
        },
        "676d71ed16f743acba2d1549513736a4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68f69bca68e74fc1b6b4ca44522c1129": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "690b5d40d42945ba88bead3144aa5e4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c6f73850cca44fb81b14ef0f220ddf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c83884f93b04a44a92c5892030adcef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9c3fbd69e3d04e138468e7e72ee0d727",
              "IPY_MODEL_248057b78ea544079a83614f931658ad",
              "IPY_MODEL_9b78abe5fca547ee8773c8bbb31086fe"
            ],
            "layout": "IPY_MODEL_a76d69b64dba4538936a01b90148c375"
          }
        },
        "6d1cceebff77421da9171af587e5bb92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d662ffc0b894a8297db3b936c996e35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e474c849dd6460b816d9c8b83f987a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6fd8de0e3a794e6486818cbf20ad547e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_f5934541a91e402bb27abc76e332a5c3",
            "style": "IPY_MODEL_ca6a389c63324a2d956431cbcdccfff0",
            "tooltip": ""
          }
        },
        "701a6c57af6d40598e022be19c49a038": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7033e35b84764f14b4fecd164d45cdcf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71b170336d6643f2ac876240a45ddc1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "734d5361b28449898e6df8904b6c18fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c35423555ae4209b5a803693d93e23f",
            "placeholder": "​",
            "style": "IPY_MODEL_a8eb50a96c2c47f7a999637e5adb22e1",
            "value": "metadata.json: 100%"
          }
        },
        "73574a7cbd8a4b418099318935f6340f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7399733e3453480194d9b193f1bf04ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74625abb9239466c95559809a23350fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7516284f683940219b6ac2f7448dff7b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75b2a0ab425d47f5a7f4c2ea630751dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_769e165de76d4166a61bd99abd7f4fcb",
              "IPY_MODEL_ceff33a00f4d42b8b2f27772279b8dd5",
              "IPY_MODEL_a170e8431dc1413a83dda2469b0a52c7"
            ],
            "layout": "IPY_MODEL_b0bf7b4ba0ad4ab185cfdc84e54c6a56"
          }
        },
        "769e165de76d4166a61bd99abd7f4fcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b729de8cc6445ee91c84d40b9da7b6f",
            "placeholder": "​",
            "style": "IPY_MODEL_aeed17bbc56d405487e1ef0a70f46ac4",
            "value": "densenet121.pth: 100%"
          }
        },
        "78f8168bad5d4265bd75c19e77da4255": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_cdb2832cbd104911861f7c704445ab43",
            "style": "IPY_MODEL_ec9260de7187424d987343f6ff2a291c",
            "tooltip": ""
          }
        },
        "7a6dd31436ab492e8a614433266bbfe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ae784365652478fa4a656da0f3fc83d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c3be50ee85244bb87425662432891c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e4d90c97dcc46a2908aee15c89bd8f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f0f48d3fa0d41219ef3e72861488697": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f4e3d51b7df40e898a81eddef78ac67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85427a41d3ed46a7860cde4c7fc75abe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd23641e80f046108169bbf66dfa86cf",
            "placeholder": "​",
            "style": "IPY_MODEL_42e1eaee788e49e78ad6cb4636422443",
            "value": "  ...gmentation/efficientnet-b0_best.pth: 100%"
          }
        },
        "85a01d3131f64653a6817af9b8c1e028": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "864781bf4b3c4feca7f7b8cc956ef087": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1848703da4b249ed82b848fcfebd1183",
            "placeholder": "​",
            "style": "IPY_MODEL_078e8140f5f345f5beb6b7b8f68ee792",
            "value": "metadata.json: 100%"
          }
        },
        "8929649aa5a64fba8fafc85c7e4a44cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ae784365652478fa4a656da0f3fc83d",
            "placeholder": "​",
            "style": "IPY_MODEL_7a6dd31436ab492e8a614433266bbfe4",
            "value": " 259/259 [00:00&lt;00:00, 32.7kB/s]"
          }
        },
        "8ad7dbeeeb8943f59c012ff5e46f784b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fc0bb64d856494782f64779250a89af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bbb3e6d4c06d4e96b6c854937e679791",
              "IPY_MODEL_bcc8ac5e0d774af2afe06ce7b61c5b9a",
              "IPY_MODEL_bbabf8d873374ee48fb8af159c95885b"
            ],
            "layout": "IPY_MODEL_aab8f90650fe4b7dab655a7bfcc0f604"
          }
        },
        "919c718171594ef9b471595764e24f05": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91c555e8b3c34b209786dde10052fc97": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91e25d63304d46958ec3b95d211946a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_300f7ef89bc640368879db7aaf65f99b",
            "placeholder": "​",
            "style": "IPY_MODEL_d23ae5987d0f46f0918647f5f5016e20",
            "value": "Connecting..."
          }
        },
        "99c0d062ad3e4dca902dbbad07566614": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b78abe5fca547ee8773c8bbb31086fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62be97d1eb414b66b68a908d01d141cb",
            "placeholder": "​",
            "style": "IPY_MODEL_491fb8c59a9b44019c7bea0563a27de4",
            "value": " 28.4MB / 28.4MB, 15.8MB/s  "
          }
        },
        "9bb1db85d930439789026854be8bf656": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_255ffb9efcc042b99a2197864f3b4a7e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39d2a5f78e9e4eebbdc69b9c1a313f93",
            "value": 1
          }
        },
        "9bcd5353852f43a4a70299227d42eedf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "9c2c1f8792cc4dab81b2921ba548afda": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c3fbd69e3d04e138468e7e72ee0d727": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da096eb330e14e7c9ada93773a83176e",
            "placeholder": "​",
            "style": "IPY_MODEL_f37b481087554f44a6552c07a2eb4d74",
            "value": "New Data Upload                         : 100%"
          }
        },
        "9e9dfafe028e4e33b623fccfb828f019": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4014b7f04944ab8a51a76b6dedbf5af",
            "placeholder": "​",
            "style": "IPY_MODEL_f165849f936c437a8959af0066f9814c",
            "value": " 44.8MB / 44.8MB, 5.46MB/s  "
          }
        },
        "9f273521e5744713a358e2f61f6f2531": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fb1d080fc6f4210b0dd6b56a799a839": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0b9c40dcd254fff802708fa00dcd954": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a170e8431dc1413a83dda2469b0a52c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_676d71ed16f743acba2d1549513736a4",
            "placeholder": "​",
            "style": "IPY_MODEL_7e4d90c97dcc46a2908aee15c89bd8f7",
            "value": " 28.4M/28.4M [00:00&lt;00:00, 45.9MB/s]"
          }
        },
        "a46efd669e9d4eba990daa75ea35fa06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a63a93a2d1bc4253a5766c56a54c9c27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a76d69b64dba4538936a01b90148c375": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8eb50a96c2c47f7a999637e5adb22e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aab8f90650fe4b7dab655a7bfcc0f604": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab9df7184f9d4ef0891d6a1aeefd534f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "ac0d6439339e45ed9a904c4a996a0092": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "aeed17bbc56d405487e1ef0a70f46ac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af18df28813e4976aebb5cd98731de71": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0bf7b4ba0ad4ab185cfdc84e54c6a56": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b11418d389f14f61835ca88981efe14e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d8637001d234cf3ac601f6c1222f479",
            "placeholder": "​",
            "style": "IPY_MODEL_a63a93a2d1bc4253a5766c56a54c9c27",
            "value": "Processing Files (1 / 1)                : 100%"
          }
        },
        "b1a8c2f4fb024c79abc4eba51f3d8d0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38049d92939545b1916e72b26b63300d",
            "placeholder": "​",
            "style": "IPY_MODEL_dec697e9c3a3493e9db275f06b2c6bb0",
            "value": "Processing Files (1 / 1)                : 100%"
          }
        },
        "b1f897fc3b554d35a5335211cf9a4038": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2f1f595119d4fec940f7a7ce0e45328": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d47cdf87bc6740fba2c1043503d847ff",
            "placeholder": "​",
            "style": "IPY_MODEL_128ed33b329f4389935dbce22aa09d29",
            "value": "New Data Upload                         : 100%"
          }
        },
        "b2f981efbe034fa09f3e7ba7815b305e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6857e635f27449f9baedaa94885b1e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3569099dbdef45e48f58eca091d6d9a4",
              "IPY_MODEL_e21393a709f947889c3bdc44630bc452",
              "IPY_MODEL_b79b6f20e49547569dc46827ae8b521f"
            ],
            "layout": "IPY_MODEL_207dbc983ee544498c30175a1d2c9f4c"
          }
        },
        "b70b2f9f77bb4273b5378692559df3cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b79b6f20e49547569dc46827ae8b521f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_382e39221d4d448d8aade1ba9fa90d62",
            "placeholder": "​",
            "style": "IPY_MODEL_53944f045a644539935b14ebffec014b",
            "value": " 28.4MB / 28.4MB            "
          }
        },
        "b9392e24db88439f8b677263d2797e83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_734d5361b28449898e6df8904b6c18fe",
              "IPY_MODEL_5e876cb107a5434fa71ed4bfacdba859",
              "IPY_MODEL_fd6133b5ef9340358df785f35ff85dc3"
            ],
            "layout": "IPY_MODEL_66e63e0cb17049e580037b2af3eb22ae"
          }
        },
        "b9974637402149d1879b39da44de3b58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bbabf8d873374ee48fb8af159c95885b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f533200ed0734d47b41778921b5750ba",
            "placeholder": "​",
            "style": "IPY_MODEL_e8a742327d88496db8b7da507b911249",
            "value": "  111MB /  111MB            "
          }
        },
        "bbb3e6d4c06d4e96b6c854937e679791": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddf570c99bdd4e34a8694003e888246f",
            "placeholder": "​",
            "style": "IPY_MODEL_99c0d062ad3e4dca902dbbad07566614",
            "value": "  ...tation/convnext_tiny_clahe_best.pth: 100%"
          }
        },
        "bc5c4f26fae443958de1229d2c100d8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce72272810f74e6fb7975efa8f99cb96",
            "placeholder": "​",
            "style": "IPY_MODEL_c26884c312c54ee88b24970417d55bff",
            "value": " 44.8MB / 44.8MB, 5.46MB/s  "
          }
        },
        "bcc8ac5e0d774af2afe06ce7b61c5b9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01ee4c7e72de4974b01f1147840b1826",
            "max": 111368480,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9974637402149d1879b39da44de3b58",
            "value": 111368480
          }
        },
        "bceecfb9889d49bb89f94e5bc83cb6bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_7516284f683940219b6ac2f7448dff7b",
            "style": "IPY_MODEL_3cfeda64c5864633b1431c8a69ec7576",
            "value": true
          }
        },
        "bd07bae1f1664153b6cda5929fc36c9b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf60893451f2487abd1eb6ec810b9a34": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c01841a3885543e199a96f72d2d9b3a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "c0a43c2bfdb54ad6af2b14c719f334e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_af18df28813e4976aebb5cd98731de71",
            "placeholder": "​",
            "style": "IPY_MODEL_3f497bf091e947f4888364cfff0beec4",
            "value": ""
          }
        },
        "c19998596f5045c7b893d3f749d6883d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2b6df7428714a4d9c4ddc2d1a4061ef",
            "placeholder": "​",
            "style": "IPY_MODEL_16bfe4754e82469b9e0ec1b6d7cf490d",
            "value": " 17.1MB / 17.1MB            "
          }
        },
        "c26884c312c54ee88b24970417d55bff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2b6df7428714a4d9c4ddc2d1a4061ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2d09d44b3b44ed6ad39f748ff7d94c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f575e212d1a44d15ab77ec08d45ad4bd",
              "IPY_MODEL_cb2f9c815bfc474a8504477f12dc1d9b",
              "IPY_MODEL_3e373320964a4d1da85380fdb28a019c"
            ],
            "layout": "IPY_MODEL_0086f32407ae4af48e79aff5e5db3804"
          }
        },
        "c56c50d4b97346a4b17a7786f01a59db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c59cf49a0fe84a1f98dc18dea43f857c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "c730a45d27704e00a2a87f7ea30323c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_669fb11cb724446990b9394a28b6deb2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_028ac75023f24cc88993dc7b1177c08f",
            "value": 1
          }
        },
        "c7c397483182460f8fc56fd18bd59cc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8a4e87781a744578d129ab0be2f7cfa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c912ad05683349dc84d2c7d457ae967b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52fc0e54eabd4d5e9438ea1af691e502",
            "max": 17053514,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38bd19899cbc4c408112a9d1e2dde545",
            "value": 17053514
          }
        },
        "c93250d3ce29458ba706e777ecdd641d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c99b3f0c4fe74268912b7ae250db62cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca6a389c63324a2d956431cbcdccfff0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "cb2f9c815bfc474a8504477f12dc1d9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17c7c858039245a4ba1f3c2f866b48be",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6933959d0544a83ae35994cf299a169",
            "value": 1
          }
        },
        "cb871555a9694bda925a8c34714c9545": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "cc5d0c0931574396add894a480131399": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cdb2832cbd104911861f7c704445ab43": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce72272810f74e6fb7975efa8f99cb96": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ceff33a00f4d42b8b2f27772279b8dd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1abdc373872049199c36d4476ab61978",
            "max": 28447536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cc5d0c0931574396add894a480131399",
            "value": 28447536
          }
        },
        "cf6f78d6e58e48059e75e2df19399c1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c99b3f0c4fe74268912b7ae250db62cd",
            "placeholder": "​",
            "style": "IPY_MODEL_4aa69667a56945d99a0d63daa9ace205",
            "value": "  ...net_v3_large_cropped_clahe_best.pth: 100%"
          }
        },
        "d23ae5987d0f46f0918647f5f5016e20": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3b7a274677a4abd9dff3bed5ddaf1da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2f981efbe034fa09f3e7ba7815b305e",
            "placeholder": "​",
            "style": "IPY_MODEL_2612f9c633ff4a54a3c0895b2750ca18",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "d44f4f2c73ea45fcae527fb2ecd9c348": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d476a017467b4900aa222b8d28a7828d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d47cdf87bc6740fba2c1043503d847ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4a19ec480d640da85e373529d9dbfce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_269c55c3ce224bceb370ebfa1ddc4b10",
            "placeholder": "​",
            "style": "IPY_MODEL_5a61a1bad9fb4bb2804b6d4615c8fcc5",
            "value": "  111MB /  111MB, 11.1MB/s  "
          }
        },
        "d4d841d9e50f4720ab3a17548d8295ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_030dc819b0af410b83aa907b7defcd02",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74625abb9239466c95559809a23350fe",
            "value": 1
          }
        },
        "d59d32b6f6d04ecc9332073f376dada3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b01f693bc0643979791c115a3c23ee8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2719564fdb754508aa2c89e0b4551543",
            "value": 1
          }
        },
        "d73c0a76ccd448349983edf62d524976": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9af9466b5974cd8a6a3ab86ce16e343": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da096eb330e14e7c9ada93773a83176e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db40513ae30f4966961fb109aa064c0d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcda11f9da084871a50a60ff96c390a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd55b87d468442c890a3942ed914c5fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_036eebc17d3042eab30113c74ba97e81",
            "placeholder": "​",
            "style": "IPY_MODEL_0172a41630864ffcb0e9252eb2d6248f",
            "value": " 16.4MB / 16.4MB, 2.82MB/s  "
          }
        },
        "ddf570c99bdd4e34a8694003e888246f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dec697e9c3a3493e9db275f06b2c6bb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dfb21724818b46a0826c1ea3f6a9217a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e21393a709f947889c3bdc44630bc452": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85a01d3131f64653a6817af9b8c1e028",
            "max": 28447536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c3be50ee85244bb87425662432891c2",
            "value": 28447536
          }
        },
        "e579c80be5e74ff59197ac83de7bb8cb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "e58aeb7d2e94401e8937332a8600da9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d476a017467b4900aa222b8d28a7828d",
            "placeholder": "​",
            "style": "IPY_MODEL_2a31a6a494854aa09484c456a3aea269",
            "value": " 44.8MB / 44.8MB            "
          }
        },
        "e60897b477e6460d96609136bafd445a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8a4e87781a744578d129ab0be2f7cfa",
            "max": 17053514,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0b9c40dcd254fff802708fa00dcd954",
            "value": 17053514
          }
        },
        "e63c47c9cd264d548e85673e1df8ba17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_c59cf49a0fe84a1f98dc18dea43f857c"
          }
        },
        "e65e29ba548940a6a8d2b8cddae5acaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb871555a9694bda925a8c34714c9545",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c5d24e4b9ab47078ae7a0d7ef55ab5b",
            "value": 1
          }
        },
        "e7f8ab59c56e497791b7ec0b0339b4bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e83f621ee7eb4b2284be0b304fb6562e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_7399733e3453480194d9b193f1bf04ed",
            "style": "IPY_MODEL_c7c397483182460f8fc56fd18bd59cc6",
            "value": true
          }
        },
        "e8a742327d88496db8b7da507b911249": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8f34cb2e4194abbb9d44c2c2d29c703": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4c3d54403584c3ba4b697e2e3b10cf6",
              "IPY_MODEL_26260a46c4584c90af7ea5857f55300d",
              "IPY_MODEL_1cdf43a09a68434da1605911122f5891"
            ],
            "layout": "IPY_MODEL_089d7b0841b540cbbbc5b47d0896de64"
          }
        },
        "e9ac64047ce44a59b57babcf95abe13f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68f69bca68e74fc1b6b4ca44522c1129",
            "placeholder": "​",
            "style": "IPY_MODEL_f361ce53cfd84a859d1107212088bb12",
            "value": "mobilenet_v3_large_cropped_clahe_best.pt(…): 100%"
          }
        },
        "eba0fa1932d549beb037df7e10444cb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85427a41d3ed46a7860cde4c7fc75abe",
              "IPY_MODEL_4b3f5b0576ee4beba97baf04ab4f9ef1",
              "IPY_MODEL_1f3ba421d43a4a9ea67d8e77455a13b4"
            ],
            "layout": "IPY_MODEL_db40513ae30f4966961fb109aa064c0d"
          }
        },
        "ec58321961e34a0a8aa121b520135aef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4bf4c53dba72468ba9cbca573c30f6ba",
              "IPY_MODEL_e65e29ba548940a6a8d2b8cddae5acaa",
              "IPY_MODEL_1f0fafd3fa9b4cea84823716835c679e"
            ],
            "layout": "IPY_MODEL_2c89be3bc8214895bb54c300c2e91d2b"
          }
        },
        "ec9260de7187424d987343f6ff2a291c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "ef9109f280d044f09a98bf6cc3d99413": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f165849f936c437a8959af0066f9814c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f322c0123ef54d588aff5dde49ebf8e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b2f1f595119d4fec940f7a7ce0e45328",
              "IPY_MODEL_d4d841d9e50f4720ab3a17548d8295ee",
              "IPY_MODEL_bc5c4f26fae443958de1229d2c100d8a"
            ],
            "layout": "IPY_MODEL_39e4f3661a69460ab58315c42a027257"
          }
        },
        "f33b6fec8c444c38b51d0be21162140c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f361ce53cfd84a859d1107212088bb12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f37b481087554f44a6552c07a2eb4d74": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4014b7f04944ab8a51a76b6dedbf5af": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f44ebda2e48846f0908b4a78110e8c66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b70b2f9f77bb4273b5378692559df3cc",
            "placeholder": "​",
            "style": "IPY_MODEL_d9af9466b5974cd8a6a3ab86ce16e343",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "f47ac73baea642549dbafee5f4212607": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4c3d54403584c3ba4b697e2e3b10cf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d73c0a76ccd448349983edf62d524976",
            "placeholder": "​",
            "style": "IPY_MODEL_6d662ffc0b894a8297db3b936c996e35",
            "value": "Processing Files (1 / 1)                : 100%"
          }
        },
        "f533200ed0734d47b41778921b5750ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f575e212d1a44d15ab77ec08d45ad4bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd07bae1f1664153b6cda5929fc36c9b",
            "placeholder": "​",
            "style": "IPY_MODEL_2d7d896f82604054b0037d4bb502f691",
            "value": "Processing Files (1 / 1)                : 100%"
          }
        },
        "f5934541a91e402bb27abc76e332a5c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6933959d0544a83ae35994cf299a169": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fcb6f5628847461b8fe20e3b8715b830": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fd23641e80f046108169bbf66dfa86cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd6133b5ef9340358df785f35ff85dc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c93250d3ce29458ba706e777ecdd641d",
            "placeholder": "​",
            "style": "IPY_MODEL_b1f897fc3b554d35a5335211cf9a4038",
            "value": " 561/561 [00:00&lt;00:00, 63.9kB/s]"
          }
        },
        "fda3f086741b4b01bdd6626085f69108": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
