{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPAz8S9QmOV9"
      },
      "source": [
        "# *** DOWNLOADING & MERGING DATASETS + APPLYING CLAHE & GITHUB SEGMENTATION *** (no need to upload dataset, it is in drive + you can run all this section at once)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyVCTeFWmR5S",
        "outputId": "64bfcb5e-f9f3-44a1-fdb5-8596d7b028ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-hair-segmentation'...\n",
            "remote: Enumerating objects: 427, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 427 (delta 5), reused 10 (delta 1), pack-reused 409 (from 1)\u001b[K\n",
            "Receiving objects: 100% (427/427), 51.63 MiB | 47.84 MiB/s, done.\n",
            "Resolving deltas: 100% (225/225), done.\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hnavigating to ./data/ ...\n",
            "Now downloading Figaro1k.zip ...\n",
            "--2025-08-12 17:55:44--  https://www.dropbox.com/s/35momrh68zuhkei/Figaro1k.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.dropbox.com/scl/fi/jw6ic618vbo6jozwlndq1/Figaro1k.zip?rlkey=x5pu7n6m52pkja7nxn7jx0qy2 [following]\n",
            "--2025-08-12 17:55:44--  https://www.dropbox.com/scl/fi/jw6ic618vbo6jozwlndq1/Figaro1k.zip?rlkey=x5pu7n6m52pkja7nxn7jx0qy2\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uca4156c31140b0daa91d90b09f3.dl.dropboxusercontent.com/cd/0/inline/CvQav8f8vu5MGhQhjV8EAhBHynfztzjJ2N0eCvJd8r_lzi-y58vly2yC0vqJy73UOf3doc-YeXerkDwsRiTW3o0wcjD8FFVlGS2gXkJLK4ZtacWAnzIjttqRQ-Q76wPlNQc/file# [following]\n",
            "--2025-08-12 17:55:45--  https://uca4156c31140b0daa91d90b09f3.dl.dropboxusercontent.com/cd/0/inline/CvQav8f8vu5MGhQhjV8EAhBHynfztzjJ2N0eCvJd8r_lzi-y58vly2yC0vqJy73UOf3doc-YeXerkDwsRiTW3o0wcjD8FFVlGS2gXkJLK4ZtacWAnzIjttqRQ-Q76wPlNQc/file\n",
            "Resolving uca4156c31140b0daa91d90b09f3.dl.dropboxusercontent.com (uca4156c31140b0daa91d90b09f3.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:601c:15::a27d:60f\n",
            "Connecting to uca4156c31140b0daa91d90b09f3.dl.dropboxusercontent.com (uca4156c31140b0daa91d90b09f3.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CvT8z9uZiwNG_TCFxGZxa483_pHlR8kkj7z_qAqXQIqRKOvf97uXEkfSMP1nSBJrYU_MFzc0z7AuXV93yJNams03jTzKpXgxgydXpowka9walpG11E4wUOIYk4soLGfNQmPDJ_4ZjljNdYF70VtHUq-7cIK5GHl1lGeLEeeyVvvxiqvioMR9PT_zGMAs3nRpVJE8M8dRqt1i6IHhDgHXhDFV7MzOxjN2ynmVve-7wIvxNPhHD0BOshlCnk4Q_NTJh6ChfB39EdjhgVWmIR9mOSd6ZMvdeez9zuLdzjgtkD92iW-7LZIRMYC8ANaxVMk-H4DxSWsdWSlkEeJ7sHIm0AWMcRB8HpJbNGgOjsnNyCdT5w/file [following]\n",
            "--2025-08-12 17:55:45--  https://uca4156c31140b0daa91d90b09f3.dl.dropboxusercontent.com/cd/0/inline2/CvT8z9uZiwNG_TCFxGZxa483_pHlR8kkj7z_qAqXQIqRKOvf97uXEkfSMP1nSBJrYU_MFzc0z7AuXV93yJNams03jTzKpXgxgydXpowka9walpG11E4wUOIYk4soLGfNQmPDJ_4ZjljNdYF70VtHUq-7cIK5GHl1lGeLEeeyVvvxiqvioMR9PT_zGMAs3nRpVJE8M8dRqt1i6IHhDgHXhDFV7MzOxjN2ynmVve-7wIvxNPhHD0BOshlCnk4Q_NTJh6ChfB39EdjhgVWmIR9mOSd6ZMvdeez9zuLdzjgtkD92iW-7LZIRMYC8ANaxVMk-H4DxSWsdWSlkEeJ7sHIm0AWMcRB8HpJbNGgOjsnNyCdT5w/file\n",
            "Reusing existing connection to uca4156c31140b0daa91d90b09f3.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 65727260 (63M) [application/zip]\n",
            "Saving to: ‘Figaro1k.zip’\n",
            "\n",
            "Figaro1k.zip        100%[===================>]  62.68M  83.7MB/s    in 0.7s    \n",
            "\n",
            "2025-08-12 17:55:47 (83.7 MB/s) - ‘Figaro1k.zip’ saved [65727260/65727260]\n",
            "\n",
            "Unzip Figaro1k.zip ...\n",
            "Archive:  Figaro1k.zip\n",
            "   creating: Figaro1k/\n",
            "   creating: Figaro1k/GT/\n",
            "  inflating: Figaro1k/GT/.DS_Store   \n",
            "   creating: Figaro1k/GT/Training/\n",
            "  inflating: Figaro1k/GT/Training/Frame00902-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00717-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00046-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00146-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00709-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00058-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00609-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00158-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00774-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00025-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00292-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00674-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00125-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00392-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00961-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00861-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00810-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00910-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00054-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00705-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00678-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00029-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00778-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00380-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00137-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00280-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00037-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00766-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00873-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00973-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00484-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00633-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00162-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00733-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00062-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00926-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00838-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00938-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00845-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00945-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00101-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00750-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00001-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00070-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00721-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00170-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00621-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00496-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00934-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00834-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00949-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00849-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00588-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00488-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00957-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00857-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00980-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00880-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00273-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01015-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00522-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00422-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00210-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00541-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00310-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00441-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00992-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00430-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00687-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01007-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00530-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00261-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00787-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00799-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01019-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00453-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00302-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00553-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00357-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00406-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00257-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00506-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01031-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00349-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00418-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00249-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00334-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00465-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00183-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00234-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00565-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00083-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00514-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01023-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00245-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00414-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00345-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00569-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00338-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00091-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01040-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00577-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00226-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00191-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00477-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00326-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00451-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00300-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00200-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00532-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00890-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01009-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00789-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00689-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00212-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00543-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00312-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00443-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00797-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00271-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01017-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00697-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00371-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00420-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00982-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00882-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00508-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00359-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00093-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00575-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00224-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00193-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00475-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00324-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01021-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00516-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00336-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00467-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00181-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00236-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00567-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00081-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00355-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00255-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00504-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00479-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00228-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00148-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00619-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00719-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00871-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00382-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00664-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00035-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00764-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00156-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00607-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00056-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00707-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00812-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00963-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00863-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00776-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00027-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00290-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00676-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00127-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00390-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00044-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00615-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00144-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00800-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00768-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00039-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00139-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00928-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00828-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00011-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00740-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00111-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00955-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00855-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00936-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00836-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00072-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00723-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00623-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00494-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00598-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00652-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00103-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00752-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00003-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00847-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00947-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00824-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00924-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00486-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00160-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00586-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00731-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00859-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00959-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00332-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00185-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00085-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01029-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00400-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00351-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00500-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00097-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00220-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00571-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01046-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00197-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00320-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00189-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00243-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01025-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00343-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00412-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00547-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00447-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00316-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00898-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00208-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00793-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00275-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00693-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00424-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00375-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00986-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00886-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00304-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00455-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00204-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00555-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00379-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00428-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00279-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00528-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00436-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00536-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01001-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00781-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00894-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00994-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00107-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00656-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00007-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00756-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00843-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00943-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00119-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00648-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00019-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00748-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00820-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00920-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00482-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00635-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00582-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00735-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00015-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00644-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00951-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00851-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00739-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00068-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00639-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00932-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00727-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00076-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00590-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00176-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00490-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00867-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00023-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00294-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00123-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00672-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00394-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00979-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00879-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00040-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00711-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00140-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00611-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00904-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00804-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00875-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00386-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00660-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00131-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00286-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00760-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00031-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00808-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00908-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00398-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00298-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00603-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00152-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00703-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00816-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00916-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00758-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00009-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00658-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00109-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00725-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00074-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00592-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00625-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00174-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00492-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00930-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00830-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00953-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00853-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00746-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00017-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00646-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00117-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00480-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00166-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00637-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00580-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00066-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00737-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00922-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00841-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00941-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00654-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00005-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00754-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00629-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00078-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00814-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00914-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00150-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00384-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00662-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00133-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00284-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00033-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00288-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00388-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00806-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00042-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00713-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00142-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00613-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00021-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00770-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00296-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00121-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00670-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00396-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00965-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00865-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00918-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00818-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00241-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00510-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01027-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00341-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00410-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00095-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00222-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01044-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00573-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00195-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00322-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00473-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00199-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00099-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01048-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00402-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00502-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01035-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00253-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00461-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00330-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00561-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00230-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00087-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00318-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00449-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00218-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00549-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00896-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00996-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00434-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00683-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01003-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00783-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00306-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00206-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00557-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00988-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00984-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00884-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00791-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01011-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00277-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00691-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00426-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00377-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00545-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00214-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00445-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00538-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00269-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00438-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00369-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00235-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00564-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00082-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00335-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00464-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00248-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00519-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00348-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00419-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00256-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00507-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01030-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00356-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00407-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00190-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00476-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00327-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01041-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00576-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00227-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00468-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00339-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00568-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00239-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00415-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00344-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00515-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01022-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00244-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00311-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00440-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00211-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00540-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00881-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00981-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00372-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00423-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00272-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01014-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00552-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00452-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00698-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00993-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00893-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01006-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00260-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00786-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00360-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00686-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00944-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00844-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00751-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00651-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00100-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00939-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00839-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00585-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00732-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00485-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00632-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00163-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00927-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00956-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00012-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00743-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00489-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00589-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00848-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00948-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00171-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00620-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00497-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00071-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00597-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00835-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00675-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00124-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00393-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00775-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00860-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00608-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00159-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00708-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00059-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00803-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00147-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00716-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00047-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00036-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00136-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00667-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00972-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00872-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00028-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00779-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00679-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00911-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00811-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00055-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00704-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00155-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00604-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00837-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00937-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00173-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00495-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00073-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00722-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00595-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00110-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00641-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00854-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00954-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00829-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00958-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00925-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00825-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00061-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00487-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00630-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00161-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00753-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00002-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00653-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00946-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00846-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00499-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00057-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00706-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00157-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00606-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00913-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00813-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00970-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00870-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00283-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00034-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00765-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00383-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00665-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00049-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00718-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00149-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00618-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00669-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00038-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00614-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00145-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00045-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00901-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00677-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00126-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00777-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00026-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00291-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00417-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00346-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01020-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00517-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00192-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00474-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00325-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00092-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00574-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01043-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00225-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00509-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00258-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00229-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00578-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00329-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00478-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00254-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01032-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00505-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00354-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00405-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00237-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00566-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00080-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00337-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00466-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00180-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00533-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01004-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00784-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00362-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00684-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00891-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00550-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00201-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00301-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00696-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00370-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00421-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00796-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00270-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00521-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01016-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00313-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00213-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00542-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00688-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01008-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00788-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00141-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00610-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00041-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00710-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00805-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00905-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00878-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00978-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00866-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00966-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00122-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00673-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00395-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00022-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00773-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00295-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00702-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00053-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00602-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00917-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00299-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00399-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00909-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00809-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00874-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00287-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00761-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00661-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00130-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00921-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00821-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00065-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00734-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00634-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00749-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00118-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00649-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00006-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00757-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00106-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00657-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00942-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00842-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00833-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00933-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00626-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00177-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00726-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00077-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00591-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00169-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00738-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00745-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00014-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00950-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00692-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00425-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00525-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01012-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00274-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00887-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00987-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00309-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00209-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00899-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00999-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00446-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00317-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00546-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00217-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00266-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00537-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01000-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00780-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00366-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00437-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00995-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00895-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00278-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00529-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00378-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00429-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00205-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00554-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00305-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01036-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00501-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00250-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00350-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01028-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00562-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00233-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00462-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00184-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00342-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00413-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00242-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01024-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00513-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00188-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00088-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00196-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00470-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00221-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00570-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01047-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00989-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00556-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00307-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00456-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00264-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01002-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00535-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00364-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00682-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00219-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00548-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00448-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00439-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00368-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00539-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00268-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00444-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00315-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00544-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00215-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00885-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00985-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00690-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00427-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00376-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00790-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00527-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00276-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01038-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00194-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00323-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00472-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00094-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00223-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01045-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00572-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00340-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00411-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00240-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00511-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00560-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00231-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00086-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00460-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00331-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00403-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00352-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame01049-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00198-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00763-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00385-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00663-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00132-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00976-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00876-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00915-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00815-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00700-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00051-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00600-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00968-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00868-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00919-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00120-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00671-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00397-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00020-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00771-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00297-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00864-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00807-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00907-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00143-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00612-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00043-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00712-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00389-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00289-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00852-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00952-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00647-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00116-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00747-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00016-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00624-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00175-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00493-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00724-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00075-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00593-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00831-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00931-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00659-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00108-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00759-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00008-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00728-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00179-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00628-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00940-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00840-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00004-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00755-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00104-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00655-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00581-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00067-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00736-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00481-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00167-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00636-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00923-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Training/Frame00823-gt.pbm  \n",
            "   creating: Figaro1k/GT/Testing/\n",
            "  inflating: Figaro1k/GT/Testing/Frame00802-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00617-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00154-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00605-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00129-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00666-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00584-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00826-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00650-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00596-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00013-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00742-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00113-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00642-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00795-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00695-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00373-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00892-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00361-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00699-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00202-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00518-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00238-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00469-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00551-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00432-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00363-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00685-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame01005-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00263-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00785-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00990-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00520-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00259-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00408-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame01042-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00247-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00416-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00347-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame01050-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00404-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame01033-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00328-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00579-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00048-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00971-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00135-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00282-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00912-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00715-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00900-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00668-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00640-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00594-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00172-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00498-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00631-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00060-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00463-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00563-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00232-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame01037-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00251-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00471-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00089-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00512-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00216-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00998-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00559-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00459-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00308-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00524-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame01013-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00367-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00681-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00267-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00164-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00064-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00744-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00115-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00168-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00832-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00627-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00967-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00772-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00975-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00052-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00822-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00105-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00178-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00729-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00869-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00969-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00601-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00701-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00050-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00762-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00877-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00977-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00906-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame01039-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00353-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00187-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00365-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00265-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00534-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00457-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00888-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00526-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00314-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00182-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00090-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00694-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00794-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00523-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00203-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00303-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00798-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame01018-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00531-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00431-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00063-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00827-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00856-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00112-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00643-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00720-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00935-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00024-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00293-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00960-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00903-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00616-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00281-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00767-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00381-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00128-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00622-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00010-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00741-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00929-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00858-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00587-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00730-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00102-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00599-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00134-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00138-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00769-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00714-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00801-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00862-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00962-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00391-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00246-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00409-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00358-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00262-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00433-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00991-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00450-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00883-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00983-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00442-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00153-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00817-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00974-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00030-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00387-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00583-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00483-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00165-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00018-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00491-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00638-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00069-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00645-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00114-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00850-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00374-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00792-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00458-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00558-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00680-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00454-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00401-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00084-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00333-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00321-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00096-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00889-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00207-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00997-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00897-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00782-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00435-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00319-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame01010-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame01026-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00186-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00503-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame01034-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00252-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00098-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00285-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00032-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00151-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00819-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00964-gt.pbm  \n",
            "  inflating: Figaro1k/GT/Testing/Frame00079-gt.pbm  \n",
            "   creating: Figaro1k/Original/\n",
            "  inflating: Figaro1k/Original/.DS_Store  \n",
            "   creating: Figaro1k/Original/Training/\n",
            "  inflating: Figaro1k/Original/Training/Frame00815-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00805-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00392-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00382-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00406-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00253-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00243-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00125-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00760-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00618-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00770-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00608-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00702-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00712-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00157-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00147-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00231-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00349-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00221-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00359-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00464-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01016-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00474-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01006-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00288-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00298-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00867-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00086-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00893-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00072-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00062-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00637-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00439-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00541-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00429-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00304-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00480-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00490-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00952-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00942-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00784-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00848-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00930-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00920-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00376-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00366-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01029-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00533-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00655-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00989-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00999-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00663-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00673-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00036-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00026-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00228-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00350-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00340-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00505-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00515-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00291-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00916-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00195-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00185-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00567-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00577-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00332-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00322-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00054-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00044-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00779-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00611-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00851-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00841-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00939-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00593-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00217-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01030-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00452-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01020-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00724-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00734-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00171-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00009-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00161-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00019-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00980-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00103-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00746-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00756-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00420-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00430-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00548-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00275-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00499-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00489-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00833-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00823-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00687-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00697-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00280-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00290-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00907-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00917-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00027-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00037-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00672-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00662-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00514-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00504-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00341-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00239-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00351-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00229-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00323-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00576-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00566-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00610-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00768-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00600-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00778-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00045-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00055-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00965-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00184-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00194-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01021-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00453-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01031-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00443-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00206-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00981-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00160-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00008-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00170-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00735-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00725-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00938-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00840-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00928-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00592-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00582-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00488-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00696-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00686-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00757-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00747-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00264-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00274-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01043-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00549-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00421-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00242-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00417-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00407-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00609-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00771-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00619-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00761-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00124-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00804-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00814-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00383-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00393-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00299-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00289-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00087-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00866-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00097-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00876-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00146-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00156-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00713-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00703-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01007-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00475-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01017-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00465-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00220-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00348-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00230-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00481-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00943-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00953-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00636-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00626-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00882-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00073-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00305-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00315-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00550-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00428-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00540-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00438-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01038-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00532-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01028-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00522-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00377-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00001-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00179-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00011-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00988-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00169-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00654-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00644-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00921-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00859-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00931-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00849-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00057-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00047-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00602-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00612-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00564-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00574-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00331-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00249-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00388-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00398-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00196-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00915-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00905-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00292-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00343-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00506-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00516-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00660-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00718-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00670-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00708-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00035-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00025-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00948-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00830-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00958-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00820-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00684-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00423-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01041-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00276-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00266-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00068-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00110-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00078-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00100-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00899-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00745-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00755-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00727-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00737-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00993-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00162-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00214-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00204-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00539-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00441-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00529-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01023-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00451-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00580-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00590-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00852-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00842-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00095-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00874-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00085-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00864-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00222-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01015-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00467-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00477-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00679-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00669-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00711-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00144-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00136-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00126-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00763-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00773-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00405-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00415-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00250-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00338-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00240-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00816-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00806-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00646-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00656-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00003-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00375-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00530-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00448-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00589-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00787-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00797-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00933-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00923-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00951-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00829-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00941-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00839-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00493-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00542-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01048-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00552-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00317-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00307-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00071-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00109-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00890-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00061-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00880-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00119-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00624-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00634-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00476-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01004-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00466-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01014-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00223-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00233-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00145-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00155-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00710-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00700-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00678-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00865-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00875-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00094-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00380-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00390-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00807-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00127-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00137-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00241-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00339-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00329-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00414-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00588-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00598-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00922-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00932-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00796-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00786-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00002-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00012-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00657-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00647-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00449-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00521-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00364-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00306-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00316-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00553-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01049-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00543-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00635-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00625-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00881-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00118-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00108-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00891-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00070-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00838-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00940-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00828-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00950-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00492-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00482-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00399-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00389-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00966-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00197-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00976-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00613-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00603-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00046-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00056-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00258-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00320-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00248-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00330-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00575-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00565-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00517-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00507-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00342-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00352-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00034-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00709-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00671-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00719-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00661-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00904-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00914-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00283-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00277-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01040-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00422-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00754-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00101-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00898-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00111-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00821-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00959-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00831-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00949-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00591-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00581-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00843-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00853-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00163-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00982-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00173-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00992-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00736-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00726-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01022-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00528-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00440-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01032-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00538-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00205-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00215-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00639-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00751-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00629-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00104-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00272-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00427-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00437-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01045-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00690-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00834-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00824-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00846-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00445-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00455-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01027-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00210-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00368-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00200-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00378-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00176-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00166-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00987-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00723-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00733-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00192-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00973-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00963-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00335-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00325-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00418-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00560-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00570-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00606-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00053-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00043-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00149-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00031-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00159-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00021-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00664-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00674-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01008-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00502-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00357-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00296-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00286-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00088-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00911-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00879-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00901-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00937-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00927-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00783-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00793-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00209-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00371-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00219-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00017-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00007-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00652-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00758-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00620-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00748-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00630-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00894-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00075-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00884-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00065-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00313-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00546-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00556-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00487-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00497-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00689-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00955-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00945-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00150-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00028-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00140-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00038-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00705-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01011-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00473-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01001-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00236-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00226-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00870-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00908-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00091-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00860-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00081-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00918-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00812-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00395-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00385-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00254-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00244-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00411-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00569-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00777-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00132-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00122-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00360-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00218-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00370-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00208-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00535-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00525-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00653-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00006-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00016-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00926-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00936-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00496-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00486-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00944-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00954-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00688-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00698-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00885-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00074-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00895-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00749-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00621-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00759-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00557-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00547-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00302-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00312-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00080-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00919-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00861-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00909-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00871-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00704-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00039-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00141-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00029-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00227-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00237-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01000-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00472-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00462-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00568-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00410-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00578-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00400-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00245-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00255-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00123-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00133-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00776-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00766-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00803-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00813-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00384-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00394-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00825-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00835-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00691-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00628-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00750-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00740-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01044-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00436-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00426-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00273-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00379-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00201-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00369-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00211-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01036-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00444-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00732-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00722-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00986-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00167-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00996-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00177-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00847-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00857-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00595-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00585-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00571-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00561-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00419-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00324-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00334-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00042-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00607-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00183-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00972-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00193-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00287-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00297-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00099-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00878-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00910-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00868-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00675-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00665-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00020-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00158-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00148-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00346-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00356-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00513-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01019-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01009-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00956-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00946-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00484-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00494-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00310-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00268-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00300-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00278-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00545-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00555-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00623-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00633-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00076-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00066-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00887-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00014-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00004-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00641-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00739-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00651-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00527-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00537-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00372-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00362-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00934-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00924-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00780-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00790-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00764-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00774-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00049-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00131-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00059-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00121-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00257-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00402-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00412-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00396-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00386-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00811-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00188-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00979-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00198-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00092-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00873-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00082-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00863-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01012-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00460-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01002-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00508-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00470-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00235-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00225-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00143-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00706-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00716-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00994-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00175-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00984-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00658-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00648-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00446-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01024-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00456-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00213-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00597-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00855-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00845-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00799-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00789-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00683-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00693-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00837-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00309-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00271-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00261-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00424-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01046-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00434-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00752-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00117-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00107-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00902-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00295-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00501-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00479-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00511-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00354-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00344-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00022-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00667-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00677-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00615-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00040-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00336-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00326-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00573-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00970-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00191-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00808-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00818-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00181-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00586-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00788-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00844-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00854-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00731-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00649-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00721-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00659-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00985-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00174-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00995-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00212-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01025-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00447-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01035-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01047-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00425-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00260-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00318-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00270-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00106-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00116-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00753-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00743-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00836-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00692-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00682-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00345-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00355-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00468-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00510-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00478-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00500-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00676-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00023-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00033-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00913-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00284-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00294-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00180-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00961-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00190-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00809-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00139-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00041-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00051-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00614-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00604-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00572-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00562-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00327-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00337-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00554-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00544-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00279-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00301-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00269-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00311-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00886-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00067-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00896-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00077-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00632-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00947-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00957-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00495-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00485-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00791-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00781-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00925-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00728-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00738-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00005-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00015-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00536-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00397-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00199-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00800-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00978-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00810-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00189-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00968-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00120-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00058-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00130-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00775-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00765-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00413-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00403-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00256-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00224-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00234-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00509-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame01003-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00461-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00519-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00717-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00707-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00142-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00152-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00083-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00872-org.jpg  \n",
            "  inflating: Figaro1k/Original/Training/Frame00093-org.jpg  \n",
            "   creating: Figaro1k/Original/Testing/\n",
            "  inflating: Figaro1k/Original/Testing/Frame00416-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00135-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00877-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00096-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00883-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00627-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00551-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00314-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00794-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00858-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00523-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame01039-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00645-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00168-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00010-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00178-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00238-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00281-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00906-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00974-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00964-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00601-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00769-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00929-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00583-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00207-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00442-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00990-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00113-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00558-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame01042-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00265-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00333-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00975-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00216-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00018-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00991-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00850-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00498-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00822-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00832-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00102-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00112-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00431-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00559-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00252-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00134-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00358-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00491-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00063-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00892-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00367-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00998-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00795-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00785-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00321-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00259-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00977-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00967-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00186-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00282-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00353-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00694-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00433-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00889-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00172-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00983-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame01033-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00232-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame01005-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00701-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00154-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00328-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00391-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00381-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00013-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00365-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00520-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00458-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00599-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00483-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00668-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00084-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00817-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00772-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00762-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00251-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00404-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00531-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00459-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00374-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00060-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00187-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00024-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00293-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00267-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00432-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame01050-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00744-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00079-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00888-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00069-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00695-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00685-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00450-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00741-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00114-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00262-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00680-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00856-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00584-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00594-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame01037-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00997-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00182-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00408-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00616-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame01018-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00512-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00347-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00869-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00098-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00524-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00534-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00361-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00642-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00303-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00699-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00715-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00463-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00802-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00401-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00579-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00767-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00643-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00792-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00782-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00064-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00631-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00090-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00714-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00151-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame01010-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00681-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00105-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00115-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00638-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00263-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame01026-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00454-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00409-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00052-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00617-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00962-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00900-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00089-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00030-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00503-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00897-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00729-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00247-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00969-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00801-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00518-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00153-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00165-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00720-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00730-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame01034-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00203-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00587-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00827-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00319-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00742-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00912-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00285-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00469-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00032-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00605-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00050-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00128-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00138-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00563-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00960-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00596-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00798-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00164-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00202-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00457-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00435-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00308-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00826-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00666-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00903-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00819-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00971-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00129-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00622-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00935-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00650-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00640-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00363-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00373-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00526-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00387-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00048-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00246-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00471-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame01013-org.jpg  \n",
            "  inflating: Figaro1k/Original/Testing/Frame00862-org.jpg  \n",
            "Removing unnecessary files ...\n",
            "Finished!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 2) Clone the repository and cd into it\n",
        "!git clone https://github.com/YBIGTA/pytorch-hair-segmentation.git\n",
        "import os\n",
        "os.chdir('/content/pytorch-hair-segmentation')\n",
        "\n",
        "# 3) Install necessary dependencies (PyTorch, OpenCV, NumPy)\n",
        "!pip install --quiet torch torchvision opencv-contrib-python numpy\n",
        "\n",
        "# 4) Download & prepare the Figaro-1k dataset (no args → creates ./data/Figaro1k)\n",
        "!chmod +x data/figaro.sh\n",
        "!bash data/figaro.sh\n",
        "\n",
        "# 5) Copy Figaro-1k into /content for faster I/O\n",
        "!cp -r data/Figaro1k /content/Figaro1k\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC9L2r4xSo7m"
      },
      "source": [
        "Figaro1k segmentation training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsXXmiwds3sh",
        "outputId": "a09607a8-fe8b-4e5e-e43f-11f2e77a75fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained_backbone' is deprecated since 0.13 and may be removed in the future, please use 'weights_backbone' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights_backbone' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights_backbone=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights_backbone=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 124MB/s]\n",
            "Epoch 1/10 [Train]: 100%|██████████| 210/210 [00:08<00:00, 24.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Train Loss: 0.2341\n",
            "Saved checkpoint: /content/models/deeplabv3_epoch_00.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 210/210 [00:07<00:00, 28.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 2] Train Loss: 0.1733\n",
            "Saved checkpoint: /content/models/deeplabv3_epoch_01.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 210/210 [00:07<00:00, 28.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 3] Train Loss: 0.1516\n",
            "Saved checkpoint: /content/models/deeplabv3_epoch_02.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 210/210 [00:07<00:00, 28.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 4] Train Loss: 0.1369\n",
            "Saved checkpoint: /content/models/deeplabv3_epoch_03.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10 [Train]: 100%|██████████| 210/210 [00:07<00:00, 28.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 5] Train Loss: 0.1343\n",
            "Saved checkpoint: /content/models/deeplabv3_epoch_04.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10 [Train]: 100%|██████████| 210/210 [00:07<00:00, 28.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 6] Train Loss: 0.1201\n",
            "Saved checkpoint: /content/models/deeplabv3_epoch_05.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10 [Train]: 100%|██████████| 210/210 [00:07<00:00, 28.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 7] Train Loss: 0.1150\n",
            "Saved checkpoint: /content/models/deeplabv3_epoch_06.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10 [Train]: 100%|██████████| 210/210 [00:07<00:00, 27.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 8] Train Loss: 0.1127\n",
            "Saved checkpoint: /content/models/deeplabv3_epoch_07.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10 [Train]: 100%|██████████| 210/210 [00:07<00:00, 28.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 9] Train Loss: 0.0997\n",
            "Saved checkpoint: /content/models/deeplabv3_epoch_08.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10 [Train]: 100%|██████████| 210/210 [00:07<00:00, 28.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 10] Train Loss: 0.0999\n",
            "Saved checkpoint: /content/models/deeplabv3_epoch_09.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 53/53 [00:01<00:00, 44.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Results → IoU: 0.8769,  F1: 0.9334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "################################################################################\n",
        "# Single Colab Cell: End‐to‐End Hair Segmentation with DeepLabV3+\n",
        "# (Handling “Training” & “Testing” Subfolders with PBM Masks)\n",
        "#\n",
        "# This pipeline will:\n",
        "#  1. Clone the YBIGTA hair‐segmentation repo.\n",
        "#  2. Use its data/figaro.sh script to download Figaro‐1k into the repo’s data folder,\n",
        "#     which creates:\n",
        "#       • data/Figaro1k/Original/Training/FrameXXXXX-org.jpg\n",
        "#       • data/Figaro1k/GT/Training/    FrameXXXXX-gt.pbm\n",
        "#       (and similarly for “Testing” subfolders).\n",
        "#  3. Train a DeepLabV3+ (ResNet‐50 backbone) model on all images in Original/Training.\n",
        "#  4. Evaluate IoU/F1 on all images in Original/Testing.\n",
        "#  5. Run inference on your own “flat_images” folder in Colab’s local storage to produce overlays.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#  • Select GPU runtime: Runtime → Change runtime type → GPU.\n",
        "#  • Create a folder “/content/flat_images/” via the Colab UI and upload your own .jpg/.png files there.\n",
        "################################################################################\n",
        "\n",
        "# (1) Install required libraries: PyTorch, TorchVision, OpenCV, tqdm\n",
        "!pip install --quiet torch torchvision opencv-python tqdm\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "################################################################################\n",
        "# (2) Clone the YBIGTA repo & download Figaro‐1k locally\n",
        "################################################################################\n",
        "\n",
        "if not os.path.isdir('/content/pytorch-hair-segmentation'):\n",
        "    !git clone https://github.com/YBIGTA/pytorch-hair-segmentation.git /content/pytorch-hair-segmentation\n",
        "\n",
        "os.chdir('/content/pytorch-hair-segmentation')\n",
        "\n",
        "# Run the provided script (no arguments) to download into data/Figaro1k\n",
        "# !chmod +x data/figaro.sh\n",
        "# !bash data/figaro.sh\n",
        "\n",
        "# Confirm directory structure:\n",
        "dataset_root = '/content/pytorch-hair-segmentation/data/Figaro1k'\n",
        "assert os.path.isdir(os.path.join(dataset_root, 'Original', 'Training')),  \"Original/Training not found\"\n",
        "assert os.path.isdir(os.path.join(dataset_root, 'Original', 'Testing')),  \"Original/Testing not found\"\n",
        "assert os.path.isdir(os.path.join(dataset_root, 'GT', 'Training')),        \"GT/Training not found\"\n",
        "assert os.path.isdir(os.path.join(dataset_root, 'GT', 'Testing')),         \"GT/Testing not found\"\n",
        "\n",
        "################################################################################\n",
        "# (3) Define a PyTorch Dataset class that matches “*‐org.jpg” to “*‐gt.pbm”\n",
        "################################################################################\n",
        "\n",
        "class FigaroHairDatasetNested(Dataset):\n",
        "    def __init__(self, root_dir, split=\"train\", img_size=256):\n",
        "        \"\"\"\n",
        "        root_dir: \"/content/pytorch-hair-segmentation/data/Figaro1k\"\n",
        "        split: \"train\"  → use all of Original/Training and corresponding GT/Training\n",
        "               \"test\"   → use all of Original/Testing  and corresponding GT/Testing\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.img_size = img_size\n",
        "\n",
        "        if split == \"train\":\n",
        "            img_dir  = os.path.join(root_dir, \"Original\", \"Training\")\n",
        "            mask_dir = os.path.join(root_dir, \"GT\",       \"Training\")\n",
        "        elif split == \"test\":\n",
        "            img_dir  = os.path.join(root_dir, \"Original\", \"Testing\")\n",
        "            mask_dir = os.path.join(root_dir, \"GT\",       \"Testing\")\n",
        "        else:\n",
        "            raise ValueError(\"split must be 'train' or 'test'\")\n",
        "\n",
        "        # List all image files ending with \"-org.jpg\"\n",
        "        self.img_paths = sorted([\n",
        "            os.path.join(img_dir, fname)\n",
        "            for fname in os.listdir(img_dir)\n",
        "            if fname.lower().endswith(\"-org.jpg\")\n",
        "        ])\n",
        "        # For each image, compute corresponding mask by replacing \"-org.jpg\" → \"-gt.pbm\"\n",
        "        self.mask_paths = []\n",
        "        for img_path in self.img_paths:\n",
        "            base = os.path.basename(img_path)               # e.g. \"Frame00001-org.jpg\"\n",
        "            mask_name = base.replace(\"-org.jpg\", \"-gt.pbm\")  # \"Frame00001-gt.pbm\"\n",
        "            mask_path = os.path.join(mask_dir, mask_name)\n",
        "            if not os.path.isfile(mask_path):\n",
        "                raise FileNotFoundError(f\"Mask file not found: {mask_path}\")\n",
        "            self.mask_paths.append(mask_path)\n",
        "\n",
        "        # Transforms for images\n",
        "        self.img_transform = transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "        # Transforms for masks (PBM loaded as grayscale)\n",
        "        self.mask_transform = transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size), interpolation=Image.NEAREST),\n",
        "            transforms.ToTensor(),  # yields shape (1,img_size,img_size) with values [0,1]\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path  = self.img_paths[idx]\n",
        "        mask_path = self.mask_paths[idx]\n",
        "        # Load image\n",
        "        img  = Image.open(img_path).convert(\"RGB\")\n",
        "        # Load mask (PBM format is supported by PIL)\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "        # Apply transforms\n",
        "        x = self.img_transform(img)             # (3, img_size, img_size)\n",
        "        y = self.mask_transform(mask)           # (1, img_size, img_size)\n",
        "        # Binary threshold: hair=1, background=0\n",
        "        y = (y > 0.5).float()                   # (1, img_size, img_size)\n",
        "        y = y.squeeze(0).long()                 # (img_size, img_size), dtype long\n",
        "        return x, y\n",
        "\n",
        "################################################################################\n",
        "# (4) Instantiate DataLoaders for “train” and “test”\n",
        "################################################################################\n",
        "\n",
        "batch_size = 4\n",
        "img_size   = 256\n",
        "\n",
        "train_ds = FigaroHairDatasetNested(root_dir=dataset_root, split=\"train\", img_size=img_size)\n",
        "test_ds  = FigaroHairDatasetNested(root_dir=dataset_root, split=\"test\",  img_size=img_size)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "################################################################################\n",
        "# (5) Define the Model: DeepLabV3+ with a ResNet‐50 Backbone\n",
        "################################################################################\n",
        "\n",
        "model = deeplabv3_resnet50(pretrained_backbone=True, pretrained=False, num_classes=2)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "################################################################################\n",
        "# (6) Training Loop: Loss, Optimizer, and Saving Checkpoints Locally\n",
        "################################################################################\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()   # for 2‐class segmentation\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "num_epochs  = 10\n",
        "save_folder = \"/content/models\"\n",
        "os.makedirs(save_folder, exist_ok=True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for imgs, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
        "        imgs  = imgs.to(device)    # (B,3,H,W)\n",
        "        masks = masks.to(device)   # (B,H,W)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)[\"out\"]    # (B,2,H,W)\n",
        "        loss    = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_ds)\n",
        "    print(f\"[Epoch {epoch+1}] Train Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    # Save a checkpoint for this epoch\n",
        "    ckpt_path = os.path.join(save_folder, f\"deeplabv3_epoch_{epoch:02d}.pth\")\n",
        "    torch.save(model.state_dict(), ckpt_path)\n",
        "    print(f\"Saved checkpoint: {ckpt_path}\")\n",
        "\n",
        "# After training finishes, the final checkpoint is:\n",
        "final_ckpt = os.path.join(save_folder, f\"deeplabv3_epoch_{num_epochs-1:02d}.pth\")\n",
        "\n",
        "################################################################################\n",
        "# (7) Evaluate on “Testing” Folder Using the Final Checkpoint\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(final_ckpt))\n",
        "model.eval()\n",
        "\n",
        "test_iou_sum = 0.0\n",
        "test_f1_sum  = 0.0\n",
        "num_batches  = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, masks in tqdm(test_loader, desc=\"Testing\"):\n",
        "        imgs  = imgs.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        outputs = model(imgs)[\"out\"]       # (B,2,H,W)\n",
        "        preds   = torch.argmax(outputs, dim=1)  # (B,H,W)\n",
        "\n",
        "        preds_flat = preds.view(-1)\n",
        "        masks_flat = masks.view(-1)\n",
        "\n",
        "        inter = torch.sum((preds_flat == 1) & (masks_flat == 1)).item()\n",
        "        union = torch.sum((preds_flat == 1) | (masks_flat == 1)).item()\n",
        "        iou   = 1.0 if union == 0 else inter / union\n",
        "\n",
        "        TP = inter\n",
        "        FP = torch.sum((preds_flat == 1) & (masks_flat == 0)).item()\n",
        "        FN = torch.sum((preds_flat == 0) & (masks_flat == 1)).item()\n",
        "        f1 = 1.0 if (2 * TP + FP + FN) == 0 else 2 * TP / (2 * TP + FP + FN)\n",
        "\n",
        "        test_iou_sum += iou\n",
        "        test_f1_sum  += f1\n",
        "        num_batches  += 1\n",
        "\n",
        "avg_test_iou = test_iou_sum / num_batches\n",
        "avg_test_f1  = test_f1_sum / num_batches\n",
        "print(f\"Testing Results → IoU: {avg_test_iou:.4f},  F1: {avg_test_f1:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179,
          "referenced_widgets": [
            "23ac1a3a8ecf4ec09094ae21aa817991",
            "9ed2258af4fb4643b467c3936e568c85",
            "a5f3715d7cbc48d6b459676a7444589b",
            "bc9c197b7ae141eaac1ddb55d880b1c6",
            "eb3922c952814868933578fffa63b984",
            "8afeb10959fe447ea0fc20203ad497b5",
            "940c4ce7694b43238eff6e270ac6f0c0",
            "6d3532f07f2a456186155ab2d30e6af2",
            "dbd1ffc6b21d4ef2bd4e01f77d89b325",
            "639babf06c59419b9627ae0b7023de45",
            "c2d60374f0ed4c34a0861eb6009071d3",
            "0b80fce333514e569108211bdc8a9447",
            "f199383a3b394d51896af8647077323e",
            "8bd1a7a92a694aefae73508e614e8ee3",
            "71fbf539eaab4240bd7bb73f99e2d2a3",
            "77b02f880f13481690efc5177d6fca92",
            "9db8299c2d4a4171baace78b96b1f58b",
            "98bebf69edc249f88e7f9f5e213f03d3",
            "33d142e37a1946e598e35deed5586168",
            "9e1f328c172549928e45b8e4ae8b18e8",
            "052eee3ddf9844e08a45fe3b67c28938",
            "cf4424f0336e4056b5a8bd6b0c3985e8",
            "b84c2053e08a43728bbfa2eb0feee2ca",
            "72551f1a124547bab79719961fd1b0db",
            "6c414dd774c54bf49e23215c61b20583",
            "a2e39f8ed96d40739a538aa8542df517",
            "513b2d0816294f27abd44f7284b032f1",
            "a8cb129722024720a160f9c398459f3c",
            "4f35542698004811839b80722c379f46",
            "264602f0b2df4f4291089f5d88fb4f42",
            "e68a1b79a90647ec94aacde97c49d31c",
            "7dd6d4fb8f554d73abf59fd2e0909459",
            "9d5d25bdf2994028bb8ef28b43e1f3c2",
            "4efb922081e144fc80bbb0575e501df8",
            "854d86ddbdae48d7b6b3ef7aa62e229f",
            "49f055ac5acb462b99cfa4485634d427",
            "737fac1a430f4d9a9e4111b61ddce8b6",
            "bf12ca6987b64d4ea4e0cc965e4c3cbe",
            "7d96044031594510abcbd4dac1025040",
            "6108ac494aae4137a63cd58bcfa8634f",
            "a8956c19ecaa4f8a8d91d1ed0b9897b9",
            "209921462b404299b32ead5a004925b3",
            "2623ae7bade042ada374f022725b7166",
            "9180da59d528438db1f6d73e88d5fe87",
            "97fc33175f054c218c3efa1cf14ee89b",
            "f1e529d340704e85b6357a6ef4003fed",
            "b8c2177df2f24d47b7f82541159b696e",
            "661bab5f5dda4fc2923e869973fba919",
            "2ea4e4565ed741208958a53bf7328629",
            "8a49e21273ca48d189e431c13b10f978",
            "4e7a52db094e473ba5a794c97649f657",
            "7521e3e6e90f40b6b5f905d252e80d94",
            "f513a73fe8104db3b693caccdac477fb"
          ]
        },
        "id": "Kc4XlxrThpXZ",
        "outputId": "7d328bd1-1050-4204-8eb3-af2c5d92875f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23ac1a3a8ecf4ec09094ae21aa817991"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98bebf69edc249f88e7f9f5e213f03d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload                         : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f35542698004811839b80722c379f46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  ...ir-segmentation/deeplabv3_final.pth:   3%|3         | 5.01MB /  159MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6108ac494aae4137a63cd58bcfa8634f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model uploaded to: https://huggingface.co/alamb98/deeplabv3-hair-segmentation\n"
          ]
        }
      ],
      "source": [
        "################################################################################\n",
        "# (8) Upload DeepLabV3+ Hair Segmentation Model to Hugging Face Hub\n",
        "################################################################################\n",
        "\n",
        "!pip install -q huggingface_hub\n",
        "\n",
        "from huggingface_hub import login, create_repo, upload_folder\n",
        "import json\n",
        "\n",
        "# 🔐 Log in to Hugging Face (you'll paste your token once)\n",
        "login()\n",
        "\n",
        "# Define repo info\n",
        "#hf_username = \"RyanThawkho\"  # ← CHANGE THIS\n",
        "hf_username = \"alamb98\"  # ← CHANGE THIS\n",
        "repo_name   = \"deeplabv3-hair-segmentation\"\n",
        "repo_id     = f\"{hf_username}/{repo_name}\"\n",
        "\n",
        "# Create model repo if it doesn't exist yet\n",
        "create_repo(repo_id, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "# Save final model weights (if not already done)\n",
        "torch.save(model.state_dict(), \"deeplabv3_final.pth\")\n",
        "\n",
        "# Save basic metadata\n",
        "metadata = {\n",
        "    \"model_name\": \"DeepLabV3+ (ResNet50 backbone)\",\n",
        "    \"image_size\": [img_size, img_size],\n",
        "    \"num_classes\": 2,\n",
        "    \"labels\": [\"background\", \"hair\"],\n",
        "    \"test_iou\": round(avg_test_iou, 4),\n",
        "    \"test_f1\": round(avg_test_f1, 4)\n",
        "}\n",
        "with open(\"metadata.json\", \"w\") as f:\n",
        "    json.dump(metadata, f)\n",
        "\n",
        "# Upload to Hugging Face (only the files we want)\n",
        "upload_folder(\n",
        "    folder_path=\".\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\",\n",
        "    allow_patterns=[\"deeplabv3_final.pth\", \"metadata.json\"]\n",
        ")\n",
        "\n",
        "print(f\"✅ Model uploaded to: https://huggingface.co/{repo_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR0StmgvIyII"
      },
      "source": [
        "download & combine our two datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDA4mZ8RHoeY"
      },
      "outputs": [],
      "source": [
        "# (0) Install necessary tools\n",
        "!pip install --quiet gdown\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "import gdown\n",
        "\n",
        "################################################################################\n",
        "# (1) GOOGLE DRIVE LINKS FOR THE TWO ZIP FILES\n",
        "#     Replace these with your actual shareable links.\n",
        "################################################################################\n",
        "\n",
        "LINK_ZIP1 = \"https://drive.google.com/file/d/1nP7u3PqnvdYEUnE9oxRo-SAvOO28w_Az/view?usp=drive_link\"\n",
        "LINK_ZIP2 = \"https://drive.google.com/file/d/1iRoI1JI7kVPP4dFbR1xiZM4lOpf9Osy5/view?usp=drive_link\"\n",
        "\n",
        "def download_from_drive(shareable_link: str, dest_path: str):\n",
        "    \"\"\"\n",
        "    Given a Google Drive shareable link, download the file to dest_path using gdown.\n",
        "    \"\"\"\n",
        "    file_id = shareable_link.split(\"/d/\")[1].split(\"/\")[0]\n",
        "    gdown_url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "    print(f\"Downloading {shareable_link} → {dest_path}\")\n",
        "    gdown.download(gdown_url, dest_path, quiet=False)\n",
        "\n",
        "# Paths where we'll save the two downloaded ZIPs\n",
        "ZIP1_PATH = \"/content/dataset1.zip\"\n",
        "ZIP2_PATH = \"/content/dataset2.zip\"\n",
        "\n",
        "download_from_drive(LINK_ZIP1, ZIP1_PATH)\n",
        "download_from_drive(LINK_ZIP2, ZIP2_PATH)\n",
        "\n",
        "################################################################################\n",
        "# (2) UNZIP EACH INTO ITS OWN FOLDER\n",
        "################################################################################\n",
        "\n",
        "UNZIP_DIR1 = \"/content/unzipped_dataset1\"\n",
        "UNZIP_DIR2 = \"/content/unzipped_dataset2\"\n",
        "\n",
        "def reset_dir(path: str):\n",
        "    if os.path.isdir(path):\n",
        "        shutil.rmtree(path)\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "reset_dir(UNZIP_DIR1)\n",
        "reset_dir(UNZIP_DIR2)\n",
        "\n",
        "print(f\"Unzipping {ZIP1_PATH} → {UNZIP_DIR1}\")\n",
        "with zipfile.ZipFile(ZIP1_PATH, \"r\") as zf1:\n",
        "    zf1.extractall(UNZIP_DIR1)\n",
        "\n",
        "print(f\"Unzipping {ZIP2_PATH} → {UNZIP_DIR2}\")\n",
        "with zipfile.ZipFile(ZIP2_PATH, \"r\") as zf2:\n",
        "    zf2.extractall(UNZIP_DIR2)\n",
        "\n",
        "################################################################################\n",
        "# (3) INSPECT and LOCATE the TRUE “BASE” FOLDER containing train/valid/test\n",
        "#\n",
        "# We write a helper that skips any “__MACOSX” and finds a subfolder that\n",
        "# contains “train” (i.e., has a “train” directory).\n",
        "################################################################################\n",
        "\n",
        "def find_split_base(unzip_dir: str) -> str:\n",
        "    \"\"\"\n",
        "    Traverse one level down under unzip_dir, ignoring folders named \"__MACOSX\".\n",
        "    If any folder at that level contains a 'train' subdirectory, return its path.\n",
        "    If unzip_dir itself contains 'train', return unzip_dir.\n",
        "    Otherwise, raise an error.\n",
        "    \"\"\"\n",
        "    # Case A: unzip_dir/train exists\n",
        "    if os.path.isdir(os.path.join(unzip_dir, \"train\")):\n",
        "        return unzip_dir\n",
        "\n",
        "    # Otherwise check children (skip \"__MACOSX\")\n",
        "    for child in sorted(os.listdir(unzip_dir)):\n",
        "        if child.startswith(\"__MACOSX\"):\n",
        "            continue\n",
        "        child_path = os.path.join(unzip_dir, child)\n",
        "        if os.path.isdir(child_path) and os.path.isdir(os.path.join(child_path, \"train\")):\n",
        "            return child_path\n",
        "\n",
        "    raise FileNotFoundError(\n",
        "        f\"Could not locate a 'train' folder under {unzip_dir} or its valid child.\"\n",
        "    )\n",
        "\n",
        "# Determine BASE1 and BASE2 automatically\n",
        "BASE1 = find_split_base(UNZIP_DIR1)\n",
        "BASE2 = find_split_base(UNZIP_DIR2)\n",
        "print(f\"Detected BASE1 = {BASE1}\")\n",
        "print(f\"Detected BASE2 = {BASE2}\")\n",
        "\n",
        "# (Optional) Show contents at that base to double-check\n",
        "print(\"\\nContents at BASE1 (first two levels):\")\n",
        "!find \"{BASE1}\" -maxdepth 2 | sed 's/^/  /'\n",
        "print(\"\\nContents at BASE2 (first two levels):\")\n",
        "!find \"{BASE2}\" -maxdepth 2 | sed 's/^/  /'\n",
        "\n",
        "################################################################################\n",
        "# (4) DEFINE COMBINED OUTPUT DIRECTORY\n",
        "################################################################################\n",
        "\n",
        "COMBINED_DIR = \"/content/norwood_dataset_combined\"\n",
        "reset_dir(COMBINED_DIR)\n",
        "\n",
        "################################################################################\n",
        "# (5) MERGE “train/Level X”, “valid/Level X”, “test/Level X” FROM BOTH BASES\n",
        "#\n",
        "# Copy every .jpg/.jpeg/.png under BASE1/<split>/<level>/ and BASE2/<split>/<level>/\n",
        "# into COMBINED_DIR/<split>/<level>/. If a filename collides, append \"_dup\".\n",
        "################################################################################\n",
        "\n",
        "def merge_split_levels(src_base: str, dst_base: str):\n",
        "    for split in [\"train\", \"valid\", \"test\"]:\n",
        "        for level in [f\"Level {i}\" for i in range(2, 8)]:\n",
        "            src_dir = os.path.join(src_base, split, level)\n",
        "            dst_dir = os.path.join(dst_base, split, level)\n",
        "\n",
        "            if not os.path.isdir(src_dir):\n",
        "                continue\n",
        "\n",
        "            os.makedirs(dst_dir, exist_ok=True)\n",
        "\n",
        "            for fname in os.listdir(src_dir):\n",
        "                lower = fname.lower()\n",
        "                if not (lower.endswith(\".jpg\") or lower.endswith(\".jpeg\") or lower.endswith(\".png\")):\n",
        "                    continue\n",
        "\n",
        "                src_path = os.path.join(src_dir, fname)\n",
        "                dst_path = os.path.join(dst_dir, fname)\n",
        "\n",
        "                # If a file of same name exists, append \"_dup\"\n",
        "                if os.path.exists(dst_path):\n",
        "                    name, ext = os.path.splitext(fname)\n",
        "                    dst_path = os.path.join(dst_dir, f\"{name}_dup{ext}\")\n",
        "\n",
        "                shutil.copyfile(src_path, dst_path)\n",
        "\n",
        "print(\"Merging unzipped_dataset1 → combined\")\n",
        "merge_split_levels(BASE1, COMBINED_DIR)\n",
        "\n",
        "print(\"Merging unzipped_dataset2 → combined\")\n",
        "merge_split_levels(BASE2, COMBINED_DIR)\n",
        "\n",
        "print(f\"\\n✅ Done! Combined dataset is now at: {COMBINED_DIR}\")\n",
        "for split in [\"train\", \"valid\", \"test\"]:\n",
        "    for level in [f\"Level {i}\" for i in range(2, 8)]:\n",
        "        combined_path = os.path.join(COMBINED_DIR, split, level)\n",
        "        if os.path.isdir(combined_path):\n",
        "            count = len(os.listdir(combined_path))\n",
        "            print(f\"  • {split}/{level}: {count} files\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_cB9hp3Z_AE"
      },
      "source": [
        "Segment our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-fk98NPjej1"
      },
      "outputs": [],
      "source": [
        "# ────────────────────────────────────────────────────────────────\n",
        "# UPLOAD THE TRAINED MODEL CHECKPOINT TO HUGGING FACE\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "!pip install --quiet huggingface_hub\n",
        "\n",
        "from huggingface_hub import login, create_repo, upload_file\n",
        "\n",
        "# 1. Log into Hugging Face\n",
        "login(token=\"your_huggingface_token\")  # Replace with your HF token\n",
        "\n",
        "# 2. Create or point to your repo\n",
        "repo_id = \"your-username/deeplabv3-hair-segmentation\"  # Choose a name\n",
        "create_repo(repo_id, private=True)  # Set to public=False if you want it private\n",
        "\n",
        "# 3. Upload the checkpoint\n",
        "upload_file(\n",
        "    path_or_fileobj=\"/content/models/deeplabv3_epoch_09.pth\",  # Your model file\n",
        "    path_in_repo=\"deeplabv3_epoch_09.pth\",                      # How it appears in repo\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAzyMhpwR-lV"
      },
      "source": [
        "## ***Segmentation with lines not fill***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rt5h5ZW4R9ne"
      },
      "outputs": [],
      "source": [
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# COLAB‐CELL: Run Hair Segmentation on train/valid/test → Save Boundary‐Only Overlays\n",
        "#             under /content/segmented_boundary/<split>/Level X/…\n",
        "#\n",
        "# Instead of filling the entire hair region in solid red, this version draws only\n",
        "# the red outline (contour) of the detected hair mask on top of the original image.\n",
        "#\n",
        "# Assumes:\n",
        "#  • Your ZIP (already unzipped) created these folders:\n",
        "#      /content/norwood_dataset/hyehye2.v3i.folder/train/Level 2 … Level 7/\n",
        "#      /content/norwood_dataset/hyehye2.v3i.folder/valid/Level 2 … Level 7/\n",
        "#      /content/norwood_dataset/hyehye2.v3i.folder/test/Level 2 … Level 7/\n",
        "#  • You have a DeepLabV3+ checkpoint at /content/models/deeplabv3_epoch_09.pth\n",
        "#  • You want to save “hair‐region boundary in red” overlays for all images.\n",
        "#\n",
        "# After this cell finishes, browse in Colab’s left “Files” pane under\n",
        "# /content/segmented_boundary/ to see the saved boundary overlays.\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "!pip install --quiet torch torchvision opencv-python tqdm\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1) Load the pretrained DeepLabV3+ model checkpoint\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "CHECKPOINT_PATH = \"/content/models/deeplabv3_epoch_09.pth\"  # <— adjust if needed\n",
        "\n",
        "model = deeplabv3_resnet50(pretrained_backbone=True, pretrained=False, num_classes=2)\n",
        "model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=DEVICE))\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "# 2) Define the same image transforms used during segmentation inference\n",
        "IMG_SIZE = 256\n",
        "transform_img = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# 3) Point to your “ordinarily‐located” folders of raw images\n",
        "BASE = \"/content/norwood_dataset_combined\"\n",
        "SPLITS = [\"train\", \"valid\", \"test\"]\n",
        "LEVELS = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", … \"Level 7\"]\n",
        "\n",
        "# 4) Create /content/segmented_boundary/<split>/Level X/… if they don’t already exist\n",
        "SEGMENTED_BOUNDARY_ROOT = \"/content/segmented_boundary\"\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVELS:\n",
        "        os.makedirs(os.path.join(SEGMENTED_BOUNDARY_ROOT, split, lvl), exist_ok=True)\n",
        "\n",
        "# 5) Loop over each split (train/valid/test) & each Level X, run segmentation,\n",
        "#    find contours of hair mask, draw red contours on original, and save\n",
        "for split in SPLITS:\n",
        "    print(f\"\\n▶ Processing {split.upper()} set …\")\n",
        "    for lvl in LEVELS:\n",
        "        raw_dir = os.path.join(BASE, split, lvl)\n",
        "        save_dir = os.path.join(SEGMENTED_BOUNDARY_ROOT, split, lvl)\n",
        "\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Expected folder not found: {raw_dir}\")\n",
        "\n",
        "        print(f\"  • Level: {lvl}  →  {len(os.listdir(raw_dir))} images\")\n",
        "        for fname in tqdm(os.listdir(raw_dir), desc=f\"    {lvl}\", leave=False):\n",
        "            if not fname.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\")):\n",
        "                continue\n",
        "\n",
        "            # Load the raw image\n",
        "            img_path = os.path.join(raw_dir, fname)\n",
        "            orig = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "            # Resize to model’s expected size\n",
        "            resized = orig.resize((IMG_SIZE, IMG_SIZE))\n",
        "            x = transform_img(resized).unsqueeze(0).to(DEVICE)  # (1,3,256,256)\n",
        "\n",
        "            # Forward‐pass → get binary mask (0=background, 1=hair)\n",
        "            with torch.no_grad():\n",
        "                out = model(x)[\"out\"]                             # (1,2,256,256)\n",
        "                pred = torch.argmax(out, dim=1).squeeze(0).cpu().numpy()  # (256,256)\n",
        "\n",
        "            # Resize mask back to original image resolution\n",
        "            mask_resized = cv2.resize(pred.astype(np.uint8),\n",
        "                                      (orig.width, orig.height),\n",
        "                                      interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "            # Convert mask to 8‐bit single‐channel for contour detection\n",
        "            mask_uint8 = (mask_resized * 255).astype(np.uint8)\n",
        "\n",
        "            # 6) Find contours of the hair region\n",
        "            contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "            # 7) Draw red contours on top of the original\n",
        "            overlay = np.array(orig).astype(np.uint8).copy()\n",
        "            cv2.drawContours(overlay, contours, -1, color=(255, 0, 0), thickness=2)  # (H,W,3), BGR by OpenCV\n",
        "\n",
        "            # Note: cv2 draws in BGR by default; convert original to BGR before drawing:\n",
        "            # Actually, overlay is in RGB, so we need to convert the color tuple:\n",
        "            # But here, cv2.drawContours expects BGR if the image is a NumPy array in BGR.\n",
        "            # Since `overlay` is actually in RGB order, we can swap the color channels:\n",
        "            #   overlay[..., [2,1,0]] if needed.\n",
        "            # To keep it simple, let's convert overlay to BGR, draw, then convert back:\n",
        "\n",
        "            # Step a) Convert RGB→BGR\n",
        "            overlay_bgr = cv2.cvtColor(np.array(orig).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
        "            cv2.drawContours(overlay_bgr, contours, -1, color=(0, 0, 255), thickness=2)  # Red in BGR\n",
        "            # Step b) Convert back to RGB\n",
        "            overlay_rgb = cv2.cvtColor(overlay_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # 8) Save just the contour overlay (no blending)\n",
        "            base_name = os.path.splitext(fname)[0]\n",
        "            out_name = base_name + \"_boundary.png\"\n",
        "            out_path = os.path.join(save_dir, out_name)\n",
        "            Image.fromarray(overlay_rgb).save(out_path)\n",
        "\n",
        "    print(f\"✔ Finished boundary‐only segmentation for {split}.\\n\")\n",
        "\n",
        "print(\"\\n✅ All boundary‐only overlays are now saved under /content/segmented_boundary/\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_IYEEh0SKQB"
      },
      "source": [
        "Apply clahe on the segmented dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0vaK5jJ17-WY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# (A) Change these two paths to match your environment\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "INPUT_ROOT  = \"/content/segmented_boundary\"\n",
        "OUTPUT_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# (B) CLAHE setup\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# (C) Walk splits → levels → images, apply CLAHE, save to OUTPUT_ROOT\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "for split in [\"train\", \"valid\", \"test\"]:\n",
        "    split_in_dir  = os.path.join(INPUT_ROOT, split)\n",
        "    split_out_dir = os.path.join(OUTPUT_ROOT, split)\n",
        "\n",
        "    if not os.path.isdir(split_in_dir):\n",
        "        print(f\"[WARN] Skipping missing split folder: {split_in_dir}\")\n",
        "        continue\n",
        "\n",
        "    # Create the split folder in the output if it doesn't exist\n",
        "    os.makedirs(split_out_dir, exist_ok=True)\n",
        "\n",
        "    # Each “level” subfolder (Level 2, Level 3, … Level 7)\n",
        "    for level_folder in sorted(os.listdir(split_in_dir)):\n",
        "        level_in_path = os.path.join(split_in_dir, level_folder)\n",
        "        if not os.path.isdir(level_in_path):\n",
        "            continue  # skip any stray files\n",
        "\n",
        "        # Create the same level subfolder in output\n",
        "        level_out_path = os.path.join(split_out_dir, level_folder)\n",
        "        os.makedirs(level_out_path, exist_ok=True)\n",
        "\n",
        "        # Process each image in this level folder (allow .jpg, .jpeg, .png)\n",
        "        for img_name in sorted(os.listdir(level_in_path)):\n",
        "            lower = img_name.lower()\n",
        "            if not (lower.endswith(\".jpg\") or lower.endswith(\".jpeg\") or lower.endswith(\".png\")):\n",
        "                continue\n",
        "\n",
        "            input_path  = os.path.join(level_in_path, img_name)\n",
        "            output_name = os.path.splitext(img_name)[0] + \".png\"  # save as .png\n",
        "            output_path = os.path.join(level_out_path, output_name)\n",
        "\n",
        "            img = cv2.imread(input_path)\n",
        "            if img is None:\n",
        "                print(f\"  [SKIP] Could not read image: {input_path}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Convert BGR → LAB\n",
        "                lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
        "                l, a, b = cv2.split(lab)\n",
        "\n",
        "                # Apply CLAHE on the L channel\n",
        "                cl = clahe.apply(l)\n",
        "\n",
        "                # Merge the enhanced L channel back with A and B\n",
        "                merged = cv2.merge((cl, a, b))\n",
        "\n",
        "                # Convert LAB → BGR\n",
        "                enhanced_img = cv2.cvtColor(merged, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "                # Save as PNG (retaining the same basename)\n",
        "                cv2.imwrite(output_path, enhanced_img)\n",
        "                print(f\"  ✓ Saved CLAHE: {output_path}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  [ERROR] processing {input_path}: {e}\")\n",
        "\n",
        "print(\"\\n➡️  Done with CLAHE preprocessing.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF21mIEvSoKG"
      },
      "source": [
        "combine raw with clahe in one folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d48_Yry_vFv"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import shutil\n",
        "\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "# # (A) Adjust these four source paths to match your environment exactly\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# SEG_ORIG_ROOT   = \"/content/segmented_boundary\"\n",
        "# SEG_CLAHE_ROOT  = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# RAW_ORIG_ROOT   = \"/content/norwood_dataset/hyehye2.v3i.folder 2\"\n",
        "# RAW_CLAHE_ROOT  = \"/content/norwood_dataset/hyehye2.v3i.folder 2_clahe\"\n",
        "\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "# # (B) Define your two “combined” destination roots\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# COMBINED_SEG_ROOT = \"/content/combined_segmented_boundary\"\n",
        "# COMBINED_RAW_ROOT = \"/content/combined_norwood_raw\"\n",
        "\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "# # (C) List of splits & levels (we assume exactly these subfolders exist in each source)\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# SPLITS      = [\"train\", \"valid\", \"test\"]\n",
        "# LEVEL_NAMES = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "# # (D) Helper to “safe copy” an entire directory tree, appending a suffix to filenames\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# def copy_tree_with_suffix(src_root: str, dst_root: str, suffix: str):\n",
        "#     \"\"\"\n",
        "#     Walk through every file under `src_root/<split>/<level>/...` and copy it into\n",
        "#     `dst_root/<split>/<level>/...`, appending `suffix` before the file extension.\n",
        "\n",
        "#     Example:\n",
        "#       src_root = \"/content/segmented_boundary\"\n",
        "#       dst_root = \"/content/combined_segmented_boundary\"\n",
        "#       suffix   = \"_orig\"\n",
        "\n",
        "#       then \"/content/segmented_boundary/train/Level 2/img123.jpg\"\n",
        "#       → copies to \"/content/combined_segmented_boundary/train/Level 2/img123_orig.jpg\"\n",
        "#     \"\"\"\n",
        "#     for split in SPLITS:\n",
        "#         for lvl in LEVEL_NAMES:\n",
        "#             in_dir  = os.path.join(src_root, split, lvl)\n",
        "#             out_dir = os.path.join(dst_root, split, lvl)\n",
        "\n",
        "#             if not os.path.isdir(in_dir):\n",
        "#                 # If a split/level folder doesn’t exist, skip it\n",
        "#                 continue\n",
        "\n",
        "#             # Ensure the destination folder exists\n",
        "#             os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "#             for fname in os.listdir(in_dir):\n",
        "#                 # Only copy image files (assume .png or .jpg)\n",
        "#                 if not (fname.lower().endswith(\".jpg\") or fname.lower().endswith(\".jpeg\") or fname.lower().endswith(\".png\")):\n",
        "#                     continue\n",
        "\n",
        "#                 src_path = os.path.join(in_dir, fname)\n",
        "#                 name, ext = os.path.splitext(fname)\n",
        "#                 dst_fname = f\"{name}{suffix}{ext}\"\n",
        "#                 dst_path = os.path.join(out_dir, dst_fname)\n",
        "\n",
        "#                 try:\n",
        "#                     shutil.copyfile(src_path, dst_path)\n",
        "#                 except Exception as e:\n",
        "#                     print(f\"  [ERROR] copying {src_path} → {dst_path}: {e}\")\n",
        "\n",
        "\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "# # (E) 1) Copy all segmented (no‐CLAHE) into COMBINED_SEG_ROOT with suffix \"_orig\"\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# print(\"Copying segmented (no‐CLAHE) → combined (suffix=_orig)\")\n",
        "# copy_tree_with_suffix(SEG_ORIG_ROOT, COMBINED_SEG_ROOT, suffix=\"_orig\")\n",
        "\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "# # (F) 2) Copy all segmented (CLAHE) into COMBINED_SEG_ROOT with suffix \"_clahe\"\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# print(\"Copying segmented (CLAHE) → combined (suffix=_clahe)\")\n",
        "# copy_tree_with_suffix(SEG_CLAHE_ROOT, COMBINED_SEG_ROOT, suffix=\"_clahe\")\n",
        "\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "# # (G) 3) Copy all raw (no‐CLAHE) into COMBINED_RAW_ROOT with suffix \"_orig\"\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# print(\"Copying raw (no‐CLAHE) → combined (suffix=_orig)\")\n",
        "# copy_tree_with_suffix(RAW_ORIG_ROOT, COMBINED_RAW_ROOT, suffix=\"_orig\")\n",
        "\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "# # (H) 4) Copy all raw (CLAHE) into COMBINED_RAW_ROOT with suffix \"_clahe\"\n",
        "# # ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# print(\"Copying raw (CLAHE) → combined (suffix=_clahe)\")\n",
        "# copy_tree_with_suffix(RAW_CLAHE_ROOT, COMBINED_RAW_ROOT, suffix=\"_clahe\")\n",
        "\n",
        "# print(\"\\n✅ Done! Two combined folders created:\")\n",
        "# print(f\"  • Combined segmented images: {COMBINED_SEG_ROOT}\")\n",
        "# print(f\"  • Combined raw images:       {COMBINED_RAW_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rv43QQq6d5RU"
      },
      "outputs": [],
      "source": [
        "# prompt: write a code which counts that amount of images in each folder in /content/segmented_boundary_clahe\n",
        "\n",
        "import os\n",
        "\n",
        "target_directory = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "if not os.path.isdir(target_directory):\n",
        "  print(f\"Directory not found: {target_directory}\")\n",
        "else:\n",
        "  print(f\"Counting images in subfolders of: {target_directory}\")\n",
        "  for root, dirs, files in os.walk(target_directory):\n",
        "    image_count = 0\n",
        "    for name in files:\n",
        "      # Check for common image file extensions (case-insensitive)\n",
        "      if name.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
        "        image_count += 1\n",
        "    # Only print if the directory is not the root and contains images\n",
        "    if root != target_directory and image_count > 0:\n",
        "      print(f\"  {root}: {image_count} images\")\n",
        "    # If it's the root directory, print its image count if any\n",
        "    elif root == target_directory and image_count > 0:\n",
        "         print(f\"  {root}: {image_count} images (directly in root)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9a-BwG5oaWao"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oWo1qno5lnO"
      },
      "source": [
        "# ***MODELS***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPjDFfIUczNH"
      },
      "source": [
        "Loop for ratio [IGNORE]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7MwdcSdc1eN"
      },
      "outputs": [],
      "source": [
        "# ################################################################################\n",
        "# # Colab‐Cell: Hyperparameter Search over expand_ratio (0%–60%) for EfficientNet-B0\n",
        "# # on “Cropped‐Interior” Norwood‐Stage Classification\n",
        "# #\n",
        "# # We loop expand_ratio from 0.00 to 0.60 in 0.01 increments. For each ratio:\n",
        "# #   1) Instantiate a dataset that crops original images to the hair‐region’s bounding\n",
        "# #      box expanded by expand_ratio * max(width, height).\n",
        "# #   2) Train EfficientNet-B0 (pretrained, final layer → 6 classes) for 10 epochs on train/valid.\n",
        "# #   3) Evaluate on the test split, compute macro F1.\n",
        "# #   4) Track the highest test‐set macro F1 and the corresponding expand_ratio.\n",
        "# #\n",
        "# # At the end, we print the best expand_ratio and F1, and save that model’s weights.\n",
        "# #\n",
        "# # BEFORE RUNNING:\n",
        "# #  • Ensure folders exist:\n",
        "# #      /content/segmented_boundary/<split>/Level X/*_boundary.png\n",
        "# #      /content/norwood_dataset/hyehye2.v3i.folder/<split>/Level X>/*.jpg\n",
        "# #  • Select GPU runtime: Runtime → Change runtime type → GPU.\n",
        "# ################################################################################\n",
        "\n",
        "# # (0) Install & import dependencies\n",
        "# !pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "# import os\n",
        "# import cv2\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torchvision import transforms\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from PIL import Image\n",
        "# import numpy as np\n",
        "# from tqdm import tqdm\n",
        "# from collections import Counter\n",
        "\n",
        "# # EfficientNet-B0\n",
        "# from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# # sklearn metrics\n",
        "# from sklearn.metrics import confusion_matrix, f1_score\n",
        "# import pandas as pd\n",
        "# import seaborn as sns\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# ################################################################################\n",
        "# # (1) Custom Dataset: Crop to an expanded hair‐region bounding box (variable margin)\n",
        "# ################################################################################\n",
        "\n",
        "# class NorwoodCroppedExpandedDataset(Dataset):\n",
        "#     def __init__(self,\n",
        "#                  raw_root: str,\n",
        "#                  boundary_root: str,\n",
        "#                  split: str,\n",
        "#                  level_names: list,\n",
        "#                  transform=None,\n",
        "#                  expand_ratio: float = 0.10):\n",
        "#         \"\"\"\n",
        "#         raw_root:      \"/content/norwood_dataset/hyehye2.v3i.folder\"\n",
        "#         boundary_root: \"/content/segmented_boundary\"\n",
        "#         split:         one of \"train\", \"valid\", \"test\"\n",
        "#         level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "#         transform:     torchvision transforms applied to each cropped image\n",
        "#         expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "#         \"\"\"\n",
        "#         self.samples = []  # (raw_img_path, boundary_img_path, label_index)\n",
        "#         self.transform = transform\n",
        "#         self.expand_ratio = expand_ratio\n",
        "\n",
        "#         for idx, lvl in enumerate(level_names):\n",
        "#             raw_dir = os.path.join(raw_root, split, lvl)\n",
        "#             bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "#             if not os.path.isdir(raw_dir):\n",
        "#                 raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "#             if not os.path.isdir(bnd_dir):\n",
        "#                 raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "#             for fname in os.listdir(bnd_dir):\n",
        "#                 if not fname.lower().endswith(\"_boundary.png\"):\n",
        "#                     continue\n",
        "#                 base = fname[:-len(\"_boundary.png\")]  # e.g. \"21-Front_jpg.rf...\"\n",
        "#                 raw_fname = base + \".jpg\"\n",
        "#                 raw_path = os.path.join(raw_dir, raw_fname)\n",
        "#                 bnd_path = os.path.join(bnd_dir, fname)\n",
        "#                 if not os.path.isfile(raw_path):\n",
        "#                     raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "#                 self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "#         if len(self.samples) == 0:\n",
        "#             raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.samples)\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "#         # Load raw and boundary as RGB NumPy arrays\n",
        "#         raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "#         bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "#         H, W, _ = raw_img.shape\n",
        "\n",
        "#         # Binary mask of red boundary via HSV threshold\n",
        "#         hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "#         lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "#         lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "#         mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "#         mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "#         red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "#         # Find external contours\n",
        "#         contours, _ = cv2.findContours(\n",
        "#             red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "#         )\n",
        "#         if len(contours) == 0:\n",
        "#             x, y, w, h = 0, 0, W, H\n",
        "#         else:\n",
        "#             all_pts = np.vstack(contours).squeeze()\n",
        "#             xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "#             x_min, x_max = xs.min(), xs.max()\n",
        "#             y_min, y_max = ys.min(), ys.max()\n",
        "#             x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "#         # Expand bounding box by expand_ratio * max(w, h)\n",
        "#         margin = int(self.expand_ratio * max(w, h))\n",
        "#         x1 = max(0, x - margin)\n",
        "#         y1 = max(0, y - margin)\n",
        "#         x2 = min(W, x + w + margin)\n",
        "#         y2 = min(H, y + h + margin)\n",
        "\n",
        "#         # Crop raw image\n",
        "#         cropped = raw_img[y1:y2, x1:x2]\n",
        "#         if cropped.size == 0:\n",
        "#             cropped = raw_img.copy()\n",
        "\n",
        "#         # Convert to PIL and apply transforms\n",
        "#         cropped_pil = Image.fromarray(cropped)\n",
        "#         if self.transform:\n",
        "#             cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "#         return cropped_pil, label\n",
        "\n",
        "# ################################################################################\n",
        "# # (2) Paths, Classes, and Transforms\n",
        "# ################################################################################\n",
        "\n",
        "# RAW_ROOT      = \"/content/norwood_dataset/hyehye2.v3i.folder\"\n",
        "# BOUNDARY_ROOT = \"/content/segmented_boundary\"\n",
        "# SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "# LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# # Verify folder structure once\n",
        "# for split in SPLITS:\n",
        "#     for lvl in LEVEL_NAMES:\n",
        "#         raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "#         bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "#         if not os.path.isdir(raw_dir):\n",
        "#             raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "#         if not os.path.isdir(bnd_dir):\n",
        "#             raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "# print(\"Verified raw + boundary folder structure.\")\n",
        "\n",
        "# # Transforms: resize to 224×224, augment on train\n",
        "# train_transform = transforms.Compose([\n",
        "#     transforms.Resize((224, 224)),\n",
        "#     transforms.RandomHorizontalFlip(p=0.5),\n",
        "#     transforms.RandomRotation(10),\n",
        "#     transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#                          std=[0.229, 0.224, 0.225]),\n",
        "# ])\n",
        "# eval_transform = transforms.Compose([\n",
        "#     transforms.Resize((224, 224)),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "#                          std=[0.229, 0.224, 0.225]),\n",
        "# ])\n",
        "\n",
        "# ################################################################################\n",
        "# # (3) Hyperparameter Search Loop: Expand Ratio from 0.00 to 0.60 (step 0.01)\n",
        "# ################################################################################\n",
        "\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# best_f1    = 0.0\n",
        "# best_ratio = 0.0\n",
        "# best_model_path = \"/content/best_efficientnetb0_expand.pth\"\n",
        "\n",
        "# for ratio_percent in range(0, 101):  # 0 → 60 inclusive\n",
        "#     expand_ratio = ratio_percent / 100.0\n",
        "#     print(f\"\\n▶ Starting expand_ratio = {expand_ratio:.2f}\")\n",
        "\n",
        "#     # (3a) Create Datasets & DataLoaders for this expand_ratio\n",
        "#     train_dataset = NorwoodCroppedExpandedDataset(\n",
        "#         raw_root=RAW_ROOT,\n",
        "#         boundary_root=BOUNDARY_ROOT,\n",
        "#         split=\"train\",\n",
        "#         level_names=LEVEL_NAMES,\n",
        "#         transform=train_transform,\n",
        "#         expand_ratio=expand_ratio\n",
        "#     )\n",
        "#     valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "#         raw_root=RAW_ROOT,\n",
        "#         boundary_root=BOUNDARY_ROOT,\n",
        "#         split=\"valid\",\n",
        "#         level_names=LEVEL_NAMES,\n",
        "#         transform=eval_transform,\n",
        "#         expand_ratio=expand_ratio\n",
        "#     )\n",
        "#     test_dataset = NorwoodCroppedExpandedDataset(\n",
        "#         raw_root=RAW_ROOT,\n",
        "#         boundary_root=BOUNDARY_ROOT,\n",
        "#         split=\"test\",\n",
        "#         level_names=LEVEL_NAMES,\n",
        "#         transform=eval_transform,\n",
        "#         expand_ratio=expand_ratio\n",
        "#     )\n",
        "\n",
        "#     train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "#                               num_workers=2, pin_memory=True)\n",
        "#     valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "#                               num_workers=2, pin_memory=True)\n",
        "#     test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "#                               num_workers=2, pin_memory=True)\n",
        "\n",
        "#     # (3b) Build a fresh EfficientNet-B0 for each ratio\n",
        "#     model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "#     in_features = model._fc.in_features\n",
        "#     model._fc = nn.Linear(in_features, len(LEVEL_NAMES))  # 6 classes\n",
        "#     model = model.to(device)\n",
        "\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "#     optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "#     # (3c) Train for 10 epochs, track best valid accuracy\n",
        "#     best_valid_acc = 0.0\n",
        "#     checkpoint_path = f\"/content/temp_efficientnetb0_expand_{ratio_percent:02d}.pth\"\n",
        "\n",
        "#     for epoch in range(1, 11):  # 10 epochs\n",
        "#         # --- Training Phase ---\n",
        "#         model.train()\n",
        "#         running_loss = 0.0\n",
        "#         running_corrects = 0\n",
        "#         total_train = 0\n",
        "\n",
        "#         for inputs, labels in train_loader:\n",
        "#             inputs = inputs.to(device)\n",
        "#             labels = labels.to(device)\n",
        "\n",
        "#             optimizer.zero_grad()\n",
        "#             outputs = model(inputs)\n",
        "#             loss = criterion(outputs, labels)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             running_loss += loss.item() * inputs.size(0)\n",
        "#             _, preds = torch.max(outputs, 1)\n",
        "#             running_corrects += torch.sum(preds == labels).item()\n",
        "#             total_train += inputs.size(0)\n",
        "\n",
        "#         epoch_acc = running_corrects / total_train\n",
        "#         print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "#         # --- Validation Phase ---\n",
        "#         model.eval()\n",
        "#         val_corrects = 0\n",
        "#         total_valid = 0\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             for inputs, labels in valid_loader:\n",
        "#                 inputs = inputs.to(device)\n",
        "#                 labels = labels.to(device)\n",
        "\n",
        "#                 outputs = model(inputs)\n",
        "#                 _, preds = torch.max(outputs, 1)\n",
        "#                 val_corrects += torch.sum(preds == labels).item()\n",
        "#                 total_valid += inputs.size(0)\n",
        "\n",
        "#         val_acc = val_corrects / total_valid\n",
        "#         print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "#         # Save checkpoint if validation improved\n",
        "#         if val_acc > best_valid_acc:\n",
        "#             best_valid_acc = val_acc\n",
        "#             torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "#     # Load the best checkpoint for test evaluation\n",
        "#     model.load_state_dict(torch.load(checkpoint_path))\n",
        "#     model.eval()\n",
        "\n",
        "#     # (3d) Evaluate on test split → compute macro F1\n",
        "#     all_preds = []\n",
        "#     all_labels = []\n",
        "#     with torch.no_grad():\n",
        "#         for inputs, labels in test_loader:\n",
        "#             inputs = inputs.to(device)\n",
        "#             labels = labels.to(device)\n",
        "#             outputs = model(inputs)\n",
        "#             _, preds = torch.max(outputs, 1)\n",
        "#             all_preds.extend(preds.cpu().tolist())\n",
        "#             all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "#     f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "#     print(f\"▶ expand_ratio={expand_ratio:.2f} → Test F1 (macro): {f1_macro:.4f}\")\n",
        "\n",
        "#     # Track best F1 and save that model\n",
        "#     if f1_macro > best_f1:\n",
        "#         best_f1 = f1_macro\n",
        "#         best_ratio = expand_ratio\n",
        "#         torch.save(model.state_dict(), best_model_path)\n",
        "#         print(f\"  → New best F1: {best_f1:.4f} at expand_ratio={best_ratio:.2f}\")\n",
        "\n",
        "#     # Clean up before next iteration\n",
        "#     del model\n",
        "#     torch.cuda.empty_cache()\n",
        "\n",
        "# ################################################################################\n",
        "# # (4) Report Best Result\n",
        "# ################################################################################\n",
        "\n",
        "# print(f\"\\n✅ Best expand_ratio: {best_ratio:.2f}  with macro F1: {best_f1:.4f}\")\n",
        "# print(f\"Saved best model weights to: {best_model_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HZwpgjndXDv"
      },
      "source": [
        "efficientNetBO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3jh0lSwdbO-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "\n",
        "# --- Dataset Paths ---\n",
        "base_dir = r\"C:\\Users\\Ryan\\Desktop\\hair_fall_images_clahe\"\n",
        "train_dir = os.path.join(base_dir, \"train\")\n",
        "valid_dir = os.path.join(base_dir, \"valid\")\n",
        "test_dir  = os.path.join(base_dir, \"test\")\n",
        "\n",
        "# --- Parameters ---\n",
        "img_size = (224, 224)\n",
        "batch_size = 32\n",
        "\n",
        "# --- Data Generators ---\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    zoom_range=0.2,\n",
        "    rotation_range=15,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "test_datagen  = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(train_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
        "valid_gen = valid_datagen.flow_from_directory(valid_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical')\n",
        "test_gen  = test_datagen.flow_from_directory(test_dir, target_size=img_size, batch_size=batch_size, class_mode='categorical', shuffle=False)\n",
        "\n",
        "# --- Load EfficientNetB0 ---\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# --- Add Custom Classification Layers ---\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "predictions = Dense(train_gen.num_classes, activation='softmax')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# UnFreeze last 20 layers\n",
        "for layer in base_model.layers[-20:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# --- Compile & Train ---\n",
        "model.compile(optimizer=Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# add more weight to level 6,7\n",
        "class_weights_dict = {\n",
        "    0: 1.0,\n",
        "    1: 1.2,\n",
        "    2: 1.0,\n",
        "    3: 1.4,\n",
        "    4: 1.8,  # Level 6\n",
        "    5: 2.0   # Level 7\n",
        "}\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    patience=8,  # stop if no val improvement after 8 epochs\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# reduce_lr = ReduceLROnPlateau(\n",
        "#     patience=3,  # reduce LR if no val improvement for 3 epochs\n",
        "#     factor=0.5,  # reduce LR by half\n",
        "#     verbose=1\n",
        "# )\n",
        "\n",
        "model.fit(\n",
        "    train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    epochs=30,\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=[early_stop]\n",
        ")\n",
        "\n",
        "# --- Evaluate on Test Set ---\n",
        "loss, acc = model.evaluate(test_gen)\n",
        "print(f\"\\nTest Loss: {loss:.4f}, Accuracy: {acc:.4f}\")\n",
        "\n",
        "# --- Prediction & Reporting ---\n",
        "y_pred = model.predict(test_gen)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = test_gen.classes\n",
        "class_labels = list(test_gen.class_indices.keys())\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred_classes, target_names=class_labels))\n",
        "\n",
        "f1_macro = f1_score(y_true, y_pred_classes, average='macro')\n",
        "f1_weighted = f1_score(y_true, y_pred_classes, average='weighted')\n",
        "print(f\"Macro F1-score: {f1_macro:.4f}\")\n",
        "print(f\"Weighted F1-score: {f1_weighted:.4f}\")\n",
        "\n",
        "# --- Confusion Matrix ---\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('EfficientNetB0 - Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5My0z4wprFg9"
      },
      "source": [
        "Effeicientnetb0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjMPFRjODOWC"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build EfficientNet-B0 classifier, move to GPU\n",
        "################################################################################\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load pretrained EfficientNet-B0, replace final layer\n",
        "model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "in_features = model._fc.in_features\n",
        "model._fc = nn.Linear(in_features, len(LEVEL_NAMES))  # 6 classes\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK2hdDzFpi-q"
      },
      "source": [
        "EFFECIENTNETB0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGl9Phkwhp6o"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build EfficientNet-B0 classifier, move to GPU\n",
        "################################################################################\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load pretrained EfficientNet-B0, replace final layer\n",
        "model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "in_features = model._fc.in_features\n",
        "model._fc = nn.Linear(in_features, len(LEVEL_NAMES))  # 6 classes\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQQJS2iPcaaj"
      },
      "source": [
        "Comparasion with other models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPYMGZ7nld6t"
      },
      "source": [
        "ResNet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "55b2e6f329dd4dd7bb33a6175dd0674e",
            "e579c80be5e74ff59197ac83de7bb8cb",
            "2e23acffcd00437db07e8641759c4949",
            "b1a8c2f4fb024c79abc4eba51f3d8d0e",
            "43fcb1b22c364c748b89891857011e29",
            "9e9dfafe028e4e33b623fccfb828f019",
            "33b976d13ecc4cf1999b4cc33b7114df",
            "38049d92939545b1916e72b26b63300d",
            "dec697e9c3a3493e9db275f06b2c6bb0",
            "3b25f03278f84042a8aed2320f1761a9",
            "048a31892d2340ff846fec152bc12692",
            "f4014b7f04944ab8a51a76b6dedbf5af",
            "f165849f936c437a8959af0066f9814c",
            "f322c0123ef54d588aff5dde49ebf8e3",
            "b2f1f595119d4fec940f7a7ce0e45328",
            "d4d841d9e50f4720ab3a17548d8295ee",
            "bc5c4f26fae443958de1229d2c100d8a",
            "39e4f3661a69460ab58315c42a027257",
            "d47cdf87bc6740fba2c1043503d847ff",
            "128ed33b329f4389935dbce22aa09d29",
            "030dc819b0af410b83aa907b7defcd02",
            "74625abb9239466c95559809a23350fe",
            "ce72272810f74e6fb7975efa8f99cb96",
            "c26884c312c54ee88b24970417d55bff",
            "491fc6f9dd9a4f23bdbabb3ca486fda9",
            "2c70402c6aa048d6832c538634a30bd8",
            "37da321c152c4e3b80b8951dbf5ae661",
            "e58aeb7d2e94401e8937332a8600da9f",
            "1816f61f91c84c4c94b5940b8dc01aad",
            "9f273521e5744713a358e2f61f6f2531",
            "690b5d40d42945ba88bead3144aa5e4d",
            "3e378bfd7b59415789cf29923cb69c74",
            "50e64c611d224f26ae2a58b829068b25",
            "d476a017467b4900aa222b8d28a7828d",
            "2a31a6a494854aa09484c456a3aea269"
          ]
        },
        "id": "-yuLhcA0cZ9r",
        "outputId": "bda6a97b-95ee-46a6-f292-f2f4740d931b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verified combined raw + combined segmented folder structure.\n",
            "\n",
            "▶ Running with expand_ratio = 0.01\n",
            "Train samples: 9069\n",
            "Valid samples: 895\n",
            "Test samples:  416\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 284/284 [01:02<00:00,  4.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 1] Train Acc: 0.7071\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  5.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 1] Valid Acc: 0.7430\n",
            "      → New best valid acc: 0.7430. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 284/284 [01:02<00:00,  4.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 2] Train Acc: 0.8560\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  5.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 2] Valid Acc: 0.7933\n",
            "      → New best valid acc: 0.7933. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 284/284 [01:01<00:00,  4.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 3] Train Acc: 0.9136\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  5.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 3] Valid Acc: 0.7821\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 284/284 [01:02<00:00,  4.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 4] Train Acc: 0.9416\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  5.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 4] Valid Acc: 0.7989\n",
            "      → New best valid acc: 0.7989. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 [Train]: 100%|██████████| 284/284 [01:02<00:00,  4.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 5] Train Acc: 0.9557\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  5.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 5] Valid Acc: 0.8067\n",
            "      → New best valid acc: 0.8067. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 [Train]: 100%|██████████| 284/284 [01:02<00:00,  4.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 6] Train Acc: 0.9626\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  5.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 6] Valid Acc: 0.8179\n",
            "      → New best valid acc: 0.8179. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 [Train]: 100%|██████████| 284/284 [01:01<00:00,  4.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 7] Train Acc: 0.9700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  5.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 7] Valid Acc: 0.8212\n",
            "      → New best valid acc: 0.8212. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 [Train]: 100%|██████████| 284/284 [01:01<00:00,  4.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 8] Train Acc: 0.9771\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  5.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 8] Valid Acc: 0.8279\n",
            "      → New best valid acc: 0.8279. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 [Train]: 100%|██████████| 284/284 [01:01<00:00,  4.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 9] Train Acc: 0.9776\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  5.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 9] Valid Acc: 0.8168\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 [Train]: 100%|██████████| 284/284 [01:01<00:00,  4.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 10] Train Acc: 0.9805\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  5.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 10] Valid Acc: 0.8346\n",
            "      → New best valid acc: 0.8346. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 13/13 [00:02<00:00,  4.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "▶ Final Test Accuracy: 85.34%  (416 samples)\n",
            "▶ Final Test Macro F1: 0.7631\n",
            "\n",
            "Confusion Matrix:\n",
            "         Level 2  Level 3  Level 4  Level 5  Level 6  Level 7\n",
            "Level 2      194       13        2        0        0        1\n",
            "Level 3       10       83        2        0        0        0\n",
            "Level 4        1        7       34        5        4        0\n",
            "Level 5        0        2        2        8        1        0\n",
            "Level 6        0        1        3        2       14        0\n",
            "Level 7        0        1        0        1        3       22\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Level 2     0.9463    0.9238    0.9349       210\n",
            "     Level 3     0.7757    0.8737    0.8218        95\n",
            "     Level 4     0.7907    0.6667    0.7234        51\n",
            "     Level 5     0.5000    0.6154    0.5517        13\n",
            "     Level 6     0.6364    0.7000    0.6667        20\n",
            "     Level 7     0.9565    0.8148    0.8800        27\n",
            "\n",
            "    accuracy                         0.8534       416\n",
            "   macro avg     0.7676    0.7657    0.7631       416\n",
            "weighted avg     0.8601    0.8534    0.8547       416\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55b2e6f329dd4dd7bb33a6175dd0674e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e23acffcd00437db07e8641759c4949",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f322c0123ef54d588aff5dde49ebf8e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload                         : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "491fc6f9dd9a4f23bdbabb3ca486fda9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  ...orch-hair-segmentation/resnet18.pth:   1%|1         |  567kB / 44.8MB            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Model uploaded to: https://huggingface.co/alamb98/resnet18\n"
          ]
        }
      ],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import json\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "with open(\"/content/labels.json\", \"w\") as f:\n",
        "    json.dump(LEVEL_NAMES, f)\n",
        "\n",
        "deploy_cfg = {\n",
        "    \"image_size\": [224, 224],\n",
        "    \"normalize_mean\": [0.485, 0.456, 0.406],\n",
        "    \"normalize_std\": [0.229, 0.224, 0.225],\n",
        "    \"model_name\": MODEL_NAME\n",
        "}\n",
        "with open(\"/content/preprocess.json\", \"w\") as f:\n",
        "    json.dump(deploy_cfg, f)\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build your classifier & move to GPU\n",
        "################################################################################\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "MODEL_NAME = \"resnet18\"   # ← change this to mobilenet_v2, resnet18, vgg16, or custom_cnn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if MODEL_NAME == \"efficientnet-b0\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v2\":\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet18\":\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vgg16\":\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    in_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"custom_cnn\":\n",
        "    # Define your CustomCNN somewhere above:\n",
        "    # class CustomCNN(nn.Module):\n",
        "    #     def __init__(self, num_classes): …\n",
        "    #     def forward(self, x): …\n",
        "    model = CustomCNN(num_classes=len(LEVEL_NAMES))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown MODEL_NAME: {MODEL_NAME}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# (7) Upload best model weights + metadata to Hugging Face Hub\n",
        "################################################################################\n",
        "\n",
        "!pip install -q huggingface_hub\n",
        "\n",
        "from huggingface_hub import login, create_repo, upload_folder\n",
        "\n",
        "# 🔐 Log in with your Hugging Face token (only once per session)\n",
        "login()\n",
        "\n",
        "# Set your repo ID — change this to your actual username/repo name\n",
        "hf_username = \"alamb98\"  # ← change this\n",
        "model_name = MODEL_NAME  # or mobilenetv3..., effnetb0...\n",
        "repo_id = f\"{hf_username}/{model_name}\"\n",
        "\n",
        "# Create a new model repo or skip if it exists\n",
        "create_repo(repo_id, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "# Save model weights\n",
        "torch.save(model.state_dict(), f\"{model_name}.pth\")\n",
        "\n",
        "# Save metadata\n",
        "metadata = {\n",
        "    \"model_name\": model_name,\n",
        "    \"image_size\": [224, 224],\n",
        "    \"expand_ratio\": EXPAND_RATIO,\n",
        "    \"labels\": LEVEL_NAMES,\n",
        "    \"test_accuracy\": round(test_acc, 4),\n",
        "    \"macro_f1_score\": round(test_f1_macro, 4)\n",
        "}\n",
        "with open(\"metadata.json\", \"w\") as f:\n",
        "    json.dump(metadata, f)\n",
        "\n",
        "# Upload both files to Hugging Face\n",
        "upload_folder(folder_path=\".\", repo_id=repo_id, repo_type=\"model\", allow_patterns=[\n",
        "    f\"{model_name}.pth\",\n",
        "    \"metadata.json\"\n",
        "])\n",
        "\n",
        "print(f\"\\n✅ Model uploaded to: https://huggingface.co/{repo_id}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ca6uL9jSwsV"
      },
      "source": [
        "RUN IT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "KmJkrs_1peu-",
        "outputId": "f0cec7c0-693b-4d17-c5ed-d7bfdc0d456e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            " 84%|████████▍ | 238/284 [01:48<00:21,  2.19it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1597275892.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1597275892.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mcropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mraw_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mraw_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mimg_pil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcropped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mimg_pil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_pil\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3327\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstrides\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tobytes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3329\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3330\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tostring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3331\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "# 0. Install requirements\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch huggingface_hub\n",
        "\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "# 1. Imports\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "import os, json, cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "from huggingface_hub import login, create_repo, upload_folder\n",
        "\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "# 2. Setup\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "MODEL_NAME = \"resnet18\"\n",
        "EXPAND_RATIO = 0.01\n",
        "hf_username = \"alamb98\"  # <- change if needed\n",
        "\n",
        "RAW_ROOT = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "SPLITS = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES = [f\"Level {i}\" for i in range(2, 8)]\n",
        "\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "# 3. Dataset\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self, raw_root, boundary_root, split, level_names, transform=None, expand_ratio=0.01):\n",
        "        self.samples = []\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "                name, ext = os.path.splitext(fname)\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "                if os.path.isfile(raw_path):\n",
        "                    self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        mask1 = cv2.inRange(hsv, (0, 100, 100), (10, 255, 255))\n",
        "        mask2 = cv2.inRange(hsv, (160, 100, 100), (180, 255, 255))\n",
        "        red_mask = cv2.bitwise_or(mask1, mask2)\n",
        "        contours, _ = cv2.findContours(red_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            x, y, w, h = np.min(all_pts[:, 0]), np.min(all_pts[:, 1]), np.ptp(all_pts[:, 0]), np.ptp(all_pts[:, 1])\n",
        "\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "        cropped = raw_img[y1:y2, x1:x2] if raw_img[y1:y2, x1:x2].size else raw_img.copy()\n",
        "\n",
        "        img_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            img_pil = self.transform(img_pil)\n",
        "        return img_pil, label\n",
        "\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "# 4. Transforms & DataLoaders\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(0.1, 0.1, 0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_set = NorwoodCroppedExpandedDataset(RAW_ROOT, BOUNDARY_ROOT, \"train\", LEVEL_NAMES, train_transform, EXPAND_RATIO)\n",
        "valid_set = NorwoodCroppedExpandedDataset(RAW_ROOT, BOUNDARY_ROOT, \"valid\", LEVEL_NAMES, eval_transform, EXPAND_RATIO)\n",
        "test_set  = NorwoodCroppedExpandedDataset(RAW_ROOT, BOUNDARY_ROOT, \"test\",  LEVEL_NAMES, eval_transform, EXPAND_RATIO)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(valid_set, batch_size=32)\n",
        "test_loader  = DataLoader(test_set,  batch_size=32)\n",
        "\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "# 5. Model Training\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, len(LEVEL_NAMES))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "for epoch in range(10):  # keep small for demo\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, labels in tqdm(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "# 6. Evaluation on Test Set\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES))\n",
        "print(f\"✅ Test Accuracy: {test_acc:.4f}, Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "# 7. Upload to Hugging Face\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "from huggingface_hub import login, create_repo, upload_folder\n",
        "\n",
        "# Login to Hugging Face (only once per session)\n",
        "login()  # You’ll be asked to paste your HF token\n",
        "\n",
        "# Define repo ID\n",
        "repo_id = f\"{hf_username}/{MODEL_NAME}\"\n",
        "\n",
        "# Create repo if it doesn't exist\n",
        "create_repo(repo_id, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "# Save model weights\n",
        "torch.save(model.state_dict(), f\"{MODEL_NAME}.pth\")\n",
        "\n",
        "# Save metadata\n",
        "metadata = {\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"expand_ratio\": EXPAND_RATIO,\n",
        "    \"image_size\": [224, 224],\n",
        "    \"labels\": LEVEL_NAMES,\n",
        "    \"test_accuracy\": round(test_acc, 4),\n",
        "    \"macro_f1_score\": round(test_f1_macro, 4)\n",
        "}\n",
        "with open(\"metadata.json\", \"w\") as f:\n",
        "    json.dump(metadata, f)\n",
        "\n",
        "# Upload model and metadata\n",
        "upload_folder(\n",
        "    folder_path=\".\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\",\n",
        "    allow_patterns=[f\"{MODEL_NAME}.pth\", \"metadata.json\"]\n",
        ")\n",
        "\n",
        "print(f\"\\n✅ Model uploaded to: https://huggingface.co/{repo_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rYMDUBGlTXe"
      },
      "source": [
        "mobilenet_v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ndvzg-qYkz6M",
        "outputId": "6f1e98a9-61a4-40ef-8760-25e6e3b5ed8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Verified combined raw + combined segmented folder structure.\n",
            "\n",
            "▶ Running with expand_ratio = 0.01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 9069\n",
            "Valid samples: 895\n",
            "Test samples:  416\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 108MB/s]\n",
            "Epoch 1/10 [Train]: 100%|██████████| 284/284 [02:08<00:00,  2.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 1] Train Acc: 0.6788\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Valid]: 100%|██████████| 28/28 [00:10<00:00,  2.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 1] Valid Acc: 0.7441\n",
            "      → New best valid acc: 0.7441. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 284/284 [02:02<00:00,  2.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 2] Train Acc: 0.8220\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Valid]: 100%|██████████| 28/28 [00:09<00:00,  3.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 2] Valid Acc: 0.8067\n",
            "      → New best valid acc: 0.8067. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 284/284 [02:13<00:00,  2.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 3] Train Acc: 0.8906\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Valid]: 100%|██████████| 28/28 [00:11<00:00,  2.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 3] Valid Acc: 0.8235\n",
            "      → New best valid acc: 0.8235. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 284/284 [03:00<00:00,  1.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 4] Train Acc: 0.9251\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Valid]: 100%|██████████| 28/28 [00:14<00:00,  1.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 4] Valid Acc: 0.8369\n",
            "      → New best valid acc: 0.8369. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 [Train]: 100%|██████████| 284/284 [02:16<00:00,  2.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 5] Train Acc: 0.9420\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 [Valid]: 100%|██████████| 28/28 [00:10<00:00,  2.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 5] Valid Acc: 0.8246\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 [Train]: 100%|██████████| 284/284 [02:03<00:00,  2.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 6] Train Acc: 0.9611\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 [Valid]: 100%|██████████| 28/28 [00:09<00:00,  2.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 6] Valid Acc: 0.8324\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 [Train]: 100%|██████████| 284/284 [02:06<00:00,  2.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 7] Train Acc: 0.9670\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 [Valid]: 100%|██████████| 28/28 [00:14<00:00,  1.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 7] Valid Acc: 0.8291\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 [Train]: 100%|██████████| 284/284 [02:59<00:00,  1.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 8] Train Acc: 0.9685\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 [Valid]: 100%|██████████| 28/28 [00:14<00:00,  1.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 8] Valid Acc: 0.8391\n",
            "      → New best valid acc: 0.8391. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 [Train]: 100%|██████████| 284/284 [02:04<00:00,  2.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 9] Train Acc: 0.9728\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 [Valid]: 100%|██████████| 28/28 [00:10<00:00,  2.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 9] Valid Acc: 0.8335\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 [Train]: 100%|██████████| 284/284 [02:03<00:00,  2.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 10] Train Acc: 0.9723\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 [Valid]: 100%|██████████| 28/28 [00:10<00:00,  2.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 10] Valid Acc: 0.8369\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 13/13 [00:04<00:00,  2.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "▶ Final Test Accuracy: 82.21%  (416 samples)\n",
            "▶ Final Test Macro F1: 0.7163\n",
            "\n",
            "Confusion Matrix:\n",
            "         Level 2  Level 3  Level 4  Level 5  Level 6  Level 7\n",
            "Level 2      188       20        2        0        0        0\n",
            "Level 3       11       78        5        1        0        0\n",
            "Level 4        3        6       33        5        3        1\n",
            "Level 5        1        0        4        4        2        2\n",
            "Level 6        1        1        0        0       17        1\n",
            "Level 7        1        0        0        0        4       22\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Level 2     0.9171    0.8952    0.9060       210\n",
            "     Level 3     0.7429    0.8211    0.7800        95\n",
            "     Level 4     0.7500    0.6471    0.6947        51\n",
            "     Level 5     0.4000    0.3077    0.3478        13\n",
            "     Level 6     0.6538    0.8500    0.7391        20\n",
            "     Level 7     0.8462    0.8148    0.8302        27\n",
            "\n",
            "    accuracy                         0.8221       416\n",
            "   macro avg     0.7183    0.7226    0.7163       416\n",
            "weighted avg     0.8234    0.8221    0.8210       416\n",
            "\n"
          ]
        }
      ],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import json\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "with open(\"/content/labels.json\", \"w\") as f:\n",
        "    json.dump(LEVEL_NAMES, f)\n",
        "\n",
        "deploy_cfg = {\n",
        "    \"image_size\": [224, 224],\n",
        "    \"normalize_mean\": [0.485, 0.456, 0.406],\n",
        "    \"normalize_std\": [0.229, 0.224, 0.225],\n",
        "    \"model_name\": MODEL_NAME\n",
        "}\n",
        "with open(\"/content/preprocess.json\", \"w\") as f:\n",
        "    json.dump(deploy_cfg, f)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build your classifier & move to GPU\n",
        "################################################################################\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "MODEL_NAME = \"mobilenet_v2\"   # ← change this to mobilenet_v2, resnet18, vgg16, or custom_cnn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if MODEL_NAME == \"efficientnet-b0\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v2\":\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet18\":\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vgg16\":\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    in_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"custom_cnn\":\n",
        "    # Define your CustomCNN somewhere above:\n",
        "    # class CustomCNN(nn.Module):\n",
        "    #     def __init__(self, num_classes): …\n",
        "    #     def forward(self, x): …\n",
        "    model = CustomCNN(num_classes=len(LEVEL_NAMES))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown MODEL_NAME: {MODEL_NAME}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "## best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_mobilenet_v2_expand001.pth\"\n",
        "\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5vddSu1lO-C"
      },
      "source": [
        "vgg16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouCXSHrVk93T"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build your classifier & move to GPU\n",
        "################################################################################\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "MODEL_NAME = \"vgg16\"   # ← change this to mobilenet_v2, resnet18, vgg16, or custom_cnn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if MODEL_NAME == \"efficientnet-b0\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v2\":\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet18\":\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vgg16\":\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    in_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"custom_cnn\":\n",
        "    # Define your CustomCNN somewhere above:\n",
        "    # class CustomCNN(nn.Module):\n",
        "    #     def __init__(self, num_classes): …\n",
        "    #     def forward(self, x): …\n",
        "    model = CustomCNN(num_classes=len(LEVEL_NAMES))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown MODEL_NAME: {MODEL_NAME}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmDPYr8DnW2K"
      },
      "source": [
        "effecientnet b0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2wfMIWGOnIGK",
        "outputId": "981ee91f-bc8c-44f7-b637-54050cce812f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verified combined raw + combined segmented folder structure.\n",
            "\n",
            "▶ Running with expand_ratio = 0.01\n",
            "Train samples: 9069\n",
            "Valid samples: 895\n",
            "Test samples:  416\n",
            "Loaded pretrained weights for efficientnet-b0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 284/284 [01:04<00:00,  4.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 1] Train Acc: 0.5964\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  5.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 1] Valid Acc: 0.7050\n",
            "      → New best valid acc: 0.7050. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 284/284 [01:03<00:00,  4.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 2] Train Acc: 0.7856\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  4.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 2] Valid Acc: 0.7844\n",
            "      → New best valid acc: 0.7844. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 284/284 [01:03<00:00,  4.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 3] Train Acc: 0.8661\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  4.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 3] Valid Acc: 0.8145\n",
            "      → New best valid acc: 0.8145. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 284/284 [01:02<00:00,  4.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 4] Train Acc: 0.9080\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  4.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 4] Valid Acc: 0.8369\n",
            "      → New best valid acc: 0.8369. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 [Train]: 100%|██████████| 284/284 [01:03<00:00,  4.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 5] Train Acc: 0.9397\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  4.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 5] Valid Acc: 0.8391\n",
            "      → New best valid acc: 0.8391. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 [Train]: 100%|██████████| 284/284 [01:03<00:00,  4.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 6] Train Acc: 0.9507\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  4.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 6] Valid Acc: 0.8380\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 [Train]: 100%|██████████| 284/284 [01:03<00:00,  4.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 7] Train Acc: 0.9612\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  4.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 7] Valid Acc: 0.8436\n",
            "      → New best valid acc: 0.8436. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 [Train]: 100%|██████████| 284/284 [01:03<00:00,  4.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 8] Train Acc: 0.9693\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  4.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 8] Valid Acc: 0.8324\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 [Train]: 100%|██████████| 284/284 [01:02<00:00,  4.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 9] Train Acc: 0.9733\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  4.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 9] Valid Acc: 0.8469\n",
            "      → New best valid acc: 0.8469. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 [Train]: 100%|██████████| 284/284 [01:03<00:00,  4.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 10] Train Acc: 0.9772\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  4.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 10] Valid Acc: 0.8570\n",
            "      → New best valid acc: 0.8570. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 13/13 [00:02<00:00,  4.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "▶ Final Test Accuracy: 86.54%  (416 samples)\n",
            "▶ Final Test Macro F1: 0.7775\n",
            "\n",
            "Confusion Matrix:\n",
            "         Level 2  Level 3  Level 4  Level 5  Level 6  Level 7\n",
            "Level 2      191       14        3        2        0        0\n",
            "Level 3        4       82        8        1        0        0\n",
            "Level 4        1        3       43        3        0        1\n",
            "Level 5        0        1        3        7        1        1\n",
            "Level 6        0        1        0        1       15        3\n",
            "Level 7        0        1        1        0        3       22\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Level 2     0.9745    0.9095    0.9409       210\n",
            "     Level 3     0.8039    0.8632    0.8325        95\n",
            "     Level 4     0.7414    0.8431    0.7890        51\n",
            "     Level 5     0.5000    0.5385    0.5185        13\n",
            "     Level 6     0.7895    0.7500    0.7692        20\n",
            "     Level 7     0.8148    0.8148    0.8148        27\n",
            "\n",
            "    accuracy                         0.8654       416\n",
            "   macro avg     0.7707    0.7865    0.7775       416\n",
            "weighted avg     0.8729    0.8654    0.8679       416\n",
            "\n"
          ]
        }
      ],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build your classifier & move to GPU\n",
        "################################################################################\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "# You can now set MODEL_NAME to one of:\n",
        "# \"efficientnet-b0\", \"efficientnet-b2\",\n",
        "# \"mobilenet_v2\", \"mobilenet_v3_large\",\n",
        "# \"resnet18\", \"resnet34\", \"resnet50\",\n",
        "# \"resnext50_32x4d\",\n",
        "# \"vgg16\",\n",
        "# \"densenet121\",\n",
        "# \"shufflenet_v2_x1_0\",\n",
        "# \"convnext_tiny\",\n",
        "# \"vit_b_16\",\n",
        "# \"swin_tiny_patch4_window7_224\",\n",
        "# or \"custom_cnn\"\n",
        "\n",
        "MODEL_NAME = \"efficientnet-b0\"   # ← change as desired\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if MODEL_NAME == \"efficientnet-b0\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"efficientnet-b2\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v2\":\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v3_large\":\n",
        "    model = models.mobilenet_v3_large(pretrained=True)\n",
        "    in_features = model.classifier[3].in_features\n",
        "    model.classifier[3] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet18\":\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet34\":\n",
        "    model = models.resnet34(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet50\":\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnext50_32x4d\":\n",
        "    model = models.resnext50_32x4d(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vgg16\":\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    in_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"densenet121\":\n",
        "    model = models.densenet121(pretrained=True)\n",
        "    in_features = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"shufflenet_v2_x1_0\":\n",
        "    model = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"convnext_tiny\":\n",
        "    model = models.convnext_tiny(pretrained=True)\n",
        "    in_features = model.classifier[2].in_features\n",
        "    model.classifier[2] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vit_b_16\":\n",
        "    model = models.vit_b_16(pretrained=True)\n",
        "    in_features = model.heads.head.in_features\n",
        "    model.heads.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"swin_tiny_patch4_window7_224\":\n",
        "    model = models.swin_t(pretrained=True)\n",
        "    in_features = model.head.in_features\n",
        "    model.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"custom_cnn\":\n",
        "    # Make sure you have defined CustomCNN somewhere above:\n",
        "    # class CustomCNN(nn.Module):\n",
        "    #     def __init__(self, num_classes): …\n",
        "    #     def forward(self, x): …\n",
        "    model = CustomCNN(num_classes=len(LEVEL_NAMES))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown MODEL_NAME: {MODEL_NAME}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197,
          "referenced_widgets": [
            "4e73577f142b4abba9f5f242ffa04d99",
            "ab9df7184f9d4ef0891d6a1aeefd534f",
            "50ce5572c8e24a25b5be8ca4bcf8c3ba",
            "2382c53eee6241ddb6039a8dc8b1fe4c",
            "c730a45d27704e00a2a87f7ea30323c4",
            "5d99d9b066ff4cdeb007d98b70368c0b",
            "c56c50d4b97346a4b17a7786f01a59db",
            "053f7812e3a14f1eae6762629835f624",
            "3dcfb9d923e6461b82719dbfdcd48747",
            "669fb11cb724446990b9394a28b6deb2",
            "028ac75023f24cc88993dc7b1177c08f",
            "441c0431704f41f8bf89e092180a69bf",
            "f47ac73baea642549dbafee5f4212607",
            "3049829651814c45951bb0cb889cc280",
            "25cfa84cc88f4400982255dc21059f94",
            "9bb1db85d930439789026854be8bf656",
            "dd55b87d468442c890a3942ed914c5fa",
            "f33b6fec8c444c38b51d0be21162140c",
            "9fb1d080fc6f4210b0dd6b56a799a839",
            "6d1cceebff77421da9171af587e5bb92",
            "255ffb9efcc042b99a2197864f3b4a7e",
            "39d2a5f78e9e4eebbdc69b9c1a313f93",
            "036eebc17d3042eab30113c74ba97e81",
            "0172a41630864ffcb0e9252eb2d6248f",
            "eba0fa1932d549beb037df7e10444cb4",
            "85427a41d3ed46a7860cde4c7fc75abe",
            "4b3f5b0576ee4beba97baf04ab4f9ef1",
            "1f3ba421d43a4a9ea67d8e77455a13b4",
            "db40513ae30f4966961fb109aa064c0d",
            "fd23641e80f046108169bbf66dfa86cf",
            "42e1eaee788e49e78ad6cb4636422443",
            "701a6c57af6d40598e022be19c49a038",
            "e7f8ab59c56e497791b7ec0b0339b4bc",
            "919c718171594ef9b471595764e24f05",
            "32ac2c5f1e294ba3b3aba773ac4ed87c"
          ]
        },
        "id": "Z_HDMxAuZ5r-",
        "outputId": "b2076b0e-6525-44cd-ea70-ca6b732668f4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e73577f142b4abba9f5f242ffa04d99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50ce5572c8e24a25b5be8ca4bcf8c3ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3049829651814c45951bb0cb889cc280",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload                         : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eba0fa1932d549beb037df7e10444cb4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  ...gmentation/efficientnet-b0_best.pth:   3%|3         |  554kB / 16.4MB            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Model & metadata uploaded: https://huggingface.co/alamb98/efficientnet-b0_norwood_classifier\n"
          ]
        }
      ],
      "source": [
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "# Upload trained EfficientNet Norwood classifier to Hugging Face\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "!pip install --quiet huggingface_hub\n",
        "\n",
        "from huggingface_hub import login, create_repo, upload_folder\n",
        "import json\n",
        "\n",
        "# (A) Login with your Hugging Face token (only once per session)\n",
        "login()  # Paste your token when prompted\n",
        "\n",
        "# (B) Set repo name (change if needed)\n",
        "hf_username = \"alamb98\"  # 🔁 Your HF username\n",
        "repo_id = f\"{hf_username}/{MODEL_NAME}_norwood_classifier\"\n",
        "\n",
        "# (C) Create or reuse existing repo\n",
        "create_repo(repo_id, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "# (D) Save model checkpoint & metadata\n",
        "model_path = f\"{MODEL_NAME}_best.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "metadata = {\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"expand_ratio\": EXPAND_RATIO,\n",
        "    \"num_classes\": len(LEVEL_NAMES),\n",
        "    \"class_names\": LEVEL_NAMES,\n",
        "    \"test_accuracy\": round(float(test_acc), 4),\n",
        "    \"macro_f1_score\": round(float(test_f1_macro), 4),\n",
        "    \"architecture\": \"PyTorch\",\n",
        "    \"framework\": \"torchvision.models\",\n",
        "    \"description\": (\n",
        "        \"EfficientNet-based Norwood scale classifier trained on cropped male scalp \"\n",
        "        \"images from multiple angles (front, back, left, right, top-down). \"\n",
        "        \"Crops are obtained using segmentation boundaries and expanded by \"\n",
        "        f\"{EXPAND_RATIO*100:.1f}%.\"\n",
        "    )\n",
        "}\n",
        "with open(\"metadata.json\", \"w\") as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "# (E) Upload both model and metadata.json\n",
        "upload_folder(\n",
        "    folder_path=\".\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\",\n",
        "    allow_patterns=[model_path, \"metadata.json\"]\n",
        ")\n",
        "\n",
        "print(f\"\\n✅ Model & metadata uploaded: https://huggingface.co/{repo_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axdl53uxn4eT"
      },
      "source": [
        "mobilenet_v3_large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IgR1Zg5hnxuO",
        "outputId": "9eaf623a-e55f-4e78-e6fe-8abafb5c3823"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verified combined raw + combined segmented folder structure.\n",
            "\n",
            "▶ Running with expand_ratio = 0.01\n",
            "Train samples: 9069\n",
            "Valid samples: 895\n",
            "Test samples:  416\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Epoch 1/10 [Train]: 100%|██████████| 284/284 [01:04<00:00,  4.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 1] Train Acc: 0.6567\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  4.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 1] Valid Acc: 0.7441\n",
            "      → New best valid acc: 0.7441. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 284/284 [01:02<00:00,  4.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 2] Train Acc: 0.8475\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  4.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 2] Valid Acc: 0.7933\n",
            "      → New best valid acc: 0.7933. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 284/284 [01:03<00:00,  4.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 3] Train Acc: 0.9179\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  4.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 3] Valid Acc: 0.8201\n",
            "      → New best valid acc: 0.8201. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 284/284 [01:02<00:00,  4.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 4] Train Acc: 0.9487\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  4.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 4] Valid Acc: 0.8145\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 [Train]: 100%|██████████| 284/284 [01:02<00:00,  4.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 5] Train Acc: 0.9682\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  4.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 5] Valid Acc: 0.8346\n",
            "      → New best valid acc: 0.8346. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 [Train]: 100%|██████████| 284/284 [01:02<00:00,  4.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 6] Train Acc: 0.9765\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  5.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 6] Valid Acc: 0.8268\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 [Train]: 100%|██████████| 284/284 [01:02<00:00,  4.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 7] Train Acc: 0.9792\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  5.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 7] Valid Acc: 0.8391\n",
            "      → New best valid acc: 0.8391. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 [Train]: 100%|██████████| 284/284 [01:02<00:00,  4.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 8] Train Acc: 0.9864\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  4.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 8] Valid Acc: 0.8391\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 [Train]: 100%|██████████| 284/284 [01:03<00:00,  4.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 9] Train Acc: 0.9831\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  4.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 9] Valid Acc: 0.8291\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 [Train]: 100%|██████████| 284/284 [01:03<00:00,  4.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 10] Train Acc: 0.9836\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  4.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 10] Valid Acc: 0.8402\n",
            "      → New best valid acc: 0.8402. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 13/13 [00:02<00:00,  4.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "▶ Final Test Accuracy: 85.10%  (416 samples)\n",
            "▶ Final Test Macro F1: 0.7384\n",
            "\n",
            "Confusion Matrix:\n",
            "         Level 2  Level 3  Level 4  Level 5  Level 6  Level 7\n",
            "Level 2      189       17        3        1        0        0\n",
            "Level 3        5       83        4        2        1        0\n",
            "Level 4        2        7       40        1        1        0\n",
            "Level 5        1        2        4        3        1        2\n",
            "Level 6        0        1        2        0       14        3\n",
            "Level 7        0        1        1        0        0       25\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Level 2     0.9594    0.9000    0.9287       210\n",
            "     Level 3     0.7477    0.8737    0.8058        95\n",
            "     Level 4     0.7407    0.7843    0.7619        51\n",
            "     Level 5     0.4286    0.2308    0.3000        13\n",
            "     Level 6     0.8235    0.7000    0.7568        20\n",
            "     Level 7     0.8333    0.9259    0.8772        27\n",
            "\n",
            "    accuracy                         0.8510       416\n",
            "   macro avg     0.7556    0.7358    0.7384       416\n",
            "weighted avg     0.8530    0.8510    0.8490       416\n",
            "\n"
          ]
        }
      ],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build your classifier & move to GPU\n",
        "################################################################################\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "# You can now set MODEL_NAME to one of:\n",
        "# \"efficientnet-b0\", \"efficientnet-b2\",\n",
        "# \"mobilenet_v2\", \"mobilenet_v3_large\",\n",
        "# \"resnet18\", \"resnet34\", \"resnet50\",\n",
        "# \"resnext50_32x4d\",\n",
        "# \"vgg16\",\n",
        "# \"densenet121\",\n",
        "# \"shufflenet_v2_x1_0\",\n",
        "# \"convnext_tiny\",\n",
        "# \"vit_b_16\",\n",
        "# \"swin_tiny_patch4_window7_224\",\n",
        "# or \"custom_cnn\"\n",
        "\n",
        "MODEL_NAME = \"mobilenet_v3_large\"   # ← change as desired\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if MODEL_NAME == \"efficientnet-b0\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"efficientnet-b2\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v2\":\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v3_large\":\n",
        "    model = models.mobilenet_v3_large(pretrained=True)\n",
        "    in_features = model.classifier[3].in_features\n",
        "    model.classifier[3] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet18\":\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet34\":\n",
        "    model = models.resnet34(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet50\":\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnext50_32x4d\":\n",
        "    model = models.resnext50_32x4d(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vgg16\":\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    in_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"densenet121\":\n",
        "    model = models.densenet121(pretrained=True)\n",
        "    in_features = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"shufflenet_v2_x1_0\":\n",
        "    model = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"convnext_tiny\":\n",
        "    model = models.convnext_tiny(pretrained=True)\n",
        "    in_features = model.classifier[2].in_features\n",
        "    model.classifier[2] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vit_b_16\":\n",
        "    model = models.vit_b_16(pretrained=True)\n",
        "    in_features = model.heads.head.in_features\n",
        "    model.heads.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"swin_tiny_patch4_window7_224\":\n",
        "    model = models.swin_t(pretrained=True)\n",
        "    in_features = model.head.in_features\n",
        "    model.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"custom_cnn\":\n",
        "    # Make sure you have defined CustomCNN somewhere above:\n",
        "    # class CustomCNN(nn.Module):\n",
        "    #     def __init__(self, num_classes): …\n",
        "    #     def forward(self, x): …\n",
        "    model = CustomCNN(num_classes=len(LEVEL_NAMES))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown MODEL_NAME: {MODEL_NAME}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169,
          "referenced_widgets": [
            "034ad1f086334200bc793ccd27796796",
            "9bcd5353852f43a4a70299227d42eedf",
            "c2d09d44b3b44ed6ad39f748ff7d94c5",
            "f575e212d1a44d15ab77ec08d45ad4bd",
            "cb2f9c815bfc474a8504477f12dc1d9b",
            "3e373320964a4d1da85380fdb28a019c",
            "0086f32407ae4af48e79aff5e5db3804",
            "bd07bae1f1664153b6cda5929fc36c9b",
            "2d7d896f82604054b0037d4bb502f691",
            "17c7c858039245a4ba1f3c2f866b48be",
            "f6933959d0544a83ae35994cf299a169",
            "8ad7dbeeeb8943f59c012ff5e46f784b",
            "dfb21724818b46a0826c1ea3f6a9217a",
            "ec58321961e34a0a8aa121b520135aef",
            "4bf4c53dba72468ba9cbca573c30f6ba",
            "e65e29ba548940a6a8d2b8cddae5acaa",
            "1f0fafd3fa9b4cea84823716835c679e",
            "2c89be3bc8214895bb54c300c2e91d2b",
            "65f5ec376a164b4c982c3879801bcf88",
            "6c6f73850cca44fb81b14ef0f220ddf9",
            "cb871555a9694bda925a8c34714c9545",
            "2c5d24e4b9ab47078ae7a0d7ef55ab5b",
            "50e998199cc244d593e097d5389d8d05",
            "2a64148a14404abc973b21287c28815a",
            "26be810c2f8b47b39263c862b44d58f4",
            "cf6f78d6e58e48059e75e2df19399c1a",
            "c912ad05683349dc84d2c7d457ae967b",
            "c19998596f5045c7b893d3f749d6883d",
            "0de8c3f756f041eb9ec016083d8addfe",
            "c99b3f0c4fe74268912b7ae250db62cd",
            "4aa69667a56945d99a0d63daa9ace205",
            "52fc0e54eabd4d5e9438ea1af691e502",
            "38bd19899cbc4c408112a9d1e2dde545",
            "c2b6df7428714a4d9c4ddc2d1a4061ef",
            "16bfe4754e82469b9e0ec1b6d7cf490d"
          ]
        },
        "id": "RKE7EORnhXAC",
        "outputId": "a64b402d-2cf3-4fb2-bf92-fe8c7023cabf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "034ad1f086334200bc793ccd27796796",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2d09d44b3b44ed6ad39f748ff7d94c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec58321961e34a0a8aa121b520135aef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload                         : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26be810c2f8b47b39263c862b44d58f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  ...net_v3_large_cropped_clahe_best.pth:   3%|3         |  554kB / 17.1MB            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Uploaded to: https://huggingface.co/alamb98/mobilenet_v3_large_cropped_clahe_norwood_classifier\n"
          ]
        }
      ],
      "source": [
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "# Upload trained model checkpoint and metadata to Hugging Face\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "!pip install --quiet huggingface_hub\n",
        "\n",
        "from huggingface_hub import login, create_repo, upload_folder\n",
        "import json\n",
        "\n",
        "# (A) Login with your token (only once per session)\n",
        "login()  # Paste your Hugging Face token when prompted\n",
        "\n",
        "# (B) Set repo name (adjust if needed)\n",
        "hf_username = \"alamb98\"  # 🔁 Your HF username\n",
        "repo_id = f\"{hf_username}/{MODEL_NAME}_cropped_clahe_norwood_classifier\"\n",
        "\n",
        "# (C) Create or use existing repo\n",
        "create_repo(repo_id, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "# (D) Save model checkpoint and metadata\n",
        "model_path = f\"{MODEL_NAME}_cropped_clahe_best.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "metadata = {\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"expand_ratio\": float(EXPAND_RATIO),\n",
        "    \"num_classes\": len(LEVEL_NAMES),\n",
        "    \"class_names\": LEVEL_NAMES,\n",
        "    \"test_accuracy\": round(float(test_acc), 4),\n",
        "    \"macro_f1_score\": round(float(test_f1_macro), 4),\n",
        "    \"architecture\": MODEL_NAME,\n",
        "    \"framework\": \"PyTorch\",\n",
        "    \"trained_on\": \"cropped scalp images from combined orig + CLAHE raw & segmented datasets\",\n",
        "    \"description\": (\n",
        "        f\"{MODEL_NAME} classifier for Norwood stages (2–7) using cropped hair \"\n",
        "        \"region based on segmented boundaries and raw images.\"\n",
        "    )\n",
        "}\n",
        "with open(\"metadata.json\", \"w\") as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "# (E) Upload both model and metadata.json\n",
        "upload_folder(\n",
        "    folder_path=\".\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\",\n",
        "    allow_patterns=[model_path, \"metadata.json\"]\n",
        ")\n",
        "\n",
        "print(f\"\\n✅ Uploaded to: https://huggingface.co/{repo_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0chw-Uh0n-29"
      },
      "source": [
        "resnet34"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtTnQQXhn6Io"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build your classifier & move to GPU\n",
        "################################################################################\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "# You can now set MODEL_NAME to one of:\n",
        "# \"efficientnet-b0\", \"efficientnet-b2\",\n",
        "# \"mobilenet_v2\", \"mobilenet_v3_large\",\n",
        "# \"resnet18\", \"resnet34\", \"resnet50\",\n",
        "# \"resnext50_32x4d\",\n",
        "# \"vgg16\",\n",
        "# \"densenet121\",\n",
        "# \"shufflenet_v2_x1_0\",\n",
        "# \"convnext_tiny\",\n",
        "# \"vit_b_16\",\n",
        "# \"swin_tiny_patch4_window7_224\",\n",
        "# or \"custom_cnn\"\n",
        "\n",
        "MODEL_NAME = \"resnet34\"   # ← change as desired\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if MODEL_NAME == \"efficientnet-b0\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"efficientnet-b2\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v2\":\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v3_large\":\n",
        "    model = models.mobilenet_v3_large(pretrained=True)\n",
        "    in_features = model.classifier[3].in_features\n",
        "    model.classifier[3] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet18\":\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet34\":\n",
        "    model = models.resnet34(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet50\":\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnext50_32x4d\":\n",
        "    model = models.resnext50_32x4d(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vgg16\":\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    in_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"densenet121\":\n",
        "    model = models.densenet121(pretrained=True)\n",
        "    in_features = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"shufflenet_v2_x1_0\":\n",
        "    model = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"convnext_tiny\":\n",
        "    model = models.convnext_tiny(pretrained=True)\n",
        "    in_features = model.classifier[2].in_features\n",
        "    model.classifier[2] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vit_b_16\":\n",
        "    model = models.vit_b_16(pretrained=True)\n",
        "    in_features = model.heads.head.in_features\n",
        "    model.heads.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"swin_tiny_patch4_window7_224\":\n",
        "    model = models.swin_t(pretrained=True)\n",
        "    in_features = model.head.in_features\n",
        "    model.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"custom_cnn\":\n",
        "    # Make sure you have defined CustomCNN somewhere above:\n",
        "    # class CustomCNN(nn.Module):\n",
        "    #     def __init__(self, num_classes): …\n",
        "    #     def forward(self, x): …\n",
        "    model = CustomCNN(num_classes=len(LEVEL_NAMES))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown MODEL_NAME: {MODEL_NAME}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GwQfeRioJkx"
      },
      "source": [
        "resnet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kH7V_b_CoFyt"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build your classifier & move to GPU\n",
        "################################################################################\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "# You can now set MODEL_NAME to one of:\n",
        "# \"efficientnet-b0\", \"efficientnet-b2\",\n",
        "# \"mobilenet_v2\", \"mobilenet_v3_large\",\n",
        "# \"resnet18\", \"resnet34\", \"resnet50\",\n",
        "# \"resnext50_32x4d\",\n",
        "# \"vgg16\",\n",
        "# \"densenet121\",\n",
        "# \"shufflenet_v2_x1_0\",\n",
        "# \"convnext_tiny\",\n",
        "# \"vit_b_16\",\n",
        "# \"swin_tiny_patch4_window7_224\",\n",
        "# or \"custom_cnn\"\n",
        "\n",
        "MODEL_NAME = \"resnet50\"   # ← change as desired\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if MODEL_NAME == \"efficientnet-b0\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"efficientnet-b2\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v2\":\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v3_large\":\n",
        "    model = models.mobilenet_v3_large(pretrained=True)\n",
        "    in_features = model.classifier[3].in_features\n",
        "    model.classifier[3] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet18\":\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet34\":\n",
        "    model = models.resnet34(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet50\":\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnext50_32x4d\":\n",
        "    model = models.resnext50_32x4d(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vgg16\":\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    in_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"densenet121\":\n",
        "    model = models.densenet121(pretrained=True)\n",
        "    in_features = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"shufflenet_v2_x1_0\":\n",
        "    model = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"convnext_tiny\":\n",
        "    model = models.convnext_tiny(pretrained=True)\n",
        "    in_features = model.classifier[2].in_features\n",
        "    model.classifier[2] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vit_b_16\":\n",
        "    model = models.vit_b_16(pretrained=True)\n",
        "    in_features = model.heads.head.in_features\n",
        "    model.heads.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"swin_tiny_patch4_window7_224\":\n",
        "    model = models.swin_t(pretrained=True)\n",
        "    in_features = model.head.in_features\n",
        "    model.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"custom_cnn\":\n",
        "    # Make sure you have defined CustomCNN somewhere above:\n",
        "    # class CustomCNN(nn.Module):\n",
        "    #     def __init__(self, num_classes): …\n",
        "    #     def forward(self, x): …\n",
        "    model = CustomCNN(num_classes=len(LEVEL_NAMES))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown MODEL_NAME: {MODEL_NAME}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vNnZtoQoRS1"
      },
      "source": [
        "resnext50_32x4d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPKjGVd6oNO_"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build your classifier & move to GPU\n",
        "################################################################################\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "# You can now set MODEL_NAME to one of:\n",
        "# \"efficientnet-b0\", \"efficientnet-b2\",\n",
        "# \"mobilenet_v2\", \"mobilenet_v3_large\",\n",
        "# \"resnet18\", \"resnet34\", \"resnet50\",\n",
        "# \"resnext50_32x4d\",\n",
        "# \"vgg16\",\n",
        "# \"densenet121\",\n",
        "# \"shufflenet_v2_x1_0\",\n",
        "# \"convnext_tiny\",\n",
        "# \"vit_b_16\",\n",
        "# \"swin_tiny_patch4_window7_224\",\n",
        "# or \"custom_cnn\"\n",
        "\n",
        "MODEL_NAME = \"resnext50_32x4d\"   # ← change as desired\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if MODEL_NAME == \"efficientnet-b0\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"efficientnet-b2\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v2\":\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v3_large\":\n",
        "    model = models.mobilenet_v3_large(pretrained=True)\n",
        "    in_features = model.classifier[3].in_features\n",
        "    model.classifier[3] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet18\":\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet34\":\n",
        "    model = models.resnet34(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet50\":\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnext50_32x4d\":\n",
        "    model = models.resnext50_32x4d(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vgg16\":\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    in_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"densenet121\":\n",
        "    model = models.densenet121(pretrained=True)\n",
        "    in_features = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"shufflenet_v2_x1_0\":\n",
        "    model = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"convnext_tiny\":\n",
        "    model = models.convnext_tiny(pretrained=True)\n",
        "    in_features = model.classifier[2].in_features\n",
        "    model.classifier[2] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vit_b_16\":\n",
        "    model = models.vit_b_16(pretrained=True)\n",
        "    in_features = model.heads.head.in_features\n",
        "    model.heads.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"swin_tiny_patch4_window7_224\":\n",
        "    model = models.swin_t(pretrained=True)\n",
        "    in_features = model.head.in_features\n",
        "    model.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"custom_cnn\":\n",
        "    # Make sure you have defined CustomCNN somewhere above:\n",
        "    # class CustomCNN(nn.Module):\n",
        "    #     def __init__(self, num_classes): …\n",
        "    #     def forward(self, x): …\n",
        "    model = CustomCNN(num_classes=len(LEVEL_NAMES))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown MODEL_NAME: {MODEL_NAME}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guxKl3YJocFY"
      },
      "source": [
        "densenet121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjQG5Reoodjq"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build your classifier & move to GPU\n",
        "################################################################################\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "# You can now set MODEL_NAME to one of:\n",
        "# \"efficientnet-b0\", \"efficientnet-b2\",\n",
        "# \"mobilenet_v2\", \"mobilenet_v3_large\",\n",
        "# \"resnet18\", \"resnet34\", \"resnet50\",\n",
        "# \"resnext50_32x4d\",\n",
        "# \"vgg16\",\n",
        "# \"densenet121\",\n",
        "# \"shufflenet_v2_x1_0\",\n",
        "# \"convnext_tiny\",\n",
        "# \"vit_b_16\",\n",
        "# \"swin_tiny_patch4_window7_224\",\n",
        "# or \"custom_cnn\"\n",
        "\n",
        "MODEL_NAME = \"densenet121\"   # ← change as desired\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if MODEL_NAME == \"efficientnet-b0\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"efficientnet-b2\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v2\":\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v3_large\":\n",
        "    model = models.mobilenet_v3_large(pretrained=True)\n",
        "    in_features = model.classifier[3].in_features\n",
        "    model.classifier[3] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet18\":\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet34\":\n",
        "    model = models.resnet34(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet50\":\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnext50_32x4d\":\n",
        "    model = models.resnext50_32x4d(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vgg16\":\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    in_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"densenet121\":\n",
        "    model = models.densenet121(pretrained=True)\n",
        "    in_features = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"shufflenet_v2_x1_0\":\n",
        "    model = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"convnext_tiny\":\n",
        "    model = models.convnext_tiny(pretrained=True)\n",
        "    in_features = model.classifier[2].in_features\n",
        "    model.classifier[2] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vit_b_16\":\n",
        "    model = models.vit_b_16(pretrained=True)\n",
        "    in_features = model.heads.head.in_features\n",
        "    model.heads.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"swin_tiny_patch4_window7_224\":\n",
        "    model = models.swin_t(pretrained=True)\n",
        "    in_features = model.head.in_features\n",
        "    model.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"custom_cnn\":\n",
        "    # Make sure you have defined CustomCNN somewhere above:\n",
        "    # class CustomCNN(nn.Module):\n",
        "    #     def __init__(self, num_classes): …\n",
        "    #     def forward(self, x): …\n",
        "    model = CustomCNN(num_classes=len(LEVEL_NAMES))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown MODEL_NAME: {MODEL_NAME}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsjWKJJnol38"
      },
      "source": [
        "shufflenet_v2_x1_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H39cTcVromtV"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build your classifier & move to GPU\n",
        "################################################################################\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "# You can now set MODEL_NAME to one of:\n",
        "# \"efficientnet-b0\", \"efficientnet-b2\",\n",
        "# \"mobilenet_v2\", \"mobilenet_v3_large\",\n",
        "# \"resnet18\", \"resnet34\", \"resnet50\",\n",
        "# \"resnext50_32x4d\",\n",
        "# \"vgg16\",\n",
        "# \"densenet121\",\n",
        "# \"shufflenet_v2_x1_0\",\n",
        "# \"convnext_tiny\",\n",
        "# \"vit_b_16\",\n",
        "# \"swin_tiny_patch4_window7_224\",\n",
        "# or \"custom_cnn\"\n",
        "\n",
        "MODEL_NAME = \"shufflenet_v2_x1_0\"   # ← change as desired\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if MODEL_NAME == \"efficientnet-b0\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"efficientnet-b2\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v2\":\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v3_large\":\n",
        "    model = models.mobilenet_v3_large(pretrained=True)\n",
        "    in_features = model.classifier[3].in_features\n",
        "    model.classifier[3] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet18\":\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet34\":\n",
        "    model = models.resnet34(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet50\":\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnext50_32x4d\":\n",
        "    model = models.resnext50_32x4d(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vgg16\":\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    in_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"densenet121\":\n",
        "    model = models.densenet121(pretrained=True)\n",
        "    in_features = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"shufflenet_v2_x1_0\":\n",
        "    model = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"convnext_tiny\":\n",
        "    model = models.convnext_tiny(pretrained=True)\n",
        "    in_features = model.classifier[2].in_features\n",
        "    model.classifier[2] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vit_b_16\":\n",
        "    model = models.vit_b_16(pretrained=True)\n",
        "    in_features = model.heads.head.in_features\n",
        "    model.heads.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"swin_tiny_patch4_window7_224\":\n",
        "    model = models.swin_t(pretrained=True)\n",
        "    in_features = model.head.in_features\n",
        "    model.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"custom_cnn\":\n",
        "    # Make sure you have defined CustomCNN somewhere above:\n",
        "    # class CustomCNN(nn.Module):\n",
        "    #     def __init__(self, num_classes): …\n",
        "    #     def forward(self, x): …\n",
        "    model = CustomCNN(num_classes=len(LEVEL_NAMES))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown MODEL_NAME: {MODEL_NAME}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7J4v3WMo872"
      },
      "source": [
        "convnext_tiny"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0R7rlX8ho9rY",
        "outputId": "22bdf76c-883c-4af8-9c1f-330d2e5a5c58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verified combined raw + combined segmented folder structure.\n",
            "\n",
            "▶ Running with expand_ratio = 0.01\n",
            "Train samples: 9069\n",
            "Valid samples: 895\n",
            "Test samples:  416\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ConvNeXt_Tiny_Weights.IMAGENET1K_V1`. You can also use `weights=ConvNeXt_Tiny_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/convnext_tiny-983f1562.pth\" to /root/.cache/torch/hub/checkpoints/convnext_tiny-983f1562.pth\n",
            "100%|██████████| 109M/109M [00:00<00:00, 233MB/s] \n",
            "Epoch 1/10 [Train]: 100%|██████████| 284/284 [03:43<00:00,  1.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 1] Train Acc: 0.7240\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  5.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 1] Valid Acc: 0.7944\n",
            "      → New best valid acc: 0.7944. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Train]: 100%|██████████| 284/284 [03:43<00:00,  1.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 2] Train Acc: 0.9145\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  5.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 2] Valid Acc: 0.8402\n",
            "      → New best valid acc: 0.8402. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Train]: 100%|██████████| 284/284 [03:43<00:00,  1.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 3] Train Acc: 0.9663\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  5.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 3] Valid Acc: 0.8425\n",
            "      → New best valid acc: 0.8425. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Train]: 100%|██████████| 284/284 [03:44<00:00,  1.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 4] Train Acc: 0.9789\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  5.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 4] Valid Acc: 0.8547\n",
            "      → New best valid acc: 0.8547. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 [Train]: 100%|██████████| 284/284 [03:43<00:00,  1.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 5] Train Acc: 0.9865\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  5.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 5] Valid Acc: 0.8592\n",
            "      → New best valid acc: 0.8592. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 [Train]: 100%|██████████| 284/284 [03:43<00:00,  1.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 6] Train Acc: 0.9816\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  4.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 6] Valid Acc: 0.8536\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 [Train]: 100%|██████████| 284/284 [03:42<00:00,  1.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 7] Train Acc: 0.9881\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  5.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 7] Valid Acc: 0.8570\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 [Train]: 100%|██████████| 284/284 [03:43<00:00,  1.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 8] Train Acc: 0.9819\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  5.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 8] Valid Acc: 0.8480\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 [Train]: 100%|██████████| 284/284 [03:42<00:00,  1.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 9] Train Acc: 0.9861\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  5.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 9] Valid Acc: 0.8480\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 [Train]: 100%|██████████| 284/284 [03:43<00:00,  1.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 10] Train Acc: 0.9854\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 [Valid]: 100%|██████████| 28/28 [00:05<00:00,  5.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [Epoch 10] Valid Acc: 0.8603\n",
            "      → New best valid acc: 0.8603. Saved to /content/best_efficientnetb0_single_ratio.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 13/13 [00:02<00:00,  4.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "▶ Final Test Accuracy: 85.82%  (416 samples)\n",
            "▶ Final Test Macro F1: 0.7833\n",
            "\n",
            "Confusion Matrix:\n",
            "         Level 2  Level 3  Level 4  Level 5  Level 6  Level 7\n",
            "Level 2      191       16        3        0        0        0\n",
            "Level 3        9       82        4        0        0        0\n",
            "Level 4        2        5       41        3        0        0\n",
            "Level 5        0        2        2        7        2        0\n",
            "Level 6        1        0        1        1       16        1\n",
            "Level 7        0        0        3        1        3       20\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Level 2     0.9409    0.9095    0.9249       210\n",
            "     Level 3     0.7810    0.8632    0.8200        95\n",
            "     Level 4     0.7593    0.8039    0.7810        51\n",
            "     Level 5     0.5833    0.5385    0.5600        13\n",
            "     Level 6     0.7619    0.8000    0.7805        20\n",
            "     Level 7     0.9524    0.7407    0.8333        27\n",
            "\n",
            "    accuracy                         0.8582       416\n",
            "   macro avg     0.7965    0.7760    0.7833       416\n",
            "weighted avg     0.8631    0.8582    0.8590       416\n",
            "\n"
          ]
        }
      ],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build your classifier & move to GPU\n",
        "################################################################################\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "# You can now set MODEL_NAME to one of:\n",
        "# \"efficientnet-b0\", \"efficientnet-b2\",\n",
        "# \"mobilenet_v2\", \"mobilenet_v3_large\",\n",
        "# \"resnet18\", \"resnet34\", \"resnet50\",\n",
        "# \"resnext50_32x4d\",\n",
        "# \"vgg16\",\n",
        "# \"densenet121\",\n",
        "# \"shufflenet_v2_x1_0\",\n",
        "# \"convnext_tiny\",\n",
        "# \"vit_b_16\",\n",
        "# \"swin_tiny_patch4_window7_224\",\n",
        "# or \"custom_cnn\"\n",
        "\n",
        "MODEL_NAME = \"convnext_tiny\"   # ← change as desired\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if MODEL_NAME == \"efficientnet-b0\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"efficientnet-b2\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v2\":\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v3_large\":\n",
        "    model = models.mobilenet_v3_large(pretrained=True)\n",
        "    in_features = model.classifier[3].in_features\n",
        "    model.classifier[3] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet18\":\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet34\":\n",
        "    model = models.resnet34(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet50\":\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnext50_32x4d\":\n",
        "    model = models.resnext50_32x4d(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vgg16\":\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    in_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"densenet121\":\n",
        "    model = models.densenet121(pretrained=True)\n",
        "    in_features = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"shufflenet_v2_x1_0\":\n",
        "    model = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"convnext_tiny\":\n",
        "    model = models.convnext_tiny(pretrained=True)\n",
        "    in_features = model.classifier[2].in_features\n",
        "    model.classifier[2] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vit_b_16\":\n",
        "    model = models.vit_b_16(pretrained=True)\n",
        "    in_features = model.heads.head.in_features\n",
        "    model.heads.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"swin_tiny_patch4_window7_224\":\n",
        "    model = models.swin_t(pretrained=True)\n",
        "    in_features = model.head.in_features\n",
        "    model.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"custom_cnn\":\n",
        "    # Make sure you have defined CustomCNN somewhere above:\n",
        "    # class CustomCNN(nn.Module):\n",
        "    #     def __init__(self, num_classes): …\n",
        "    #     def forward(self, x): …\n",
        "    model = CustomCNN(num_classes=len(LEVEL_NAMES))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown MODEL_NAME: {MODEL_NAME}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n",
        "\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "# Upload trained model checkpoint and metadata to Hugging Face\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "!pip install --quiet huggingface_hub\n",
        "\n",
        "from huggingface_hub import login, create_repo, upload_folder\n",
        "import json\n",
        "import torch\n",
        "\n",
        "# (A) Login with your token (only once per session)\n",
        "login()  # Paste your Hugging Face token when prompted\n",
        "\n",
        "# (B) Set repo name (adjust if needed)\n",
        "hf_username = \"alamb98\"  # 🔁 Your HF username\n",
        "repo_id = f\"{hf_username}/{MODEL_NAME}_clahe_norwood_classifier\"\n",
        "\n",
        "# (C) Create or use existing repo\n",
        "create_repo(repo_id, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "# (D) Save model checkpoint and metadata\n",
        "model_path = f\"{MODEL_NAME}_clahe_best.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "metadata = {\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"expand_ratio\": EXPAND_RATIO,\n",
        "    \"num_classes\": len(LEVEL_NAMES),\n",
        "    \"class_names\": LEVEL_NAMES,\n",
        "    \"test_accuracy\": round(float(test_acc), 4),\n",
        "    \"macro_f1_score\": round(float(test_f1_macro), 4),\n",
        "    \"architecture\": \"torchvision.models\",\n",
        "    \"framework\": \"PyTorch\",\n",
        "    \"trained_on\": \"combined raw + CLAHE + segmented scalp images\",\n",
        "    \"description\": \"ConvNeXt-Tiny classifier for male pattern baldness (Norwood stages 2–7)\"\n",
        "}\n",
        "with open(\"metadata.json\", \"w\") as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "# (E) Upload both model and metadata.json\n",
        "upload_folder(\n",
        "    folder_path=\".\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\",\n",
        "    allow_patterns=[model_path, \"metadata.json\"]\n",
        ")\n",
        "\n",
        "print(f\"\\n✅ Uploaded to: https://huggingface.co/{repo_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169,
          "referenced_widgets": [
            "29a9040263584f5fb32c3a41e61d2df8",
            "ac0d6439339e45ed9a904c4a996a0092",
            "002f10dd430748748e26d1ac4d2e0dc5",
            "b11418d389f14f61835ca88981efe14e",
            "41648911f2354641bd50058ea6293a01",
            "d4a19ec480d640da85e373529d9dbfce",
            "dcda11f9da084871a50a60ff96c390a8",
            "1d8637001d234cf3ac601f6c1222f479",
            "a63a93a2d1bc4253a5766c56a54c9c27",
            "c01841a3885543e199a96f72d2d9b3a3",
            "fcb6f5628847461b8fe20e3b8715b830",
            "269c55c3ce224bceb370ebfa1ddc4b10",
            "5a61a1bad9fb4bb2804b6d4615c8fcc5",
            "5b596c1de94243e498c5a92125afd9ff",
            "35acc52314504223b7e352b9e7da2bf9",
            "d59d32b6f6d04ecc9332073f376dada3",
            "3106f90866484f9bb11b102c920815b8",
            "ef9109f280d044f09a98bf6cc3d99413",
            "2bed2aff430b4afbaeb0644ee37cbfe2",
            "3add284622fa4af4bb4f34becf96a579",
            "2b01f693bc0643979791c115a3c23ee8",
            "2719564fdb754508aa2c89e0b4551543",
            "2d7440d3548843b88ca4045760af103a",
            "7f4e3d51b7df40e898a81eddef78ac67",
            "8fc0bb64d856494782f64779250a89af",
            "bbb3e6d4c06d4e96b6c854937e679791",
            "bcc8ac5e0d774af2afe06ce7b61c5b9a",
            "bbabf8d873374ee48fb8af159c95885b",
            "aab8f90650fe4b7dab655a7bfcc0f604",
            "ddf570c99bdd4e34a8694003e888246f",
            "99c0d062ad3e4dca902dbbad07566614",
            "01ee4c7e72de4974b01f1147840b1826",
            "b9974637402149d1879b39da44de3b58",
            "f533200ed0734d47b41778921b5750ba",
            "e8a742327d88496db8b7da507b911249"
          ]
        },
        "id": "_bqZeeWeLUM2",
        "outputId": "abacd709-a686-4e56-dfbc-87fd013c6dcd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29a9040263584f5fb32c3a41e61d2df8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "002f10dd430748748e26d1ac4d2e0dc5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b596c1de94243e498c5a92125afd9ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "New Data Upload                         : |          |  0.00B /  0.00B            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8fc0bb64d856494782f64779250a89af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  ...tation/convnext_tiny_clahe_best.pth:   0%|          |  556kB /  111MB            "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Uploaded to: https://huggingface.co/alamb98/convnext_tiny_clahe_norwood_classifier\n"
          ]
        }
      ],
      "source": [
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "# Upload trained model checkpoint and metadata to Hugging Face\n",
        "# ───────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "!pip install --quiet huggingface_hub\n",
        "\n",
        "from huggingface_hub import login, create_repo, upload_folder\n",
        "import json\n",
        "import torch\n",
        "\n",
        "# (A) Login with your token (only once per session)\n",
        "login()  # Paste your Hugging Face token when prompted\n",
        "\n",
        "# (B) Set repo name (adjust if needed)\n",
        "hf_username = \"alamb98\"  # 🔁 Your HF username\n",
        "repo_id = f\"{hf_username}/{MODEL_NAME}_clahe_norwood_classifier\"\n",
        "\n",
        "# (C) Create or use existing repo\n",
        "create_repo(repo_id, repo_type=\"model\", exist_ok=True)\n",
        "\n",
        "# (D) Save model checkpoint and metadata\n",
        "model_path = f\"{MODEL_NAME}_clahe_best.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "metadata = {\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"expand_ratio\": EXPAND_RATIO,\n",
        "    \"num_classes\": len(LEVEL_NAMES),\n",
        "    \"class_names\": LEVEL_NAMES,\n",
        "    \"test_accuracy\": round(float(test_acc), 4),\n",
        "    \"macro_f1_score\": round(float(test_f1_macro), 4),\n",
        "    \"architecture\": \"torchvision.models\",\n",
        "    \"framework\": \"PyTorch\",\n",
        "    \"trained_on\": \"combined raw + CLAHE + segmented scalp images\",\n",
        "    \"description\": \"ConvNeXt-Tiny classifier for male pattern baldness (Norwood stages 2–7)\"\n",
        "}\n",
        "with open(\"metadata.json\", \"w\") as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "# (E) Upload both model and metadata.json\n",
        "upload_folder(\n",
        "    folder_path=\".\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\",\n",
        "    allow_patterns=[model_path, \"metadata.json\"]\n",
        ")\n",
        "\n",
        "print(f\"\\n✅ Uploaded to: https://huggingface.co/{repo_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlpfIBRepD22"
      },
      "source": [
        "vit_b_16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tv08XV-9pEqx"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build your classifier & move to GPU\n",
        "################################################################################\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "# You can now set MODEL_NAME to one of:\n",
        "# \"efficientnet-b0\", \"efficientnet-b2\",\n",
        "# \"mobilenet_v2\", \"mobilenet_v3_large\",\n",
        "# \"resnet18\", \"resnet34\", \"resnet50\",\n",
        "# \"resnext50_32x4d\",\n",
        "# \"vgg16\",\n",
        "# \"densenet121\",\n",
        "# \"shufflenet_v2_x1_0\",\n",
        "# \"convnext_tiny\",\n",
        "# \"vit_b_16\",\n",
        "# \"swin_tiny_patch4_window7_224\",\n",
        "# or \"custom_cnn\"\n",
        "\n",
        "MODEL_NAME = \"vit_b_16\"   # ← change as desired\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if MODEL_NAME == \"efficientnet-b0\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"efficientnet-b2\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v2\":\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v3_large\":\n",
        "    model = models.mobilenet_v3_large(pretrained=True)\n",
        "    in_features = model.classifier[3].in_features\n",
        "    model.classifier[3] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet18\":\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet34\":\n",
        "    model = models.resnet34(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet50\":\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnext50_32x4d\":\n",
        "    model = models.resnext50_32x4d(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vgg16\":\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    in_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"densenet121\":\n",
        "    model = models.densenet121(pretrained=True)\n",
        "    in_features = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"shufflenet_v2_x1_0\":\n",
        "    model = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"convnext_tiny\":\n",
        "    model = models.convnext_tiny(pretrained=True)\n",
        "    in_features = model.classifier[2].in_features\n",
        "    model.classifier[2] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vit_b_16\":\n",
        "    model = models.vit_b_16(pretrained=True)\n",
        "    in_features = model.heads.head.in_features\n",
        "    model.heads.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"swin_tiny_patch4_window7_224\":\n",
        "    model = models.swin_t(pretrained=True)\n",
        "    in_features = model.head.in_features\n",
        "    model.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"custom_cnn\":\n",
        "    # Make sure you have defined CustomCNN somewhere above:\n",
        "    # class CustomCNN(nn.Module):\n",
        "    #     def __init__(self, num_classes): …\n",
        "    #     def forward(self, x): …\n",
        "    model = CustomCNN(num_classes=len(LEVEL_NAMES))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown MODEL_NAME: {MODEL_NAME}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\"x\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQTRUiF1pL0N"
      },
      "source": [
        "swin_tiny_patch4_window7_224"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rwa_9cGGpMri"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build your classifier & move to GPU\n",
        "################################################################################\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "# You can now set MODEL_NAME to one of:\n",
        "# \"efficientnet-b0\", \"efficientnet-b2\",\n",
        "# \"mobilenet_v2\", \"mobilenet_v3_large\",\n",
        "# \"resnet18\", \"resnet34\", \"resnet50\",\n",
        "# \"resnext50_32x4d\",\n",
        "# \"vgg16\",\n",
        "# \"densenet121\",\n",
        "# \"shufflenet_v2_x1_0\",\n",
        "# \"convnext_tiny\",\n",
        "# \"vit_b_16\",\n",
        "# \"swin_tiny_patch4_window7_224\",\n",
        "# or \"custom_cnn\"\n",
        "\n",
        "MODEL_NAME = \"swin_tiny_patch4_window7_224\"   # ← change as desired\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if MODEL_NAME == \"efficientnet-b0\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"efficientnet-b2\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v2\":\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v3_large\":\n",
        "    model = models.mobilenet_v3_large(pretrained=True)\n",
        "    in_features = model.classifier[3].in_features\n",
        "    model.classifier[3] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet18\":\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet34\":\n",
        "    model = models.resnet34(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet50\":\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnext50_32x4d\":\n",
        "    model = models.resnext50_32x4d(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vgg16\":\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    in_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"densenet121\":\n",
        "    model = models.densenet121(pretrained=True)\n",
        "    in_features = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"shufflenet_v2_x1_0\":\n",
        "    model = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"convnext_tiny\":\n",
        "    model = models.convnext_tiny(pretrained=True)\n",
        "    in_features = model.classifier[2].in_features\n",
        "    model.classifier[2] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vit_b_16\":\n",
        "    model = models.vit_b_16(pretrained=True)\n",
        "    in_features = model.heads.head.in_features\n",
        "    model.heads.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"swin_tiny_patch4_window7_224\":\n",
        "    model = models.swin_t(pretrained=True)\n",
        "    in_features = model.head.in_features\n",
        "    model.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"custom_cnn\":\n",
        "    # Make sure you have defined CustomCNN somewhere above:\n",
        "    # class CustomCNN(nn.Module):\n",
        "    #     def __init__(self, num_classes): …\n",
        "    #     def forward(self, x): …\n",
        "    model = CustomCNN(num_classes=len(LEVEL_NAMES))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown MODEL_NAME: {MODEL_NAME}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrGBF9T9xTFV"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsJNWuKyxTly"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# Colab‐Cell: Train a Norwood‐Stage Classifier on “Cropped‐Interior” Images\n",
        "# using EfficientNet-B0 with a single expand_ratio (e.g., 0.01), reading from\n",
        "# combined “orig + CLAHE” folders for raw + segmented images.\n",
        "#\n",
        "# Now we assume:\n",
        "#   • Combined segmented masks live under:\n",
        "#       /content/combined_segmented_boundary/<split>/Level X/\n",
        "#         ├── img001_boundary_orig.png\n",
        "#         ├── img001_boundary_clahe.png\n",
        "#         ├── img002_boundary_orig.png\n",
        "#         ├── img002_boundary_clahe.png\n",
        "#         └── …\n",
        "#   • Combined raw images live under:\n",
        "#       /content/combined_norwood_raw/<split>/Level X/\n",
        "#         ├── img001_orig.jpg\n",
        "#         ├── img001_clahe.jpg\n",
        "#         ├── img002_orig.jpg\n",
        "#         ├── img002_clahe.jpg\n",
        "#         └── …\n",
        "#\n",
        "# The Dataset logic will:\n",
        "#   - For each “*_boundary_*.png” in combined_segmented_boundary, extract the\n",
        "#     prefix (e.g. “img001”) and the suffix (e.g. “_orig” or “_clahe”).\n",
        "#   - Look up the matching raw image as prefix + suffix + “.jpg” in\n",
        "#     combined_norwood_raw.\n",
        "#   - Crop the raw image using the boundary mask after expanding by expand_ratio.\n",
        "#\n",
        "# Everything else—training loop, transforms, evaluation—remains unchanged.\n",
        "#\n",
        "# To change expand_ratio, just edit EXPAND_RATIO below.\n",
        "#\n",
        "# BEFORE RUNNING:\n",
        "#   • Confirm the combined directories exist as described above.\n",
        "#   • Use a GPU runtime: Runtime → Change runtime type → GPU.\n",
        "################################################################################\n",
        "\n",
        "# (0) Install & import dependencies\n",
        "!pip install --quiet torch torchvision tqdm scikit-learn pandas seaborn matplotlib opencv-python efficientnet_pytorch\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "# EfficientNet-B0\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# sklearn metrics\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "################################################################################\n",
        "# (1) Custom Dataset: Use combined folders (orig + CLAHE). The “suffix” is either\n",
        "# \"_orig\" or \"_clahe\" depending on the filename.\n",
        "################################################################################\n",
        "\n",
        "class NorwoodCroppedExpandedDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 raw_root: str,\n",
        "                 boundary_root: str,\n",
        "                 split: str,\n",
        "                 level_names: list,\n",
        "                 transform=None,\n",
        "                 expand_ratio: float = 0.01):\n",
        "        \"\"\"\n",
        "        raw_root:      \"/content/combined_norwood_raw\"\n",
        "        boundary_root: \"/content/combined_segmented_boundary\"\n",
        "        split:         one of \"train\", \"valid\", \"test\"\n",
        "        level_names:   [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "        transform:     torchvision transforms applied to each cropped image\n",
        "        expand_ratio:  fraction of max(w, h) added around the hair bounding box\n",
        "        \"\"\"\n",
        "        self.samples = []  # list of (raw_img_path, boundary_img_path, label_index)\n",
        "        self.transform = transform\n",
        "        self.expand_ratio = expand_ratio\n",
        "\n",
        "        for idx, lvl in enumerate(level_names):\n",
        "            raw_dir = os.path.join(raw_root, split, lvl)\n",
        "            bnd_dir = os.path.join(boundary_root, split, lvl)\n",
        "\n",
        "            if not os.path.isdir(raw_dir):\n",
        "                raise FileNotFoundError(f\"Raw folder not found: {raw_dir}\")\n",
        "            if not os.path.isdir(bnd_dir):\n",
        "                raise FileNotFoundError(f\"Boundary folder not found: {bnd_dir}\")\n",
        "\n",
        "            for fname in os.listdir(bnd_dir):\n",
        "                # We expect boundary filenames like \"img001_boundary_orig.png\" or \"img001_boundary_clahe.png\"\n",
        "                if \"_boundary\" not in fname.lower() or not fname.lower().endswith(\".png\"):\n",
        "                    continue\n",
        "\n",
        "                # Extract prefix and suffix from boundary filename\n",
        "                name, ext = os.path.splitext(fname)  # e.g. name = \"img001_boundary_orig\"\n",
        "                if \"_boundary\" not in name:\n",
        "                    continue\n",
        "\n",
        "                prefix, suffix = name.split(\"_boundary\", 1)\n",
        "                # suffix is either \"_orig\" or \"_clahe\"\n",
        "                raw_fname = prefix + suffix + \".jpg\"\n",
        "                raw_path = os.path.join(raw_dir, raw_fname)\n",
        "                bnd_path = os.path.join(bnd_dir, fname)\n",
        "\n",
        "                if not os.path.isfile(raw_path):\n",
        "                    raise FileNotFoundError(f\"Raw image missing for boundary: {raw_path}\")\n",
        "                self.samples.append((raw_path, bnd_path, idx))\n",
        "\n",
        "        if len(self.samples) == 0:\n",
        "            raise ValueError(f\"No samples found for split={split}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        raw_path, bnd_path, label = self.samples[index]\n",
        "\n",
        "        # Load raw and boundary as RGB NumPy arrays\n",
        "        raw_img = np.array(Image.open(raw_path).convert(\"RGB\"))\n",
        "        bnd_img = np.array(Image.open(bnd_path).convert(\"RGB\"))\n",
        "        H, W, _ = raw_img.shape\n",
        "\n",
        "        # Create binary mask of red boundary via HSV threshold\n",
        "        hsv = cv2.cvtColor(bnd_img, cv2.COLOR_RGB2HSV)\n",
        "        lower1 = np.array([0, 100, 100]); upper1 = np.array([10, 255, 255])\n",
        "        lower2 = np.array([160, 100, 100]); upper2 = np.array([180, 255, 255])\n",
        "        mask1 = cv2.inRange(hsv, lower1, upper1)\n",
        "        mask2 = cv2.inRange(hsv, lower2, upper2)\n",
        "        red_boundary_mask = cv2.bitwise_or(mask1, mask2)\n",
        "\n",
        "        # Find external contours on the red mask\n",
        "        contours, _ = cv2.findContours(\n",
        "            red_boundary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "\n",
        "        if len(contours) == 0:\n",
        "            # If no contour found, use full image\n",
        "            x, y, w, h = 0, 0, W, H\n",
        "        else:\n",
        "            # Merge all contour points to compute a tight bounding box\n",
        "            all_pts = np.vstack(contours).squeeze()\n",
        "            xs, ys = all_pts[:, 0], all_pts[:, 1]\n",
        "            x_min, x_max = xs.min(), xs.max()\n",
        "            y_min, y_max = ys.min(), ys.max()\n",
        "            x, y, w, h = int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)\n",
        "\n",
        "        # Expand bounding box by expand_ratio * max(w, h)\n",
        "        margin = int(self.expand_ratio * max(w, h))\n",
        "        x1 = max(0, x - margin)\n",
        "        y1 = max(0, y - margin)\n",
        "        x2 = min(W, x + w + margin)\n",
        "        y2 = min(H, y + h + margin)\n",
        "\n",
        "        # Crop raw image\n",
        "        cropped = raw_img[y1:y2, x1:x2]\n",
        "        if cropped.size == 0:\n",
        "            cropped = raw_img.copy()\n",
        "\n",
        "        # Convert to PIL and apply transforms\n",
        "        cropped_pil = Image.fromarray(cropped)\n",
        "        if self.transform:\n",
        "            cropped_pil = self.transform(cropped_pil)\n",
        "\n",
        "        return cropped_pil, label\n",
        "\n",
        "################################################################################\n",
        "# (2) Paths, Classes, and Transforms\n",
        "################################################################################\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Change these two roots to point at your combined folders:\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "RAW_ROOT      = \"/content/norwood_dataset_combined\"\n",
        "BOUNDARY_ROOT = \"/content/segmented_boundary_clahe\"\n",
        "\n",
        "# We still expect the same “train/Level 2, train/Level 3… valid/… test/…” structure\n",
        "SPLITS        = [\"train\", \"valid\", \"test\"]\n",
        "LEVEL_NAMES   = [f\"Level {i}\" for i in range(2, 8)]  # [\"Level 2\", \"Level 3\", ..., \"Level 7\"]\n",
        "\n",
        "# Verify folder structure once\n",
        "for split in SPLITS:\n",
        "    for lvl in LEVEL_NAMES:\n",
        "        raw_dir = os.path.join(RAW_ROOT,  split, lvl)\n",
        "        bnd_dir = os.path.join(BOUNDARY_ROOT, split, lvl)\n",
        "        if not os.path.isdir(raw_dir):\n",
        "            raise FileNotFoundError(f\"Missing raw folder: {raw_dir}\")\n",
        "        if not os.path.isdir(bnd_dir):\n",
        "            raise FileNotFoundError(f\"Missing boundary folder: {bnd_dir}\")\n",
        "print(\"Verified combined raw + combined segmented folder structure.\")\n",
        "\n",
        "# Transforms: resize to 224×224, augment on train\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "################################################################################\n",
        "# (3) Set expand_ratio and instantiate Datasets & DataLoaders (no loop)\n",
        "################################################################################\n",
        "\n",
        "# Change this value to experiment with a different expand_ratio:\n",
        "EXPAND_RATIO = 0.01  # 1% of max(w, h)\n",
        "\n",
        "print(f\"\\n▶ Running with expand_ratio = {EXPAND_RATIO:.2f}\")\n",
        "\n",
        "train_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"train\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=train_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "valid_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"valid\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "test_dataset = NorwoodCroppedExpandedDataset(\n",
        "    raw_root=RAW_ROOT,\n",
        "    boundary_root=BOUNDARY_ROOT,\n",
        "    split=\"test\",\n",
        "    level_names=LEVEL_NAMES,\n",
        "    transform=eval_transform,\n",
        "    expand_ratio=EXPAND_RATIO\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}\")\n",
        "print(f\"Valid samples: {len(valid_dataset)}\")\n",
        "print(f\"Test samples:  {len(test_dataset)}\")\n",
        "\n",
        "################################################################################\n",
        "# (4) Build your classifier & move to GPU\n",
        "################################################################################\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "# You can now set MODEL_NAME to one of:\n",
        "# \"efficientnet-b0\", \"efficientnet-b2\",\n",
        "# \"mobilenet_v2\", \"mobilenet_v3_large\",\n",
        "# \"resnet18\", \"resnet34\", \"resnet50\",\n",
        "# \"resnext50_32x4d\",\n",
        "# \"vgg16\",\n",
        "# \"densenet121\",\n",
        "# \"shufflenet_v2_x1_0\",\n",
        "# \"convnext_tiny\",\n",
        "# \"vit_b_16\",\n",
        "# \"swin_tiny_patch4_window7_224\",\n",
        "# or \"custom_cnn\"\n",
        "\n",
        "MODEL_NAME = \"efficientnet-b2\"   # ← change as desired\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if MODEL_NAME == \"efficientnet-b0\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"efficientnet-b2\":\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b2\")\n",
        "    in_features = model._fc.in_features\n",
        "    model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v2\":\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"mobilenet_v3_large\":\n",
        "    model = models.mobilenet_v3_large(pretrained=True)\n",
        "    in_features = model.classifier[3].in_features\n",
        "    model.classifier[3] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet18\":\n",
        "    model = models.resnet18(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet34\":\n",
        "    model = models.resnet34(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnet50\":\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"resnext50_32x4d\":\n",
        "    model = models.resnext50_32x4d(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vgg16\":\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    in_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"densenet121\":\n",
        "    model = models.densenet121(pretrained=True)\n",
        "    in_features = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"shufflenet_v2_x1_0\":\n",
        "    model = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "    in_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"convnext_tiny\":\n",
        "    model = models.convnext_tiny(pretrained=True)\n",
        "    in_features = model.classifier[2].in_features\n",
        "    model.classifier[2] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"vit_b_16\":\n",
        "    model = models.vit_b_16(pretrained=True)\n",
        "    in_features = model.heads.head.in_features\n",
        "    model.heads.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"swin_tiny_patch4_window7_224\":\n",
        "    model = models.swin_t(pretrained=True)\n",
        "    in_features = model.head.in_features\n",
        "    model.head = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "elif MODEL_NAME == \"custom_cnn\":\n",
        "    # Make sure you have defined CustomCNN somewhere above:\n",
        "    # class CustomCNN(nn.Module):\n",
        "    #     def __init__(self, num_classes): …\n",
        "    #     def forward(self, x): …\n",
        "    model = CustomCNN(num_classes=len(LEVEL_NAMES))\n",
        "\n",
        "else:\n",
        "    raise ValueError(f\"Unknown MODEL_NAME: {MODEL_NAME}\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "################################################################################\n",
        "# (5) Training & validation loop (10 epochs)\n",
        "################################################################################\n",
        "\n",
        "num_epochs      = 10\n",
        "best_valid_acc  = 0.0\n",
        "best_checkpoint = \"/content/best_efficientnetb0_single_ratio.pth\"\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # --- Training Phase ---\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [Train]\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(preds == labels).item()\n",
        "        total_train += inputs.size(0)\n",
        "\n",
        "    epoch_acc = running_corrects / total_train\n",
        "    print(f\"    [Epoch {epoch}] Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # --- Validation Phase ---\n",
        "    model.eval()\n",
        "    val_corrects = 0\n",
        "    total_valid = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Epoch {epoch}/{num_epochs} [Valid]\"):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_corrects += torch.sum(preds == labels).item()\n",
        "            total_valid += inputs.size(0)\n",
        "\n",
        "    val_acc = val_corrects / total_valid\n",
        "    print(f\"    [Epoch {epoch}] Valid Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save checkpoint if validation improves\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_checkpoint)\n",
        "        print(f\"      → New best valid acc: {best_valid_acc:.4f}. Saved to {best_checkpoint}\")\n",
        "\n",
        "################################################################################\n",
        "# (6) Load best checkpoint and evaluate on test set\n",
        "################################################################################\n",
        "\n",
        "model.load_state_dict(torch.load(best_checkpoint))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "test_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "test_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "print(f\"\\n▶ Final Test Accuracy: {test_acc*100:.2f}%  ({len(all_labels)} samples)\")\n",
        "print(f\"▶ Final Test Macro F1: {test_f1_macro:.4f}\")\n",
        "\n",
        "# (Optional) Print confusion matrix and classification report\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(LEVEL_NAMES))))\n",
        "cm_df = pd.DataFrame(cm, index=LEVEL_NAMES, columns=LEVEL_NAMES)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.ylabel(\"True Level\")\n",
        "plt.xlabel(\"Predicted Level\")\n",
        "plt.title(f\"Confusion Matrix (expand_ratio={EXPAND_RATIO:.2f})\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=LEVEL_NAMES, digits=4))\n",
        "\n",
        "################################################################################\n",
        "# End of script\n",
        "################################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20aRVKMlkodl"
      },
      "source": [
        "### GRAPHS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3l9H89Y9krpo"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Prepare your data\n",
        "data = {\n",
        "    \"model\": [\n",
        "        \"EfficientNet-B0\", \"ResNet18\", \"MobileNetV2\", \"VGG16\",\n",
        "        \"MobileNetV3-Large\", \"ResNet34\", \"ResNet50\", \"ResNeXt50_32x4d\",\n",
        "        \"DenseNet121\", \"ShuffleNetV2_x1_0\", \"ConvNeXt-Tiny\", \"ViT-B/16\", \"Swin-Tiny\"\n",
        "    ],\n",
        "    \"f1_score\": [\n",
        "        0.75, 0.76, 0.73, 0.70,\n",
        "        0.75, 0.71, 0.72, 0.74,\n",
        "        0.74, 0.68, 0.75, 0.70, 0.72\n",
        "    ],\n",
        "    \"test_accuracy\": [\n",
        "        0.86, 0.86, 0.82, 0.80,\n",
        "        0.85, 0.84, 0.81, 0.86,\n",
        "        0.84, 0.82, 0.85, 0.81, 0.86\n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 2) Sort by descending F1 so highest is on the left\n",
        "df_sorted = df.sort_values(by=\"f1_score\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "# 3) Create bar chart\n",
        "x = np.arange(len(df_sorted))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 7))\n",
        "bars_f1 = ax.bar(x - width/2, df_sorted[\"f1_score\"], width, label='Macro F1 Score')\n",
        "bars_acc = ax.bar(x + width/2, df_sorted[\"test_accuracy\"], width, label='Test Accuracy')\n",
        "\n",
        "# 4) Annotate each bar with its value\n",
        "for bar in bars_f1:\n",
        "    h = bar.get_height()\n",
        "    ax.annotate(f\"{h:.2f}\",\n",
        "                xy=(bar.get_x() + bar.get_width()/2, h),\n",
        "                xytext=(0, 3),\n",
        "                textcoords=\"offset points\",\n",
        "                ha='center', va='bottom')\n",
        "\n",
        "for bar in bars_acc:\n",
        "    h = bar.get_height()\n",
        "    ax.annotate(f\"{h:.2f}\",\n",
        "                xy=(bar.get_x() + bar.get_width()/2, h),\n",
        "                xytext=(0, 3),\n",
        "                textcoords=\"offset points\",\n",
        "                ha='center', va='bottom')\n",
        "\n",
        "# 5) Formatting\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(df_sorted[\"model\"], rotation=45, ha='right')\n",
        "ax.set_xlabel('Model')\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Macro F1 Score and Test Accuracy by Model (Sorted by the Macro F1 Scrore - Left to right)')\n",
        "\n",
        "# Place legend outside to the right\n",
        "ax.legend(loc='upper left', bbox_to_anchor=(1.02, 1), borderaxespad=0)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 0.85, 1])  # leave space on the right for legend\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCxnHZSu-9L_"
      },
      "source": [
        "# HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Zs-CTFclin4",
        "outputId": "d5231785-c396-4f7c-f491-e106e38df32f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytorch\n",
            "  Downloading pytorch-1.0.2.tar.gz (689 bytes)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pytorch\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for pytorch (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for pytorch\n",
            "Failed to build pytorch\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (pytorch)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKRyeL3zg9KW"
      },
      "source": [
        "resnet18 input with huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be76c17d",
        "outputId": "6171ee3d-a94d-46b0-ee24-1303f0c2662b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted: Level 5\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet torch torchvision Pillow huggingface_hub\n",
        "\n",
        "import json, torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# --- download weights + metadata from your HF repo ---\n",
        "state_dict_path = hf_hub_download(\"alamb98/resnet18\", filename=\"resnet18.pth\")\n",
        "meta_path       = hf_hub_download(\"alamb98/resnet18\", filename=\"metadata.json\")\n",
        "\n",
        "with open(meta_path, \"r\") as f:\n",
        "    meta = json.load(f)\n",
        "LEVEL_NAMES = meta.get(\"labels\", [f\"Level {i}\" for i in range(2, 8)])\n",
        "\n",
        "# --- build model the same way you trained it ---\n",
        "model = models.resnet18(weights=None)           # no built-in weights\n",
        "in_features = model.fc.in_features\n",
        "model.fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "model.load_state_dict(torch.load(state_dict_path, map_location=\"cpu\"))\n",
        "model.eval()\n",
        "\n",
        "# --- preprocessing identical to your eval_transform ---\n",
        "tfm = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# LOCAL path → open directly\n",
        "image_path = \"/content/10-Front_jpg.rf.11094538dd1409e0db0d7d91baa661b4.jpg\"\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "x = tfm(image).unsqueeze(0)  # (1,3,224,224)\n",
        "with torch.no_grad():\n",
        "    logits = model(x)\n",
        "pred_idx = logits.argmax(-1).item()\n",
        "print(\"Predicted:\", LEVEL_NAMES[pred_idx])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8g6LNqS0-yL"
      },
      "source": [
        "mobilenet_v3_large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226,
          "referenced_widgets": [
            "428a076fbed5463587662d9dd470e23e",
            "e9ac64047ce44a59b57babcf95abe13f",
            "e60897b477e6460d96609136bafd445a",
            "672dadbc846c4f95b0aa4af5d9b529d4",
            "bf60893451f2487abd1eb6ec810b9a34",
            "68f69bca68e74fc1b6b4ca44522c1129",
            "f361ce53cfd84a859d1107212088bb12",
            "c8a4e87781a744578d129ab0be2f7cfa",
            "a0b9c40dcd254fff802708fa00dcd954",
            "65ab429d7a514d6fa10b4bddf05d3704",
            "3c00484debea455db1bcd01d917776ac",
            "b9392e24db88439f8b677263d2797e83",
            "734d5361b28449898e6df8904b6c18fe",
            "5e876cb107a5434fa71ed4bfacdba859",
            "fd6133b5ef9340358df785f35ff85dc3",
            "66e63e0cb17049e580037b2af3eb22ae",
            "5c35423555ae4209b5a803693d93e23f",
            "a8eb50a96c2c47f7a999637e5adb22e1",
            "73574a7cbd8a4b418099318935f6340f",
            "1b26deed0ee442b4bdd770df2b531965",
            "c93250d3ce29458ba706e777ecdd641d",
            "b1f897fc3b554d35a5335211cf9a4038"
          ]
        },
        "id": "wiOkjuRV0_LM",
        "outputId": "e7f396af-7f98-401c-d46e-c4db142c16f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "mobilenet_v3_large_cropped_clahe_best.pt(…):   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "428a076fbed5463587662d9dd470e23e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "metadata.json:   0%|          | 0.00/561 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b9392e24db88439f8b677263d2797e83"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: Level 7 (level=6)\n"
          ]
        }
      ],
      "source": [
        "#!pip install --quiet torch torchvision Pillow huggingface_hub\n",
        "\n",
        "import json, torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# --- repo + filenames (adjust if your HF names differ) ---\n",
        "REPO         = \"alamb98/mobilenet_v3_large_cropped_clahe_norwood_classifier\"\n",
        "WEIGHTS_FILE = \"mobilenet_v3_large_cropped_clahe_best.pth\"   # <- exact filename in your HF repo\n",
        "META_FILE    = \"metadata.json\"            # <- contains labels/img_size/mean/std/label_base (if present)\n",
        "\n",
        "# --- download weights + metadata from your HF repo ---\n",
        "state_dict_path = hf_hub_download(REPO, filename=WEIGHTS_FILE)\n",
        "meta_path       = hf_hub_download(REPO, filename=META_FILE)\n",
        "\n",
        "with open(meta_path, \"r\") as f:\n",
        "    meta = json.load(f)\n",
        "\n",
        "LEVEL_NAMES = meta.get(\"labels\", [f\"Level {i}\" for i in range(2, 8)])\n",
        "IMG_SIZE    = int(meta.get(\"img_size\", 224))\n",
        "MEAN        = meta.get(\"mean\", [0.485, 0.456, 0.406])\n",
        "STD         = meta.get(\"std\",  [0.229, 0.224, 0.225])\n",
        "LABEL_BASE  = int(meta.get(\"label_base\", 0))  # 0 => logits 0..6 so +1; 1 => already 1..7\n",
        "\n",
        "# --- build model the same way you trained it ---\n",
        "model = models.mobilenet_v3_large(weights=None)   # no built-in pretrained weights\n",
        "in_features = model.classifier[3].in_features\n",
        "model.classifier[3] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "model.load_state_dict(torch.load(state_dict_path, map_location=\"cpu\"), strict=True)\n",
        "model.eval()\n",
        "\n",
        "# --- preprocessing identical to your eval transform ---\n",
        "tfm = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=MEAN, std=STD),\n",
        "])\n",
        "\n",
        "# --- predict helper (takes a PIL RGB image) ---\n",
        "def predict_mobilenet_v3(pil_rgb_image):\n",
        "    x = tfm(pil_rgb_image).unsqueeze(0)  # (1,3,H,W)\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)\n",
        "        pred_idx = int(logits.argmax(-1).item())\n",
        "        level = pred_idx + 1 if LABEL_BASE == 0 else pred_idx\n",
        "    # Return both numeric level and friendly label if available\n",
        "    label_str = LEVEL_NAMES[level - 1] if 1 <= level <= len(LEVEL_NAMES) else f\"Level {level}\"\n",
        "    return level, label_str\n",
        "\n",
        "# --- EXAMPLE: local path → open and run ---\n",
        "image_path = \"/content/11-Front_jpg.rf.6c9a7e546c587b57b46feae8c6f3276f.jpg\"   # <-- change this\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "level, label_str = predict_mobilenet_v3(image)\n",
        "print(f\"Predicted: {label_str} (level={level})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW6Mx5kA1XvL"
      },
      "source": [
        "effecientnetb0 with huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXyUAA4R1aYc",
        "outputId": "e3802a16-7752-4346-a036-2b81b5ede0fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted: Level 6 (level=5)\n"
          ]
        }
      ],
      "source": [
        "#!pip install --quiet torch torchvision Pillow huggingface_hub efficientnet_pytorch\n",
        "\n",
        "import json, torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from huggingface_hub import hf_hub_download\n",
        "from efficientnet_pytorch import EfficientNet  # pip install efficientnet_pytorch\n",
        "\n",
        "# --- repo + filenames (adjust if your HF names differ) ---\n",
        "REPO         = \"alamb98/efficientnet-b0_norwood_classifier\"\n",
        "WEIGHTS_FILE = \"efficientnet-b0_best.pth\"   # <- exact filename in your HF repo\n",
        "META_FILE    = \"metadata.json\"         # <- contains labels/img_size/mean/std/label_base (if present)\n",
        "\n",
        "# --- download weights + metadata ---\n",
        "state_dict_path = hf_hub_download(REPO, filename=WEIGHTS_FILE)\n",
        "meta_path       = hf_hub_download(REPO, filename=META_FILE)\n",
        "\n",
        "with open(meta_path, \"r\") as f:\n",
        "    meta = json.load(f)\n",
        "\n",
        "LEVEL_NAMES = meta.get(\"labels\", [f\"Level {i}\" for i in range(2, 8)])  # default: Level 2..7\n",
        "IMG_SIZE    = int(meta.get(\"img_size\", 224))\n",
        "MEAN        = meta.get(\"mean\", [0.485, 0.456, 0.406])\n",
        "STD         = meta.get(\"std\",  [0.229, 0.224, 0.225])\n",
        "LABEL_BASE  = int(meta.get(\"label_base\", 0))  # 0 => logits 0..6 so +1; 1 => already 1..7\n",
        "\n",
        "# --- build model exactly like training (no built-in pretrained weights) ---\n",
        "model = EfficientNet.from_name(\"efficientnet-b0\")  # no pretrained\n",
        "in_features = model._fc.in_features\n",
        "model._fc = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "# load trained weights (handles common wrapper keys/prefixes)\n",
        "state = torch.load(state_dict_path, map_location=\"cpu\")\n",
        "if isinstance(state, dict) and \"state_dict\" in state and isinstance(state[\"state_dict\"], dict):\n",
        "    state = state[\"state_dict\"]\n",
        "fixed = {}\n",
        "if isinstance(state, dict):\n",
        "    for k, v in state.items():\n",
        "        if isinstance(k, str) and k.startswith(\"module.\"):\n",
        "            k = k[len(\"module.\"):]\n",
        "        if isinstance(k, str) and k.startswith(\"model.\"):\n",
        "            k = k[len(\"model.\"):]\n",
        "        fixed[k] = v\n",
        "else:\n",
        "    fixed = state\n",
        "model.load_state_dict(fixed, strict=True)\n",
        "model.eval()\n",
        "\n",
        "# --- preprocessing identical to your eval transform (strict resize, no center-crop) ---\n",
        "tfm = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=MEAN, std=STD),\n",
        "])\n",
        "\n",
        "# --- predict helper (takes a PIL RGB image) ---\n",
        "def predict_efficientnet_b0(pil_rgb_image):\n",
        "    x = tfm(pil_rgb_image).unsqueeze(0)  # (1,3,H,W)\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)\n",
        "        pred_idx = int(logits.argmax(-1).item())\n",
        "        level = pred_idx + 1 if LABEL_BASE == 0 else pred_idx\n",
        "    label_str = LEVEL_NAMES[level - 1] if 1 <= level <= len(LEVEL_NAMES) else f\"Level {level}\"\n",
        "    return level, label_str\n",
        "\n",
        "# --- EXAMPLE: local path → open and run ---\n",
        "image_path = \"/content/10-Front_jpg.rf.11094538dd1409e0db0d7d91baa661b4.jpg\"   # <-- change this\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "level, label_str = predict_efficientnet_b0(image)\n",
        "print(f\"Predicted: {label_str} (level={level})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbflaG_-1hpT"
      },
      "source": [
        "convnext_tiny with hugginface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iy_pKWE01iI9",
        "outputId": "27022ea3-9444-474d-8bbc-6cafaaf2a828"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted: Level 6 (level=5)\n"
          ]
        }
      ],
      "source": [
        "#!pip install --quiet torch torchvision Pillow huggingface_hub\n",
        "\n",
        "import json, torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from huggingface_hub import hf_hub_download\n",
        "from PIL import Image\n",
        "\n",
        "# --- repo + filenames (adjust if your HF names differ) ---\n",
        "REPO         = \"alamb98/convnext_tiny_clahe_norwood_classifier\"\n",
        "WEIGHTS_FILE = \"convnext_tiny_clahe_best.pth\"   # exact filename in your repo\n",
        "META_FILE    = \"metadata.json\"                  # contains labels/img_size/mean/std/label_base\n",
        "\n",
        "# --- download weights + metadata ---\n",
        "state_dict_path = hf_hub_download(REPO, filename=WEIGHTS_FILE)\n",
        "meta_path       = hf_hub_download(REPO, filename=META_FILE)\n",
        "\n",
        "with open(meta_path, \"r\") as f:\n",
        "    meta = json.load(f)\n",
        "\n",
        "# Allow either \"labels\" or \"class_names\"\n",
        "LEVEL_NAMES = meta.get(\"labels\") or meta.get(\"class_names\") or [f\"Level {i}\" for i in range(2, 8)]\n",
        "IMG_SIZE    = int(meta.get(\"img_size\", 224))\n",
        "MEAN        = meta.get(\"mean\", [0.485, 0.456, 0.406])\n",
        "STD         = meta.get(\"std\",  [0.229, 0.224, 0.225])\n",
        "LABEL_BASE  = int(meta.get(\"label_base\", 0))  # 0 => logits 0..6 → add +1; 1 => already 1..7\n",
        "\n",
        "# --- build architecture exactly like training (no built-in weights) ---\n",
        "model = models.convnext_tiny(weights=None)\n",
        "in_features = model.classifier[2].in_features\n",
        "model.classifier[2] = nn.Linear(in_features, len(LEVEL_NAMES))\n",
        "\n",
        "# load trained weights (unwrap common wrappers and strip prefixes)\n",
        "state = torch.load(state_dict_path, map_location=\"cpu\")\n",
        "if isinstance(state, dict) and \"state_dict\" in state and isinstance(state[\"state_dict\"], dict):\n",
        "    state = state[\"state_dict\"]\n",
        "fixed = {}\n",
        "if isinstance(state, dict):\n",
        "    for k, v in state.items():\n",
        "        if isinstance(k, str) and k.startswith(\"module.\"):\n",
        "            k = k[len(\"module.\"):]\n",
        "        if isinstance(k, str) and k.startswith(\"model.\"):\n",
        "            k = k[len(\"model.\"):]\n",
        "        fixed[k] = v\n",
        "else:\n",
        "    fixed = state\n",
        "model.load_state_dict(fixed, strict=True)\n",
        "model.eval()\n",
        "\n",
        "# --- preprocessing identical to your eval transform (strict resize, no center-crop) ---\n",
        "tfm = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=MEAN, std=STD),\n",
        "])\n",
        "\n",
        "# --- predict helper (takes a PIL RGB image) ---\n",
        "def predict_convnext_tiny(pil_rgb_image: Image.Image):\n",
        "    \"\"\"\n",
        "    Returns (level_int, label_str).\n",
        "    - If LABEL_BASE == 0: logits are 0..6 → add +1 to map to 1..7\n",
        "    - If LABEL_BASE == 1: logits already represent 1..7\n",
        "    \"\"\"\n",
        "    x = tfm(pil_rgb_image).unsqueeze(0)  # (1,3,H,W)\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)\n",
        "        pred_idx = int(logits.argmax(-1).item())\n",
        "        level = pred_idx + 1 if LABEL_BASE == 0 else pred_idx\n",
        "    # best-effort label string from metadata\n",
        "    if 1 <= level <= len(LEVEL_NAMES):\n",
        "        label_str = LEVEL_NAMES[level - 1]  # assumes labels are ordered Level 2..7\n",
        "    else:\n",
        "        label_str = f\"Level {level}\"\n",
        "    return level, label_str\n",
        "\n",
        "# --- EXAMPLE: local path → open and run ---\n",
        "if __name__ == \"__main__\":\n",
        "    image_path = \"/content/10-Front_jpg.rf.11094538dd1409e0db0d7d91baa661b4.jpg\"  # <-- change this\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    level, label_str = predict_convnext_tiny(img)\n",
        "    print(f\"Predicted: {label_str} (level={level})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCd8fYkzI6-8"
      },
      "source": [
        "XGBoost huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Afgl9riEI-Zz",
        "outputId": "f80e5708-5804-4054-f204-0105493f8da4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: Yes hair fall\n",
            "Probability (Yes): 0.9843\n"
          ]
        }
      ],
      "source": [
        "# === Install deps (Colab-safe) ===\n",
        "!pip install -q pandas joblib huggingface_hub xgboost\n",
        "\n",
        "# === Imports ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# === Hugging Face repo + model artifact ===\n",
        "REPO_ID = \"alamb98/xgboost_hair_fall_classifier\"\n",
        "FILENAME = \"xgboost_hair_fall_classifier.joblib\"\n",
        "\n",
        "# === EXACT feature order expected by the trained model ===\n",
        "# (Taken from your mismatch error — do NOT change the order or names)\n",
        "FEATURE_COLUMNS = [\n",
        "    \"Do you stay up late at night?_Yes\",\n",
        "    \"Do you think that in your area water is a reason behind hair fall problems?_Yes\",\n",
        "    \"Is there anyone in your family having a hair fall problem or a baldness issue?_Yes\",\n",
        "    \"Do you use chemicals, hair gel, or color in your hair?_Yes\",\n",
        "    \"Do you have too much stress_Yes\",\n",
        "    \"Did you face any type of chronic illness in the past?_Yes\",\n",
        "    \"What is your age ?\"\n",
        "]\n",
        "\n",
        "def _to01(v):\n",
        "    \"\"\"Map various yes/no-like values to 1/0.\"\"\"\n",
        "    v = str(v).strip().lower()\n",
        "    return 1 if v in (\"yes\", \"y\", \"1\", \"true\", \"t\") else 0\n",
        "\n",
        "def form_to_features(form: dict) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Convert a simple dict form into the exact DataFrame shape/order your model expects.\n",
        "    form keys (your UI can name them anything):\n",
        "        - stay_up_late         -> Yes/No\n",
        "        - water_reason         -> Yes/No\n",
        "        - family_history       -> Yes/No\n",
        "        - use_chemicals        -> Yes/No\n",
        "        - stress               -> Yes/No\n",
        "        - chronic_illness      -> Yes/No\n",
        "        - age                  -> number\n",
        "    \"\"\"\n",
        "    row = {\n",
        "        \"Do you stay up late at night?_Yes\": _to01(form.get(\"stay_up_late\", \"No\")),\n",
        "        \"Do you think that in your area water is a reason behind hair fall problems?_Yes\": _to01(form.get(\"water_reason\", \"No\")),\n",
        "        \"Is there anyone in your family having a hair fall problem or a baldness issue?_Yes\": _to01(form.get(\"family_history\", \"No\")),\n",
        "        \"Do you use chemicals, hair gel, or color in your hair?_Yes\": _to01(form.get(\"use_chemicals\", \"No\")),\n",
        "        \"Do you have too much stress_Yes\": _to01(form.get(\"stress\", \"No\")),\n",
        "        \"Did you face any type of chronic illness in the past?_Yes\": _to01(form.get(\"chronic_illness\", \"No\")),\n",
        "        \"What is your age ?\": float(form.get(\"age\", 0)),\n",
        "    }\n",
        "    # Ensure column ORDER matches training exactly\n",
        "    return pd.DataFrame([row], columns=FEATURE_COLUMNS)\n",
        "\n",
        "# === Load the model from Hugging Face Hub ===\n",
        "model_path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\n",
        "xgb_model = joblib.load(model_path)\n",
        "\n",
        "def predict_from_form(form: dict):\n",
        "    \"\"\"\n",
        "    Returns dict with predicted class (0/1), label text, and probability for 'Yes'.\n",
        "    \"\"\"\n",
        "    X = form_to_features(form)\n",
        "    # XGBoost prefers float inputs\n",
        "    X = X.astype(float)\n",
        "\n",
        "    # Predict class and probability\n",
        "    y_pred = xgb_model.predict(X)[0]\n",
        "    # Some XGB versions use predict_proba; fall back if unavailable\n",
        "    if hasattr(xgb_model, \"predict_proba\"):\n",
        "        proba_yes = float(xgb_model.predict_proba(X)[0][1])\n",
        "    else:\n",
        "        # If model doesn't expose predict_proba, approximate via decision_function/sigmoid\n",
        "        # but typically XGBClassifier has predict_proba.\n",
        "        proba_yes = np.nan\n",
        "\n",
        "    label_map = {0: \"No hair fall\", 1: \"Yes hair fall\"}\n",
        "    return {\n",
        "        \"pred_class\": int(y_pred),\n",
        "        \"pred_label\": label_map.get(int(y_pred), str(y_pred)),\n",
        "        \"prob_yes\": proba_yes\n",
        "    }\n",
        "\n",
        "# === Example usage ===\n",
        "example_form = {\n",
        "    \"stay_up_late\": \"No\",\n",
        "    \"water_reason\": \"Yes\",\n",
        "    \"family_history\": \"Yes\",\n",
        "    \"use_chemicals\": \"No\",\n",
        "    \"stress\": \"Yes\",\n",
        "    \"chronic_illness\": \"No\",\n",
        "    \"age\": 27\n",
        "}\n",
        "\n",
        "result = predict_from_form(example_form)\n",
        "print(\"Prediction:\", result[\"pred_label\"])\n",
        "print(\"Probability (Yes):\", round(result[\"prob_yes\"], 4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwFMkKLtFUIq"
      },
      "source": [
        "SEGMENTATION huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olwKD76nFWNN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aef050b8-7170-4ff6-c3a9-2d4a65fc216d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prob stats  min/max/mean: 3.3787836972720697e-09 0.9997944235801697 0.22628255188465118\n",
            "Saved: /content/mask.png and /content/boundary.png\n",
            "Unique mask values: [  0 255]\n",
            "Hair pixels ratio: 0.22526611328125\n"
          ]
        }
      ],
      "source": [
        "# !pip install --quiet torch torchvision opencv-python pillow huggingface_hub\n",
        "\n",
        "import torch, numpy as np, cv2\n",
        "from PIL import Image\n",
        "from huggingface_hub import hf_hub_download\n",
        "from torchvision import transforms\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50, deeplabv3_resnet101\n",
        "\n",
        "# ===== CONFIG =====\n",
        "REPO_ID   = \"alamb98/deeplabv3-hair-segmentation\"\n",
        "CKPT_NAME = \"deeplabv3_final.pth\"\n",
        "\n",
        "IMAGE_PATH   = \"/content/29-Front_jpg.rf.3048eecf9b365863f1c21f3562f4a778.jpg\"   # <-- change this\n",
        "OUTPUT_MASK  = \"/content/mask.png\"\n",
        "OUTPUT_BOUND = \"/content/boundary.png\"\n",
        "\n",
        "# Try your actual backbone; if you trained on resnet101, change here\n",
        "BACKBONE    = \"resnet50\"      # \"resnet50\" or \"resnet101\"\n",
        "NUM_CLASSES = 2               # background, hair\n",
        "HAIR_CLASS  = 1               # if nothing shows up, try 0\n",
        "IMG_SIZE    = 256             # try 520 if you trained/evaluated at 520\n",
        "\n",
        "MEAN = [0.485, 0.456, 0.406]\n",
        "STD  = [0.229, 0.224, 0.225]\n",
        "# ===================\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "ckpt_path = hf_hub_download(repo_id=REPO_ID, filename=CKPT_NAME)\n",
        "\n",
        "# Build model to match backbone\n",
        "if BACKBONE == \"resnet50\":\n",
        "    model = deeplabv3_resnet50(weights=None, num_classes=NUM_CLASSES, aux_loss=None)\n",
        "elif BACKBONE == \"resnet101\":\n",
        "    model = deeplabv3_resnet101(weights=None, num_classes=NUM_CLASSES, aux_loss=None)\n",
        "else:\n",
        "    raise ValueError(\"Unsupported BACKBONE.\")\n",
        "\n",
        "# Load weights and report mismatches\n",
        "state = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "if isinstance(state, dict) and \"state_dict\" in state and isinstance(state[\"state_dict\"], dict):\n",
        "    state = state[\"state_dict\"]\n",
        "fixed = {}\n",
        "if isinstance(state, dict):\n",
        "    for k, v in state.items():\n",
        "        if k.startswith(\"module.\"): k = k[7:]\n",
        "        if k.startswith(\"model.\"):  k = k[6:]\n",
        "        fixed[k] = v\n",
        "else:\n",
        "    fixed = state\n",
        "\n",
        "missing, unexpected = model.load_state_dict(fixed, strict=False)\n",
        "if missing or unexpected:\n",
        "    print(\"⚠️ load_state_dict warnings:\")\n",
        "    if missing:    print(\"  Missing keys:\", missing[:10], (\"...+more\" if len(missing)>10 else \"\"))\n",
        "    if unexpected: print(\"  Unexpected keys:\", unexpected[:10], (\"...+more\" if len(unexpected)>10 else \"\"))\n",
        "\n",
        "model.to(device).eval()\n",
        "\n",
        "# Preprocess\n",
        "tfm = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=MEAN, std=STD),\n",
        "])\n",
        "\n",
        "# Load image\n",
        "orig = Image.open(IMAGE_PATH).convert(\"RGB\")\n",
        "W, H = orig.size\n",
        "x = tfm(orig).unsqueeze(0).to(device)\n",
        "\n",
        "# Inference → logits → softmax probs\n",
        "with torch.no_grad():\n",
        "    out = model(x)\n",
        "    logits = out[\"out\"][0] if isinstance(out, dict) else out[0]        # (C,H,W)\n",
        "    probs  = torch.softmax(logits, dim=0)                              # (C,H,W)\n",
        "    hair_p = probs[HAIR_CLASS].cpu().numpy()                           # (H,W)\n",
        "\n",
        "# Resize prob map back to original\n",
        "hair_p = cv2.resize(hair_p, (W, H), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "# Diagnostics\n",
        "print(\"prob stats  min/max/mean:\", float(hair_p.min()), float(hair_p.max()), float(hair_p.mean()))\n",
        "\n",
        "# Convert to 8-bit and Otsu threshold\n",
        "hair_u8 = np.clip(hair_p * 255.0, 0, 255).astype(np.uint8)\n",
        "_, mask_u8 = cv2.threshold(hair_u8, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "# Small morph clean-up (optional; comment out if you don’t want it)\n",
        "kernel = np.ones((3,3), np.uint8)\n",
        "mask_u8 = cv2.morphologyEx(mask_u8, cv2.MORPH_OPEN, kernel, iterations=1)\n",
        "mask_u8 = cv2.morphologyEx(mask_u8, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
        "\n",
        "# Save mask for inspection\n",
        "cv2.imwrite(OUTPUT_MASK, mask_u8)\n",
        "\n",
        "# Contours on original (red border only)\n",
        "img_bgr = cv2.cvtColor(np.array(orig), cv2.COLOR_RGB2BGR)\n",
        "contours, _ = cv2.findContours(mask_u8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "cv2.drawContours(img_bgr, contours, -1, (0, 0, 255), 2)  # red in BGR\n",
        "overlay = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "Image.fromarray(overlay).save(OUTPUT_BOUND)\n",
        "\n",
        "print(\"Saved:\", OUTPUT_MASK, \"and\", OUTPUT_BOUND)\n",
        "print(\"Unique mask values:\", np.unique(mask_u8))\n",
        "print(\"Hair pixels ratio:\", float((mask_u8>0).mean()))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "002f10dd430748748e26d1ac4d2e0dc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b11418d389f14f61835ca88981efe14e",
              "IPY_MODEL_41648911f2354641bd50058ea6293a01",
              "IPY_MODEL_d4a19ec480d640da85e373529d9dbfce"
            ],
            "layout": "IPY_MODEL_dcda11f9da084871a50a60ff96c390a8"
          }
        },
        "0086f32407ae4af48e79aff5e5db3804": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0172a41630864ffcb0e9252eb2d6248f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01ee4c7e72de4974b01f1147840b1826": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "028ac75023f24cc88993dc7b1177c08f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "030dc819b0af410b83aa907b7defcd02": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "034ad1f086334200bc793ccd27796796": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_9bcd5353852f43a4a70299227d42eedf"
          }
        },
        "036eebc17d3042eab30113c74ba97e81": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "048a31892d2340ff846fec152bc12692": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "053f7812e3a14f1eae6762629835f624": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0de8c3f756f041eb9ec016083d8addfe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "128ed33b329f4389935dbce22aa09d29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16bfe4754e82469b9e0ec1b6d7cf490d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17c7c858039245a4ba1f3c2f866b48be": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "1816f61f91c84c4c94b5940b8dc01aad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d8637001d234cf3ac601f6c1222f479": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f0fafd3fa9b4cea84823716835c679e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50e998199cc244d593e097d5389d8d05",
            "placeholder": "​",
            "style": "IPY_MODEL_2a64148a14404abc973b21287c28815a",
            "value": " 17.1MB / 17.1MB, 2.94MB/s  "
          }
        },
        "1f3ba421d43a4a9ea67d8e77455a13b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_919c718171594ef9b471595764e24f05",
            "placeholder": "​",
            "style": "IPY_MODEL_32ac2c5f1e294ba3b3aba773ac4ed87c",
            "value": " 16.4MB / 16.4MB            "
          }
        },
        "2382c53eee6241ddb6039a8dc8b1fe4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_053f7812e3a14f1eae6762629835f624",
            "placeholder": "​",
            "style": "IPY_MODEL_3dcfb9d923e6461b82719dbfdcd48747",
            "value": "Processing Files (1 / 1)                : 100%"
          }
        },
        "255ffb9efcc042b99a2197864f3b4a7e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "25cfa84cc88f4400982255dc21059f94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fb1d080fc6f4210b0dd6b56a799a839",
            "placeholder": "​",
            "style": "IPY_MODEL_6d1cceebff77421da9171af587e5bb92",
            "value": "New Data Upload                         : 100%"
          }
        },
        "269c55c3ce224bceb370ebfa1ddc4b10": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26be810c2f8b47b39263c862b44d58f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf6f78d6e58e48059e75e2df19399c1a",
              "IPY_MODEL_c912ad05683349dc84d2c7d457ae967b",
              "IPY_MODEL_c19998596f5045c7b893d3f749d6883d"
            ],
            "layout": "IPY_MODEL_0de8c3f756f041eb9ec016083d8addfe"
          }
        },
        "2719564fdb754508aa2c89e0b4551543": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29a9040263584f5fb32c3a41e61d2df8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_ac0d6439339e45ed9a904c4a996a0092"
          }
        },
        "2a31a6a494854aa09484c456a3aea269": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a64148a14404abc973b21287c28815a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b01f693bc0643979791c115a3c23ee8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "2bed2aff430b4afbaeb0644ee37cbfe2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c5d24e4b9ab47078ae7a0d7ef55ab5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c70402c6aa048d6832c538634a30bd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f273521e5744713a358e2f61f6f2531",
            "placeholder": "​",
            "style": "IPY_MODEL_690b5d40d42945ba88bead3144aa5e4d",
            "value": "  ...orch-hair-segmentation/resnet18.pth: 100%"
          }
        },
        "2c89be3bc8214895bb54c300c2e91d2b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d7440d3548843b88ca4045760af103a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d7d896f82604054b0037d4bb502f691": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e23acffcd00437db07e8641759c4949": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b1a8c2f4fb024c79abc4eba51f3d8d0e",
              "IPY_MODEL_43fcb1b22c364c748b89891857011e29",
              "IPY_MODEL_9e9dfafe028e4e33b623fccfb828f019"
            ],
            "layout": "IPY_MODEL_33b976d13ecc4cf1999b4cc33b7114df"
          }
        },
        "3049829651814c45951bb0cb889cc280": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_25cfa84cc88f4400982255dc21059f94",
              "IPY_MODEL_9bb1db85d930439789026854be8bf656",
              "IPY_MODEL_dd55b87d468442c890a3942ed914c5fa"
            ],
            "layout": "IPY_MODEL_f33b6fec8c444c38b51d0be21162140c"
          }
        },
        "3106f90866484f9bb11b102c920815b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d7440d3548843b88ca4045760af103a",
            "placeholder": "​",
            "style": "IPY_MODEL_7f4e3d51b7df40e898a81eddef78ac67",
            "value": "  111MB /  111MB, 11.1MB/s  "
          }
        },
        "32ac2c5f1e294ba3b3aba773ac4ed87c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33b976d13ecc4cf1999b4cc33b7114df": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35acc52314504223b7e352b9e7da2bf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bed2aff430b4afbaeb0644ee37cbfe2",
            "placeholder": "​",
            "style": "IPY_MODEL_3add284622fa4af4bb4f34becf96a579",
            "value": "New Data Upload                         : 100%"
          }
        },
        "37da321c152c4e3b80b8951dbf5ae661": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e378bfd7b59415789cf29923cb69c74",
            "max": 44797308,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_50e64c611d224f26ae2a58b829068b25",
            "value": 44797308
          }
        },
        "38049d92939545b1916e72b26b63300d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38bd19899cbc4c408112a9d1e2dde545": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39d2a5f78e9e4eebbdc69b9c1a313f93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39e4f3661a69460ab58315c42a027257": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3add284622fa4af4bb4f34becf96a579": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b25f03278f84042a8aed2320f1761a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "3dcfb9d923e6461b82719dbfdcd48747": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e373320964a4d1da85380fdb28a019c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ad7dbeeeb8943f59c012ff5e46f784b",
            "placeholder": "​",
            "style": "IPY_MODEL_dfb21724818b46a0826c1ea3f6a9217a",
            "value": " 17.1MB / 17.1MB, 2.94MB/s  "
          }
        },
        "3e378bfd7b59415789cf29923cb69c74": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41648911f2354641bd50058ea6293a01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c01841a3885543e199a96f72d2d9b3a3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fcb6f5628847461b8fe20e3b8715b830",
            "value": 1
          }
        },
        "42e1eaee788e49e78ad6cb4636422443": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43fcb1b22c364c748b89891857011e29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b25f03278f84042a8aed2320f1761a9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_048a31892d2340ff846fec152bc12692",
            "value": 1
          }
        },
        "441c0431704f41f8bf89e092180a69bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "491fc6f9dd9a4f23bdbabb3ca486fda9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c70402c6aa048d6832c538634a30bd8",
              "IPY_MODEL_37da321c152c4e3b80b8951dbf5ae661",
              "IPY_MODEL_e58aeb7d2e94401e8937332a8600da9f"
            ],
            "layout": "IPY_MODEL_1816f61f91c84c4c94b5940b8dc01aad"
          }
        },
        "4aa69667a56945d99a0d63daa9ace205": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b3f5b0576ee4beba97baf04ab4f9ef1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_701a6c57af6d40598e022be19c49a038",
            "max": 16357982,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e7f8ab59c56e497791b7ec0b0339b4bc",
            "value": 16357982
          }
        },
        "4bf4c53dba72468ba9cbca573c30f6ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65f5ec376a164b4c982c3879801bcf88",
            "placeholder": "​",
            "style": "IPY_MODEL_6c6f73850cca44fb81b14ef0f220ddf9",
            "value": "New Data Upload                         : 100%"
          }
        },
        "4e73577f142b4abba9f5f242ffa04d99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_ab9df7184f9d4ef0891d6a1aeefd534f"
          }
        },
        "50ce5572c8e24a25b5be8ca4bcf8c3ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2382c53eee6241ddb6039a8dc8b1fe4c",
              "IPY_MODEL_c730a45d27704e00a2a87f7ea30323c4",
              "IPY_MODEL_5d99d9b066ff4cdeb007d98b70368c0b"
            ],
            "layout": "IPY_MODEL_c56c50d4b97346a4b17a7786f01a59db"
          }
        },
        "50e64c611d224f26ae2a58b829068b25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50e998199cc244d593e097d5389d8d05": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52fc0e54eabd4d5e9438ea1af691e502": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55b2e6f329dd4dd7bb33a6175dd0674e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_e579c80be5e74ff59197ac83de7bb8cb"
          }
        },
        "5a61a1bad9fb4bb2804b6d4615c8fcc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b596c1de94243e498c5a92125afd9ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_35acc52314504223b7e352b9e7da2bf9",
              "IPY_MODEL_d59d32b6f6d04ecc9332073f376dada3",
              "IPY_MODEL_3106f90866484f9bb11b102c920815b8"
            ],
            "layout": "IPY_MODEL_ef9109f280d044f09a98bf6cc3d99413"
          }
        },
        "5d99d9b066ff4cdeb007d98b70368c0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_441c0431704f41f8bf89e092180a69bf",
            "placeholder": "​",
            "style": "IPY_MODEL_f47ac73baea642549dbafee5f4212607",
            "value": " 16.4MB / 16.4MB, 2.82MB/s  "
          }
        },
        "65f5ec376a164b4c982c3879801bcf88": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "669fb11cb724446990b9394a28b6deb2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "690b5d40d42945ba88bead3144aa5e4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c6f73850cca44fb81b14ef0f220ddf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d1cceebff77421da9171af587e5bb92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "701a6c57af6d40598e022be19c49a038": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74625abb9239466c95559809a23350fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f4e3d51b7df40e898a81eddef78ac67": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85427a41d3ed46a7860cde4c7fc75abe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd23641e80f046108169bbf66dfa86cf",
            "placeholder": "​",
            "style": "IPY_MODEL_42e1eaee788e49e78ad6cb4636422443",
            "value": "  ...gmentation/efficientnet-b0_best.pth: 100%"
          }
        },
        "8ad7dbeeeb8943f59c012ff5e46f784b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fc0bb64d856494782f64779250a89af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bbb3e6d4c06d4e96b6c854937e679791",
              "IPY_MODEL_bcc8ac5e0d774af2afe06ce7b61c5b9a",
              "IPY_MODEL_bbabf8d873374ee48fb8af159c95885b"
            ],
            "layout": "IPY_MODEL_aab8f90650fe4b7dab655a7bfcc0f604"
          }
        },
        "919c718171594ef9b471595764e24f05": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99c0d062ad3e4dca902dbbad07566614": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9bb1db85d930439789026854be8bf656": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_255ffb9efcc042b99a2197864f3b4a7e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39d2a5f78e9e4eebbdc69b9c1a313f93",
            "value": 1
          }
        },
        "9bcd5353852f43a4a70299227d42eedf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "9e9dfafe028e4e33b623fccfb828f019": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4014b7f04944ab8a51a76b6dedbf5af",
            "placeholder": "​",
            "style": "IPY_MODEL_f165849f936c437a8959af0066f9814c",
            "value": " 44.8MB / 44.8MB, 5.46MB/s  "
          }
        },
        "9f273521e5744713a358e2f61f6f2531": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fb1d080fc6f4210b0dd6b56a799a839": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a63a93a2d1bc4253a5766c56a54c9c27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aab8f90650fe4b7dab655a7bfcc0f604": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab9df7184f9d4ef0891d6a1aeefd534f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "ac0d6439339e45ed9a904c4a996a0092": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "b11418d389f14f61835ca88981efe14e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d8637001d234cf3ac601f6c1222f479",
            "placeholder": "​",
            "style": "IPY_MODEL_a63a93a2d1bc4253a5766c56a54c9c27",
            "value": "Processing Files (1 / 1)                : 100%"
          }
        },
        "b1a8c2f4fb024c79abc4eba51f3d8d0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38049d92939545b1916e72b26b63300d",
            "placeholder": "​",
            "style": "IPY_MODEL_dec697e9c3a3493e9db275f06b2c6bb0",
            "value": "Processing Files (1 / 1)                : 100%"
          }
        },
        "b2f1f595119d4fec940f7a7ce0e45328": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d47cdf87bc6740fba2c1043503d847ff",
            "placeholder": "​",
            "style": "IPY_MODEL_128ed33b329f4389935dbce22aa09d29",
            "value": "New Data Upload                         : 100%"
          }
        },
        "b9974637402149d1879b39da44de3b58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bbabf8d873374ee48fb8af159c95885b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f533200ed0734d47b41778921b5750ba",
            "placeholder": "​",
            "style": "IPY_MODEL_e8a742327d88496db8b7da507b911249",
            "value": "  111MB /  111MB            "
          }
        },
        "bbb3e6d4c06d4e96b6c854937e679791": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddf570c99bdd4e34a8694003e888246f",
            "placeholder": "​",
            "style": "IPY_MODEL_99c0d062ad3e4dca902dbbad07566614",
            "value": "  ...tation/convnext_tiny_clahe_best.pth: 100%"
          }
        },
        "bc5c4f26fae443958de1229d2c100d8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce72272810f74e6fb7975efa8f99cb96",
            "placeholder": "​",
            "style": "IPY_MODEL_c26884c312c54ee88b24970417d55bff",
            "value": " 44.8MB / 44.8MB, 5.46MB/s  "
          }
        },
        "bcc8ac5e0d774af2afe06ce7b61c5b9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01ee4c7e72de4974b01f1147840b1826",
            "max": 111368480,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9974637402149d1879b39da44de3b58",
            "value": 111368480
          }
        },
        "bd07bae1f1664153b6cda5929fc36c9b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c01841a3885543e199a96f72d2d9b3a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "c19998596f5045c7b893d3f749d6883d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2b6df7428714a4d9c4ddc2d1a4061ef",
            "placeholder": "​",
            "style": "IPY_MODEL_16bfe4754e82469b9e0ec1b6d7cf490d",
            "value": " 17.1MB / 17.1MB            "
          }
        },
        "c26884c312c54ee88b24970417d55bff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2b6df7428714a4d9c4ddc2d1a4061ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2d09d44b3b44ed6ad39f748ff7d94c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f575e212d1a44d15ab77ec08d45ad4bd",
              "IPY_MODEL_cb2f9c815bfc474a8504477f12dc1d9b",
              "IPY_MODEL_3e373320964a4d1da85380fdb28a019c"
            ],
            "layout": "IPY_MODEL_0086f32407ae4af48e79aff5e5db3804"
          }
        },
        "c56c50d4b97346a4b17a7786f01a59db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c730a45d27704e00a2a87f7ea30323c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_669fb11cb724446990b9394a28b6deb2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_028ac75023f24cc88993dc7b1177c08f",
            "value": 1
          }
        },
        "c912ad05683349dc84d2c7d457ae967b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52fc0e54eabd4d5e9438ea1af691e502",
            "max": 17053514,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38bd19899cbc4c408112a9d1e2dde545",
            "value": 17053514
          }
        },
        "c99b3f0c4fe74268912b7ae250db62cd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb2f9c815bfc474a8504477f12dc1d9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17c7c858039245a4ba1f3c2f866b48be",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6933959d0544a83ae35994cf299a169",
            "value": 1
          }
        },
        "cb871555a9694bda925a8c34714c9545": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ce72272810f74e6fb7975efa8f99cb96": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf6f78d6e58e48059e75e2df19399c1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c99b3f0c4fe74268912b7ae250db62cd",
            "placeholder": "​",
            "style": "IPY_MODEL_4aa69667a56945d99a0d63daa9ace205",
            "value": "  ...net_v3_large_cropped_clahe_best.pth: 100%"
          }
        },
        "d476a017467b4900aa222b8d28a7828d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d47cdf87bc6740fba2c1043503d847ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4a19ec480d640da85e373529d9dbfce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_269c55c3ce224bceb370ebfa1ddc4b10",
            "placeholder": "​",
            "style": "IPY_MODEL_5a61a1bad9fb4bb2804b6d4615c8fcc5",
            "value": "  111MB /  111MB, 11.1MB/s  "
          }
        },
        "d4d841d9e50f4720ab3a17548d8295ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_030dc819b0af410b83aa907b7defcd02",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74625abb9239466c95559809a23350fe",
            "value": 1
          }
        },
        "d59d32b6f6d04ecc9332073f376dada3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b01f693bc0643979791c115a3c23ee8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2719564fdb754508aa2c89e0b4551543",
            "value": 1
          }
        },
        "db40513ae30f4966961fb109aa064c0d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcda11f9da084871a50a60ff96c390a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd55b87d468442c890a3942ed914c5fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_036eebc17d3042eab30113c74ba97e81",
            "placeholder": "​",
            "style": "IPY_MODEL_0172a41630864ffcb0e9252eb2d6248f",
            "value": " 16.4MB / 16.4MB, 2.82MB/s  "
          }
        },
        "ddf570c99bdd4e34a8694003e888246f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dec697e9c3a3493e9db275f06b2c6bb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dfb21724818b46a0826c1ea3f6a9217a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e579c80be5e74ff59197ac83de7bb8cb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "e58aeb7d2e94401e8937332a8600da9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d476a017467b4900aa222b8d28a7828d",
            "placeholder": "​",
            "style": "IPY_MODEL_2a31a6a494854aa09484c456a3aea269",
            "value": " 44.8MB / 44.8MB            "
          }
        },
        "e65e29ba548940a6a8d2b8cddae5acaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb871555a9694bda925a8c34714c9545",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c5d24e4b9ab47078ae7a0d7ef55ab5b",
            "value": 1
          }
        },
        "e7f8ab59c56e497791b7ec0b0339b4bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8a742327d88496db8b7da507b911249": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eba0fa1932d549beb037df7e10444cb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85427a41d3ed46a7860cde4c7fc75abe",
              "IPY_MODEL_4b3f5b0576ee4beba97baf04ab4f9ef1",
              "IPY_MODEL_1f3ba421d43a4a9ea67d8e77455a13b4"
            ],
            "layout": "IPY_MODEL_db40513ae30f4966961fb109aa064c0d"
          }
        },
        "ec58321961e34a0a8aa121b520135aef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4bf4c53dba72468ba9cbca573c30f6ba",
              "IPY_MODEL_e65e29ba548940a6a8d2b8cddae5acaa",
              "IPY_MODEL_1f0fafd3fa9b4cea84823716835c679e"
            ],
            "layout": "IPY_MODEL_2c89be3bc8214895bb54c300c2e91d2b"
          }
        },
        "ef9109f280d044f09a98bf6cc3d99413": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f165849f936c437a8959af0066f9814c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f322c0123ef54d588aff5dde49ebf8e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b2f1f595119d4fec940f7a7ce0e45328",
              "IPY_MODEL_d4d841d9e50f4720ab3a17548d8295ee",
              "IPY_MODEL_bc5c4f26fae443958de1229d2c100d8a"
            ],
            "layout": "IPY_MODEL_39e4f3661a69460ab58315c42a027257"
          }
        },
        "f33b6fec8c444c38b51d0be21162140c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4014b7f04944ab8a51a76b6dedbf5af": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f47ac73baea642549dbafee5f4212607": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f533200ed0734d47b41778921b5750ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f575e212d1a44d15ab77ec08d45ad4bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd07bae1f1664153b6cda5929fc36c9b",
            "placeholder": "​",
            "style": "IPY_MODEL_2d7d896f82604054b0037d4bb502f691",
            "value": "Processing Files (1 / 1)                : 100%"
          }
        },
        "f6933959d0544a83ae35994cf299a169": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fcb6f5628847461b8fe20e3b8715b830": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fd23641e80f046108169bbf66dfa86cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23ac1a3a8ecf4ec09094ae21aa817991": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_940c4ce7694b43238eff6e270ac6f0c0"
          }
        },
        "9ed2258af4fb4643b467c3936e568c85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d3532f07f2a456186155ab2d30e6af2",
            "placeholder": "​",
            "style": "IPY_MODEL_dbd1ffc6b21d4ef2bd4e01f77d89b325",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "a5f3715d7cbc48d6b459676a7444589b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_639babf06c59419b9627ae0b7023de45",
            "placeholder": "​",
            "style": "IPY_MODEL_c2d60374f0ed4c34a0861eb6009071d3",
            "value": ""
          }
        },
        "bc9c197b7ae141eaac1ddb55d880b1c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_0b80fce333514e569108211bdc8a9447",
            "style": "IPY_MODEL_f199383a3b394d51896af8647077323e",
            "value": true
          }
        },
        "eb3922c952814868933578fffa63b984": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_8bd1a7a92a694aefae73508e614e8ee3",
            "style": "IPY_MODEL_71fbf539eaab4240bd7bb73f99e2d2a3",
            "tooltip": ""
          }
        },
        "8afeb10959fe447ea0fc20203ad497b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77b02f880f13481690efc5177d6fca92",
            "placeholder": "​",
            "style": "IPY_MODEL_9db8299c2d4a4171baace78b96b1f58b",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "940c4ce7694b43238eff6e270ac6f0c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "6d3532f07f2a456186155ab2d30e6af2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbd1ffc6b21d4ef2bd4e01f77d89b325": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "639babf06c59419b9627ae0b7023de45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2d60374f0ed4c34a0861eb6009071d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b80fce333514e569108211bdc8a9447": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f199383a3b394d51896af8647077323e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8bd1a7a92a694aefae73508e614e8ee3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71fbf539eaab4240bd7bb73f99e2d2a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "77b02f880f13481690efc5177d6fca92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9db8299c2d4a4171baace78b96b1f58b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98bebf69edc249f88e7f9f5e213f03d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_33d142e37a1946e598e35deed5586168",
              "IPY_MODEL_9e1f328c172549928e45b8e4ae8b18e8",
              "IPY_MODEL_052eee3ddf9844e08a45fe3b67c28938"
            ],
            "layout": "IPY_MODEL_cf4424f0336e4056b5a8bd6b0c3985e8"
          }
        },
        "33d142e37a1946e598e35deed5586168": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b84c2053e08a43728bbfa2eb0feee2ca",
            "placeholder": "​",
            "style": "IPY_MODEL_72551f1a124547bab79719961fd1b0db",
            "value": "Processing Files (1 / 1)                : 100%"
          }
        },
        "9e1f328c172549928e45b8e4ae8b18e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c414dd774c54bf49e23215c61b20583",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a2e39f8ed96d40739a538aa8542df517",
            "value": 1
          }
        },
        "052eee3ddf9844e08a45fe3b67c28938": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_513b2d0816294f27abd44f7284b032f1",
            "placeholder": "​",
            "style": "IPY_MODEL_a8cb129722024720a160f9c398459f3c",
            "value": "  159MB /  159MB, 49.7MB/s  "
          }
        },
        "cf4424f0336e4056b5a8bd6b0c3985e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b84c2053e08a43728bbfa2eb0feee2ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72551f1a124547bab79719961fd1b0db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c414dd774c54bf49e23215c61b20583": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a2e39f8ed96d40739a538aa8542df517": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "513b2d0816294f27abd44f7284b032f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8cb129722024720a160f9c398459f3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f35542698004811839b80722c379f46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_264602f0b2df4f4291089f5d88fb4f42",
              "IPY_MODEL_e68a1b79a90647ec94aacde97c49d31c",
              "IPY_MODEL_7dd6d4fb8f554d73abf59fd2e0909459"
            ],
            "layout": "IPY_MODEL_9d5d25bdf2994028bb8ef28b43e1f3c2"
          }
        },
        "264602f0b2df4f4291089f5d88fb4f42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4efb922081e144fc80bbb0575e501df8",
            "placeholder": "​",
            "style": "IPY_MODEL_854d86ddbdae48d7b6b3ef7aa62e229f",
            "value": "New Data Upload                         : 100%"
          }
        },
        "e68a1b79a90647ec94aacde97c49d31c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49f055ac5acb462b99cfa4485634d427",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_737fac1a430f4d9a9e4111b61ddce8b6",
            "value": 1
          }
        },
        "7dd6d4fb8f554d73abf59fd2e0909459": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf12ca6987b64d4ea4e0cc965e4c3cbe",
            "placeholder": "​",
            "style": "IPY_MODEL_7d96044031594510abcbd4dac1025040",
            "value": "  159MB /  159MB, 49.7MB/s  "
          }
        },
        "9d5d25bdf2994028bb8ef28b43e1f3c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4efb922081e144fc80bbb0575e501df8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "854d86ddbdae48d7b6b3ef7aa62e229f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49f055ac5acb462b99cfa4485634d427": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "737fac1a430f4d9a9e4111b61ddce8b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf12ca6987b64d4ea4e0cc965e4c3cbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d96044031594510abcbd4dac1025040": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6108ac494aae4137a63cd58bcfa8634f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a8956c19ecaa4f8a8d91d1ed0b9897b9",
              "IPY_MODEL_209921462b404299b32ead5a004925b3",
              "IPY_MODEL_2623ae7bade042ada374f022725b7166"
            ],
            "layout": "IPY_MODEL_9180da59d528438db1f6d73e88d5fe87"
          }
        },
        "a8956c19ecaa4f8a8d91d1ed0b9897b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97fc33175f054c218c3efa1cf14ee89b",
            "placeholder": "​",
            "style": "IPY_MODEL_f1e529d340704e85b6357a6ef4003fed",
            "value": "  ...ir-segmentation/deeplabv3_final.pth: 100%"
          }
        },
        "209921462b404299b32ead5a004925b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8c2177df2f24d47b7f82541159b696e",
            "max": 158889870,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_661bab5f5dda4fc2923e869973fba919",
            "value": 158889870
          }
        },
        "2623ae7bade042ada374f022725b7166": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ea4e4565ed741208958a53bf7328629",
            "placeholder": "​",
            "style": "IPY_MODEL_8a49e21273ca48d189e431c13b10f978",
            "value": "  159MB /  159MB            "
          }
        },
        "9180da59d528438db1f6d73e88d5fe87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97fc33175f054c218c3efa1cf14ee89b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1e529d340704e85b6357a6ef4003fed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8c2177df2f24d47b7f82541159b696e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "661bab5f5dda4fc2923e869973fba919": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ea4e4565ed741208958a53bf7328629": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a49e21273ca48d189e431c13b10f978": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e7a52db094e473ba5a794c97649f657": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7521e3e6e90f40b6b5f905d252e80d94",
            "placeholder": "​",
            "style": "IPY_MODEL_f513a73fe8104db3b693caccdac477fb",
            "value": "Connecting..."
          }
        },
        "7521e3e6e90f40b6b5f905d252e80d94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f513a73fe8104db3b693caccdac477fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "428a076fbed5463587662d9dd470e23e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9ac64047ce44a59b57babcf95abe13f",
              "IPY_MODEL_e60897b477e6460d96609136bafd445a",
              "IPY_MODEL_672dadbc846c4f95b0aa4af5d9b529d4"
            ],
            "layout": "IPY_MODEL_bf60893451f2487abd1eb6ec810b9a34"
          }
        },
        "e9ac64047ce44a59b57babcf95abe13f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68f69bca68e74fc1b6b4ca44522c1129",
            "placeholder": "​",
            "style": "IPY_MODEL_f361ce53cfd84a859d1107212088bb12",
            "value": "mobilenet_v3_large_cropped_clahe_best.pt(…): 100%"
          }
        },
        "e60897b477e6460d96609136bafd445a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8a4e87781a744578d129ab0be2f7cfa",
            "max": 17053514,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0b9c40dcd254fff802708fa00dcd954",
            "value": 17053514
          }
        },
        "672dadbc846c4f95b0aa4af5d9b529d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65ab429d7a514d6fa10b4bddf05d3704",
            "placeholder": "​",
            "style": "IPY_MODEL_3c00484debea455db1bcd01d917776ac",
            "value": " 17.1M/17.1M [00:01&lt;00:00, 14.2MB/s]"
          }
        },
        "bf60893451f2487abd1eb6ec810b9a34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68f69bca68e74fc1b6b4ca44522c1129": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f361ce53cfd84a859d1107212088bb12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8a4e87781a744578d129ab0be2f7cfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0b9c40dcd254fff802708fa00dcd954": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "65ab429d7a514d6fa10b4bddf05d3704": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c00484debea455db1bcd01d917776ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9392e24db88439f8b677263d2797e83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_734d5361b28449898e6df8904b6c18fe",
              "IPY_MODEL_5e876cb107a5434fa71ed4bfacdba859",
              "IPY_MODEL_fd6133b5ef9340358df785f35ff85dc3"
            ],
            "layout": "IPY_MODEL_66e63e0cb17049e580037b2af3eb22ae"
          }
        },
        "734d5361b28449898e6df8904b6c18fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c35423555ae4209b5a803693d93e23f",
            "placeholder": "​",
            "style": "IPY_MODEL_a8eb50a96c2c47f7a999637e5adb22e1",
            "value": "metadata.json: 100%"
          }
        },
        "5e876cb107a5434fa71ed4bfacdba859": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73574a7cbd8a4b418099318935f6340f",
            "max": 561,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b26deed0ee442b4bdd770df2b531965",
            "value": 561
          }
        },
        "fd6133b5ef9340358df785f35ff85dc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c93250d3ce29458ba706e777ecdd641d",
            "placeholder": "​",
            "style": "IPY_MODEL_b1f897fc3b554d35a5335211cf9a4038",
            "value": " 561/561 [00:00&lt;00:00, 63.9kB/s]"
          }
        },
        "66e63e0cb17049e580037b2af3eb22ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c35423555ae4209b5a803693d93e23f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8eb50a96c2c47f7a999637e5adb22e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73574a7cbd8a4b418099318935f6340f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b26deed0ee442b4bdd770df2b531965": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c93250d3ce29458ba706e777ecdd641d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1f897fc3b554d35a5335211cf9a4038": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}